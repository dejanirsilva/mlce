<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.26">

  <meta name="author" content="Dejanir Silva">
  <title>Machine Learning for Computational Economics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-cfda842e1c2da8cd79357d57ebaa6bca.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning for Computational Economics</h1>
  <p class="subtitle"><span class="module-name">Module 04: Fundamentals of Machine Learning</span><br><span class="school-name">EDHEC Business School</span></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dejanir Silva 
</div>
        <p class="quarto-title-affiliation">
            Purdue University
          </p>
    </div>
</div>

  <p class="date">January 2026</p>
</section>
<section id="introduction" class="slide level2 compact-slide">
<h2>Introduction</h2>
<p>In this module, we introduce the fundamental concepts of machine learning.</p>
<ul>
<li>Our goal is not to provide a comprehensive review of the field
<ul>
<li>See, e.g., <span class="citation" data-cites="HastieTibshiraniFriedman2009ESL">Hastie, Tibshirani, and Friedman (<a href="#/references" role="doc-biblioref" onclick="">2009</a>)</span>, <span class="citation" data-cites="deeplearning">Goodfellow, Bengio, and Courville (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span>, and <span class="citation" data-cites="Prince2023UDL">Prince (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span> for excellent reviews.</li>
</ul></li>
<li>But rather to develop the core tools needed to study high-dimensional economic models.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<p>We proceed in three steps.</p>
<ul>
<li>We show how to represent functions using <span class="text-blue"><strong>neural networks</strong></span>.</li>
<li>We explain how to estimate their parameters using <span class="text-orange"><strong>stochastic gradient descent</strong></span>.</li>
<li>We describe how <span class="text-green"><strong>automatic differentiation</strong></span> enables efficient computation of the gradients.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<p>The module is organized as follows:</p>
<ol type="1">
<li>Supervised Learning and Neural Networks</li>
<li>Gradient Descent and Its Variants</li>
<li>Automatic Differentiation and Backpropagation</li>
</ol>
</section>
<section>
<section id="i.-supervised-learning" class="title-slide slide level1 center">
<h1>I. Supervised Learning</h1>

</section>
<section id="supervised-learning" class="slide level2 compact-slide">
<h2>Supervised Learning</h2>
<p><span class="text-blue"><strong>Supervised learning</strong></span> concerns the task of inferring a mapping from inputs to outputs using labeled data.</p>
<ul>
<li>It seeks to learn a function <span class="math inline">\(f\)</span> that maps inputs <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span> to outputs <span class="math inline">\(\mathbf{y} \in \mathbb{R}^p\)</span>, given a dataset of labeled pairs <span class="math inline">\(\{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^I\)</span>.</li>
<li>In essence, this is a <em>regression problem</em>, but one that often involves extremely large input spaces and complex nonlinear dependencies.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<div class="fragment">
<p><span class="text-orange"><strong>Example: Image classification.</strong></span></p>
<ul>
<li>The ImageNet dataset contains over 15 million labeled images spanning more than 22,000 categories.</li>
<li>Each image is a <span class="math inline">\(224\times224\times3\)</span> array of pixels, where each pixel takes values between 0 and 255.</li>
</ul>
</div>
<div class="fragment">
<p>A linear model could, in principle, be used to predict the category of an image.</p>
<ul>
<li>However, such a model would treat each pixel <em>independently</em>: marginal effects are unrelated to neighboring pixels.</li>
<li>Images, by contrast, contain strong spatial structure and nonlinear relationships between pixels across channels.</li>
</ul>
</div>
<div class="fragment">
<div title="AlexNet">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>AlexNet</strong></p>
</div>
<div class="callout-content">
<p>In groundbreaking work, <span class="citation" data-cites="imagenet">Krizhevsky, Sutskever, and Hinton (<a href="#/references" role="doc-biblioref" onclick="">2012</a>)</span>—widely known as AlexNet—introduced a deep neural network architecture that achieved dramatic improvements in classification accuracy on ImageNet.</p>
<ul>
<li>Their model had eight layers and over 60 million parameters, exceptionally large at the time.</li>
<li>It demonstrated that deep neural networks could solve complex visual tasks at scale.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="imagenet-image-and-its-rgb-channels" class="slide level2 compact-slide">
<h2>ImageNet Image and its RGB Channels</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/gazelle0.png" style="width:70.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/gazelle1.png" style="width:70.0%"></p>
</div></div>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/gazelle2.png" style="width:70.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/gazelle3.png" style="width:70.0%"></p>
</div></div>
</section>
<section id="linear-regression" class="slide level2 compact-slide">
<h2>Linear Regression</h2>
<p>To illustrate the key concepts in supervised learning, it is useful to start with a familiar model: <span class="text-green"><strong>linear regression</strong></span>.</p>
<ul>
<li>Suppose we are given a dataset of <span class="math inline">\(I\)</span> observations of the input <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and output <span class="math inline">\(y_i \in \mathbb{R}\)</span> for <span class="math inline">\(i = 1, \ldots, I\)</span>.</li>
<li>Given the input <span class="math inline">\(\mathbf{x}_i\)</span>, the model prediction <span class="math inline">\(\hat{y}_i\)</span> is given by the function <span class="math inline">\(f(\mathbf{x}_i, \mathbf{\theta})\)</span>, where <span class="math inline">\(\mathbf{\theta} \in \mathbb{R}^d\)</span> is a vector of parameters. <span class="math display">\[
f(\mathbf{x}_i, \mathbf{\theta})
= \color{#0072b2}{\underbrace{\mathbf{w}^{\top}}_{\text{weights}}}\mathbf{x}_i
+ \color{#d55e00}{\underbrace{b}_{\text{bias}}}
\]</span> where <span class="math inline">\(\mathbf{\theta} = (\mathbf{w}^{\top}, b)^{\top}\)</span>.</li>
</ul>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/linear_regression.png" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p>The function <span class="math inline">\(f(\mathbf{x}_i, \mathbf{\theta})\)</span> defines a family of linear functions</p>
<ul>
<li>We obtain different functions by varying the parameters <span class="math inline">\(\mathbf{\theta}\)</span>.</li>
</ul>
<div style="margin-top: 0.5em;">

</div>
<p>Our goal is to find the parameter values that <em>best</em> fit the data.</p>
<ul>
<li>To do this, we define a loss function that measures model fit: <span class="math display">\[
\mathcal{L}(\mathbf{\theta}) = \frac{1}{2I} \sum_{i=1}^I (y_i - f(\mathbf{x}_i, \mathbf{\theta}))^2.
\]</span> This is the <span class="text-orange"><strong>mean squared error</strong></span> (MSE) loss function.</li>
</ul>
</div></div>
</div>
</section>
<section id="the-least-squares-estimator" class="slide level2 compact-slide">
<h2>The Least Squares Estimator</h2>
<p>Our goal is to find the parameters that <em>minimize</em> the loss function.</p>
<ul>
<li>In the linear regression case, we can solve for the optimal parameters in closed form.</li>
<li>We need to set the <span class="text-orange"><strong>gradient</strong></span> of the loss function to zero: <span class="math display">\[
\nabla \mathcal{L}(\mathbf{\theta}) =
\begin{bmatrix}
  \frac{\partial \mathcal{L}}{\partial \mathbf{w}} \\
  \frac{\partial \mathcal{L}}{\partial b}
\end{bmatrix}
= \frac{1}{I}\mathbf{X}^{\top} (\mathbf{X} \mathbf{\theta} - \mathbf{y}) = \mathbf{0}
\]</span> where <span class="math display">\[
\mathbf{X} = \begin{bmatrix}
  \mathbf{x}_1^{\top} &amp; 1 \\
  \mathbf{x}_2^{\top} &amp; 1 \\
  \vdots &amp; \vdots \\
  \mathbf{x}_I^{\top} &amp; 1
\end{bmatrix} \in \mathbb{R}^{I \times d},
\qquad
\mathbf{y} =
\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_I
\end{bmatrix} \in \mathbb{R}^I.
\]</span></li>
</ul>
<div class="fragment">
<p>Solving the <span class="text-blue"><strong>normal equations</strong></span> yields the least squares estimator: <span class="math display">\[
\hat{\mathbf{\theta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{y}.
\]</span></p>
<div title="Gradient">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Gradient</strong></p>
</div>
<div class="callout-content">
<p>We denote the <span class="text-green"><strong>gradient</strong></span> of a function <span class="math inline">\(f(\mathbf{x})\in \mathbb{R}\)</span> with respect to <span class="math inline">\(\mathbf{x}\in \mathbb{R}^d\)</span> as <span class="math inline">\(\nabla f(\mathbf{x})\in \mathbb{R}^d\)</span>.</p>
<ul>
<li>The gradient is a column vector, so its differential is written as <span class="math inline">\(d f(\mathbf{x}) = \nabla f(\mathbf{x})^{\top} d\mathbf{x}\)</span>.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gradient-descent" class="slide level2 compact-slide">
<h2>Gradient Descent</h2>
<p>In general, we cannot solve for the parameters in closed form.</p>
<ul>
<li>In this case, we need to use an iterative method to find the parameters, such as <span class="text-orange"><strong>gradient descent</strong></span>.</li>
</ul>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
<p>Starting from an initial guess <span class="math inline">\(\mathbf{\theta}^{(0)}\)</span>, gradient descent updates <span class="math inline">\(\mathbf{\theta}\)</span> in the direction <em>opposite</em> to the gradient of the loss function.</p>
<ul>
<li>To see why, note that the change in the loss function is approximately given by: <span class="math display">\[
\mathcal{L}(\mathbf{\theta} + \Delta \mathbf{\theta}) - \mathcal{L}(\mathbf{\theta})
\approx \nabla \mathcal{L}(\mathbf{\theta})^{\top} \Delta \mathbf{\theta},
\qquad \qquad
|\nabla \mathcal{L}(\mathbf{\theta})^{\top} \Delta \mathbf{\theta}|
\leq \|\nabla \mathcal{L}(\mathbf{\theta})\|\,\|\Delta \mathbf{\theta}\|
\]</span></li>
<li>The largest reduction in the loss function is achieved when <span class="math inline">\(\Delta \mathbf{\theta}\)</span> is proportional to the negative gradient.</li>
</ul>
<p>Given a learning rate <span class="math inline">\(\eta\)</span>, the gradient descent update rule is: <span class="math display">\[
\mathbf{\theta} \leftarrow \mathbf{\theta} - \eta \nabla \mathcal{L}(\mathbf{\theta})
\]</span></p>
<div style="border-top: 1px solid #ccc; margin: 0.0em 0;">

</div>
</div>
<div class="fragment">
<div title="Computational cost">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Computational cost</strong></p>
</div>
<div class="callout-content">
<p>The gradient of the loss function with respect to the parameters is given by: <span class="math display">\[
\nabla \mathcal{L}(\mathbf{\theta})
= \frac{1}{I} \sum_{i=1}^I
\big(f(\mathbf{x}_i, \mathbf{\theta}) - y_i\big)\,
\nabla_{\mathbf{\theta}} f(\mathbf{x}_i, \mathbf{\theta}).
\]</span> Each iteration of gradient descent therefore requires evaluating <span class="math inline">\(\nabla_{\mathbf{\theta}} f(\mathbf{x}_i, \mathbf{\theta})\)</span> for all <span class="math inline">\(I\)</span> observations.</p>
<ul>
<li>For large datasets, this can be computationally expensive</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="implementation-of-gradient-descent" class="slide level2 compact-slide">
<h2>Implementation of Gradient Descent</h2>
<div style="background-color: #f5f5f5; padding: 1em; border-left: 4px solid #0072b2; margin: 1em 0; font-size: 0.85em;">
<p><strong>Algorithm: Gradient Descent</strong></p>
<p><strong>Input:</strong> Initial guess <span class="math inline">\(\mathbf{\theta}^{(0)}\)</span>, learning rate <span class="math inline">\(\eta\)</span><br>
<strong>Output:</strong> Parameter estimate <span class="math inline">\(\mathbf{\theta}\)</span><br>
<strong>Initialize:</strong> <span class="math inline">\(t \gets 0\)</span></p>
<p><strong>Repeat</strong> until <span class="math inline">\(\|\nabla \mathcal{L}(\mathbf{\theta}^{(t)})\| &lt; \epsilon\)</span>:</p>
<ul>
<li><strong>Parameter update:</strong><br>
<span class="math inline">\(\mathbf{\theta}^{(t+1)} \gets \mathbf{\theta}^{(t)} - \eta \nabla \mathcal{L}(\mathbf{\theta}^{(t)})\)</span></li>
<li><span class="math inline">\(t \gets t + 1\)</span></li>
</ul>
<p><strong>Return:</strong> <span class="math inline">\(\mathbf{\theta}^{(t)}\)</span></p>
</div>
<div class="fragment">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_grad_descent" data-code-line-numbers="1-17|4-6|7|9-11|12-14|16"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_grad_descent-1"><a href="#ls_grad_descent-1"></a><span class="kw">function</span> <span class="fu">ls_grad_descent</span>(y<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>, x<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>, θ₀<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>; </span>
<span id="ls_grad_descent-2"><a href="#ls_grad_descent-2"></a>    learning_rate<span class="op">::</span><span class="dt">Real</span>=<span class="fl">0.01</span>, max_iter<span class="op">::</span><span class="dt">Integer</span>=<span class="fl">100</span>, ε<span class="op">::</span><span class="dt">Real </span><span class="op">=</span> <span class="fl">1e-4</span>)</span>
<span id="ls_grad_descent-3"><a href="#ls_grad_descent-3"></a>    <span class="pp">@assert</span> <span class="fu">length</span>(x) <span class="op">==</span> <span class="fu">length</span>(y)</span>
<span id="ls_grad_descent-4"><a href="#ls_grad_descent-4"></a>    I <span class="op">=</span> <span class="fu">length</span>(x)</span>
<span id="ls_grad_descent-5"><a href="#ls_grad_descent-5"></a>    X <span class="op">=</span> <span class="fu">hcat</span>(x, <span class="fu">ones</span>(I)) <span class="co"># design matrix</span></span>
<span id="ls_grad_descent-6"><a href="#ls_grad_descent-6"></a>    <span class="fu">∇f</span>(θ) <span class="op">=</span> X<span class="op">'</span> <span class="op">*</span> (X <span class="op">*</span> θ <span class="op">-</span> y) <span class="op">/</span> I <span class="co"># gradient</span></span>
<span id="ls_grad_descent-7"><a href="#ls_grad_descent-7"></a>    w_path, b_path <span class="op">=</span><span class="dt">Float64</span>[θ₀[<span class="fl">1</span>]], <span class="dt">Float64</span>[θ₀[<span class="fl">2</span>]] <span class="co"># initial values</span></span>
<span id="ls_grad_descent-8"><a href="#ls_grad_descent-8"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>max_iter</span>
<span id="ls_grad_descent-9"><a href="#ls_grad_descent-9"></a>        g <span class="op">=</span> <span class="fu">∇f</span>([w_path[<span class="kw">end</span>], b_path[<span class="kw">end</span>]])</span>
<span id="ls_grad_descent-10"><a href="#ls_grad_descent-10"></a>        <span class="fu">push!</span>(w_path, w_path[<span class="kw">end</span>] <span class="op">-</span> learning_rate <span class="op">*</span> g[<span class="fl">1</span>])</span>
<span id="ls_grad_descent-11"><a href="#ls_grad_descent-11"></a>        <span class="fu">push!</span>(b_path, b_path[<span class="kw">end</span>] <span class="op">-</span> learning_rate <span class="op">*</span> g[<span class="fl">2</span>])</span>
<span id="ls_grad_descent-12"><a href="#ls_grad_descent-12"></a>        <span class="cf">if</span> <span class="fu">norm</span>(g) <span class="op">&lt;</span> ε</span>
<span id="ls_grad_descent-13"><a href="#ls_grad_descent-13"></a>            <span class="cf">break</span></span>
<span id="ls_grad_descent-14"><a href="#ls_grad_descent-14"></a>        <span class="cf">end</span></span>
<span id="ls_grad_descent-15"><a href="#ls_grad_descent-15"></a>    <span class="cf">end</span></span>
<span id="ls_grad_descent-16"><a href="#ls_grad_descent-16"></a>    <span class="cf">return</span> (;w <span class="op">=</span> w_path, b <span class="op">=</span> b_path)</span>
<span id="ls_grad_descent-17"><a href="#ls_grad_descent-17"></a><span class="kw">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="loss-surface" class="slide level2 compact-slide">
<h2>Loss Surface</h2>
<p>The <span class="text-orange"><strong>loss surface</strong></span> shows the loss for different parameter values.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="Figures/loss_landscape_animation.gif" style="width:100.0%"></p>
<div style="text-align: center; font-size: 0.75em; margin-top: 0.5em; color: #666;">

</div>
</div><div class="column" style="width:60%;">
<div style="margin-top: 2.5em;">

</div>
<p>We can visualize the loss surface as a <em>heatmap</em>.</p>
<ul>
<li>The minimum of the loss surface is the parameter values that best fit the data.</li>
<li>This corresponds to the darkest region in the heatmap.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<p>The animation shows the gradient descent path being traced</p>
<ul>
<li><p>The yellow path shows how the algorithm moves toward the minimum.</p></li>
<li><p>The red dot marks the true parameter values used to simulate the data.</p></li>
<li><p>The algorithm converges to the minimum of the loss surface.</p></li>
</ul>
</div></div>
</section>
<section id="training-and-testing" class="slide level2 compact-slide">
<h2>Training and Testing</h2>
<p>In the discussion above, we used all available data to estimate the parameters.</p>
<ul>
<li>In practice, we typically split the data into a <span class="text-blue"><strong>training set</strong></span> and a <span class="text-orange"><strong>test set</strong></span>.</li>
<li>The training set is used to estimate the parameters, the test set is used to evaluate how well the model fits new data.</li>
</ul>
<div class="fragment">
<div style="margin-top: 0.5em;">

</div>
<div style="border-top: 1px solid #ccc; margin: 1em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>Consider data generated by a noisy version of the <em>Runge function</em>: <span class="math display">\[
  y_i = \frac{1}{1+25 z_i^2} + \epsilon_i,
  \qquad \epsilon_i \sim \text{i.i.d. with } \mathbb{E}[\epsilon_i]=0,
\]</span> with <span class="math inline">\(z_i \in [-1,1]\)</span>.</p>
</div><div class="column" style="width:50%;">
<p>Consider a polynomial regression with regressors: <span class="math display">\[
\mathbf{x}_i = [z_i, z_i^2, \ldots, z_i^{d-1}]
\]</span></p>
<p>As the degree <span class="math inline">\(d\)</span> increases, the model becomes more flexible.</p>
</div></div>
<div style="border-top: 1px solid #ccc; margin: 1em 0;">

</div>
</div>
<div class="fragment">
<div class="columns">
<div class="column" style="width:35%;">
<p><img data-src="Figures/polynomial_regression.png" style="width:100.0%"></p>
</div><div class="column" style="width:65%;">
<div style="margin-top: 1.5em;">

</div>
<p>When <span class="math inline">\(d = I\)</span>, the model reaches the <em>interpolation threshold</em>.</p>
<ul>
<li><p>But high-degree polynomial interpolants of the Runge function oscillate violently.</p></li>
<li><p>The model performs poorly on the test set for high <span class="math inline">\(d\)</span>.</p></li>
</ul>
<p>The figure shows the training loss declines steadily as <span class="math inline">\(d\)</span> increases</p>
<ul>
<li>But the test loss eventually rises: this is the hallmark of <span class="text-green"><strong>overfitting</strong></span></li>
</ul>
</div></div>
</div>
</section></section>
<section>
<section id="ii.-neural-networks" class="title-slide slide level1 center">
<h1>II. Neural Networks</h1>

</section>
<section id="shallow-neural-networks" class="slide level2 compact-slide">
<h2>Shallow Neural Networks</h2>
<p>We now move from linear to nonlinear models by introducing the <span class="text-orange"><strong>shallow neural network</strong></span> (SNN).</p>
<ul>
<li>An SNN can be viewed as a generalization of the linear regression model</li>
<li>A series of intermediate transformations combines a linear function with a nonlinear activation.</li>
</ul>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 1em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="text-blue"><strong>Hidden units</strong></span>: a nonlinear transformations of the inputs. <span class="math display">\[
h_j(\mathbf{x}, \mathbf{\theta}_j) = \sigma(\mathbf{w}_j^{\top} \mathbf{x} + b_j), \qquad j = 0, \ldots, n-1
\]</span> where <span class="math inline">\(\sigma\)</span> is an activation function, <span class="math inline">\(\mathbf{\theta}_j = (\mathbf{w}_j^{\top}, b_j)^{\top}\)</span>.</p>
</div><div class="column" style="width:50%;">
<p>The <strong>SNN</strong> can be written as: <span class="math display">\[
f(\mathbf{x}, \mathbf{\theta}) = \mathbf{w}_n^{\top} \mathbf{h}(\mathbf{x}, \mathbf{\theta}) + b_n,
\]</span> where <span class="math inline">\(\mathbf{\theta} = (\mathbf{\theta}_0^{\top}, \ldots, \mathbf{\theta}_{n}^{\top})^{\top}\)</span> and <span class="math inline">\(\mathbf{\theta}_n = (\mathbf{w}_n^{\top}, b_n)^{\top}\)</span>.</p>
</div></div>
<div style="border-top: 1px solid #ccc; ;">

</div>
<div class="r-stack">
<div class="columns fragment current-visible">
<div class="column" style="width:50%;">
<p><img data-src="Figures/activation_functions1.png" style="vertical-align: top;;width:80.0%"></p>
</div><div class="column" style="width:50%;">
<div style="margin-top: 1.5em;">

</div>
<p>The <span class="text-blue"><strong>rectified linear unit (ReLU)</strong></span>: <span class="math display">\[
\sigma(x) = \max(0, x).
\]</span></p>
<p>This is the mostly used activation function in machine learning.</p>
<ul>
<li>We will often focus on the ReLU.</li>
</ul>
</div></div>
<div class="columns fragment current-visible">
<div class="column" style="width:50%;">
<p><img data-src="Figures/activation_functions2.png" style="vertical-align: top;;width:80.0%"></p>
</div><div class="column" style="width:50%;">
<div style="margin-top: 1.5em;">

</div>
<p>The <span class="text-orange"><strong>Gaussian Error Linear Unit (GELU)</strong></span>: <span class="math display">\[
\sigma(x) = x \Phi(x),
\]</span> where <span class="math inline">\(\Phi(x)\)</span> is the standard normal cdf.</p>
<ul>
<li>It has been shown to improve performance in many applications.</li>
</ul>
</div></div>
<div class="columns fragment current-visible">
<div class="column" style="width:50%;">
<p><img data-src="Figures/activation_functions3.png" style="vertical-align: top;;width:80.0%"></p>
</div><div class="column" style="width:50%;">
<div style="margin-top: 1.5em;">

</div>
<p>The <span class="text-green"><strong>hyperbolic tangent (tanh)</strong></span>: <span class="math display">\[
\sigma(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}.
\]</span> It is historically popular in recurrent networks.</p>
<ul>
<li>It provides smooth, bounded outputs between -1 and 1.</li>
</ul>
</div></div>
<div class="columns fragment fade-in">
<div class="column" style="width:50%;">
<p><img data-src="Figures/activation_functions4.png" style="vertical-align: top;;width:80.0%"></p>
</div><div class="column" style="width:50%;">
<div style="margin-top: 1.5em;">

</div>
<p>The <span style="color: #cc79a7;"><strong>Sigmoid Linear Unit (SiLU)</strong></span>: <span class="math display">\[
\sigma(x) = \frac{x}{1 + e^{-x}}.
\]</span> Also known as Swish.</p>
<ul>
<li>SiLU yields smooth gradients even for negative inputs.</li>
</ul>
</div></div>
</div>
</div>
</section>
<section id="snns-architecture-and-implementation" class="slide level2 compact-slide">
<h2>SNNs Architecture and Implementation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/shallow_nn.png" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<div style="margin-top: 1.5em;">

</div>
<p>This figure illustrates the <em>architecture</em> of an SNN.</p>
<ul>
<li>Green nodes: <span class="text-green"><strong>input layer</strong></span>.</li>
<li>Blue nodes: <span class="text-blue"><strong>hidden layer</strong></span>.</li>
<li>Orange node: <span class="text-orange"><strong>output layer</strong></span>.</li>
</ul>
<p>The hidden units are also known as <em>neurons</em>.</p>
<ul>
<li>A neural network is a collection of neurons.</li>
<li>A <em>shallow</em> neural network has a single hidden layer.</li>
</ul>
</div></div>
<div class="fragment">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="shallow_nn" data-code-line-numbers="1-14|1-8|10-14"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="shallow_nn-1"><a href="#shallow_nn-1"></a><span class="kw">function</span> <span class="fu">shallow_nn</span>(x<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>, </span>
<span id="shallow_nn-2"><a href="#shallow_nn-2"></a>    W<span class="op">::</span><span class="dt">AbstractMatrix{&lt;:Real}</span>, b<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>,</span>
<span id="shallow_nn-3"><a href="#shallow_nn-3"></a>    wₙ<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>, bₙ<span class="op">::</span><span class="dt">Real</span>; σ<span class="op">::</span><span class="dt">Function </span><span class="op">=</span> <span class="fu">x-&gt;max</span>(<span class="fl">0</span>,x))</span>
<span id="shallow_nn-4"><a href="#shallow_nn-4"></a>    <span class="pp">@assert</span> <span class="fu">size</span>(W,<span class="fl">2</span>) <span class="op">==</span> <span class="fu">length</span>(x)  <span class="co"># ncols of W = length of x</span></span>
<span id="shallow_nn-5"><a href="#shallow_nn-5"></a>    <span class="pp">@assert</span> <span class="fu">size</span>(W,<span class="fl">1</span>) <span class="op">==</span> <span class="fu">length</span>(wₙ) <span class="co"># nrows of W = length of wₙ</span></span>
<span id="shallow_nn-6"><a href="#shallow_nn-6"></a>    <span class="pp">@assert</span> <span class="fu">length</span>(b) <span class="op">==</span> <span class="fu">size</span>(W,<span class="fl">1</span>)  <span class="co"># biases for the hidden units</span></span>
<span id="shallow_nn-7"><a href="#shallow_nn-7"></a>    <span class="cf">return</span> wₙ<span class="ch">' * σ.(W * x .+ b)+ bₙ</span></span>
<span id="shallow_nn-8"><a href="#shallow_nn-8"></a><span class="kw">end</span></span>
<span id="shallow_nn-9"><a href="#shallow_nn-9"></a><span class="co"># Convenience: scalar input (d = 1); w is the column of W</span></span>
<span id="shallow_nn-10"><a href="#shallow_nn-10"></a><span class="kw">function</span> <span class="fu">shallow_nn</span>(x<span class="op">::</span><span class="dt">Real</span>, </span>
<span id="shallow_nn-11"><a href="#shallow_nn-11"></a>    w<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>, b<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>,</span>
<span id="shallow_nn-12"><a href="#shallow_nn-12"></a>    wₙ<span class="op">::</span><span class="dt">AbstractVector{&lt;:Real}</span>, bₙ<span class="op">::</span><span class="dt">Real</span>; σ<span class="op">::</span><span class="dt">Function </span><span class="op">=</span> <span class="fu">x-&gt;max</span>(<span class="fl">0</span>,x))</span>
<span id="shallow_nn-13"><a href="#shallow_nn-13"></a>    <span class="cf">return</span> <span class="fu">shallow_nn</span>([x], <span class="fu">reshape</span>(w,<span class="fu">length</span>(w),<span class="fl">1</span>), b, wₙ, bₙ, σ <span class="op">=</span> σ)</span>
<span id="shallow_nn-14"><a href="#shallow_nn-14"></a><span class="kw">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="snns-as-piecewise-linear-functions" class="slide level2 compact-slide">
<h2>SNNs as Piecewise Linear Functions</h2>
<p>To illustrate the structure of an SNN, consider a one-dimensional input <span class="math inline">\(x_i \in [0,1]\)</span> and a ReLU activation function.</p>
<ul>
<li>Assume that all hidden-unit weights are <span class="math inline">\(w_j = 1\)</span>.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/shallow_network.png" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p><span class="math display">\[
SNN: f(x_i, \theta) = \sum_{j=0}^{n-1} w_{n,j} \max(0, x_i - \hat{x}_j) + b_n,
\]</span> where <span class="math inline">\(\hat{x}_j \equiv - b_j\)</span> is a convenient reparametrization of the biases.</p>
<div style="margin-top: 1.5em;">

</div>
<p>Ordering the breakpoints as <span class="math inline">\(0 = \hat{x}_0 &lt; \cdots &lt; \hat{x}_{n-1} &lt; 1 \equiv \hat{x}_n\)</span>, <span class="math display">\[
    f(x_i, \theta) = f(\hat{x}_j, \theta) + w_{n,j}(x_i - \hat{x}_j),
    \qquad x_i \in (\hat{x}_j, \hat{x}_{j+1}],
\]</span> with <span class="math inline">\(f(\hat{x}_0, \theta) = b_n\)</span>.</p>
</div></div>
<div class="fragment">
<p>The function is piecewise linear, with the breakpoints <span class="math inline">\(\hat{x}_j\)</span> learned by the model.</p>
<ul>
<li>If we fix the breakpoints to be equally spaced, the network collapses to the locally linear interpolant from Module 3.</li>
<li>Hence, an SNN can be interpreted as a <em>finite-difference method</em> with an <span class="text-orange"><strong>adaptive grid</strong></span> that learns where to place the nodes.</li>
</ul>
</div>
</section>
<section id="the-adaptive-choice-of-breakpoints-in-action" class="slide level2 compact-slide">
<h2>The Adaptive Choice of Breakpoints in Action</h2>
<p>ReLU networks <span class="text-orange"><strong>adaptively</strong></span> place their breakpoints in regions where the target function exhibits strong nonlinearity.</p>
<ul>
<li>To illustrate this, we fit a one–hidden–layer ReLU network to the density of a Beta distribution</li>
<li>We choose parameters <span class="math inline">\(\alpha=2\)</span> and <span class="math inline">\(\beta=10\)</span>, so the curvature is concentrated at low <span class="math inline">\(x\)</span> values.</li>
</ul>
<div class="r-stack">
<div class="columns fragment current-visible">
<div class="column" style="width:50%;">
<p><img data-src="Figures/beta_fit1a.png" style="width:90.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Model fit (15 hidden units)</strong>
</p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/beta_fit1b.png" style="width:90.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Breakpoint histogram (15 hidden units)</strong>
</p>
</div></div>
<div class="columns fragment current-visible">
<div class="column" style="width:50%;">
<p><img data-src="Figures/beta_fit2a.png" style="width:90.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Model fit (30 hidden units)</strong>
</p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/beta_fit2b.png" style="width:90.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Breakpoint histogram (30 hidden units)</strong>
</p>
</div></div>
<div class="columns fragment fade-in">
<div class="column" style="width:50%;">
<p><img data-src="Figures/beta_fit3a.png" style="width:90.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Model fit (100 hidden units)</strong>
</p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/beta_fit3b.png" style="width:90.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Breakpoint histogram (100 hidden units)</strong>
</p>
</div></div>
</div>
</section>
<section id="the-universal-approximation-theorem" class="slide level2 compact-slide">
<h2>The Universal Approximation Theorem</h2>
<p>The piecewise–linear structure of an SNN enables it to approximate nonlinear functions.</p>
<ul>
<li>A natural question is: how expressive is this model? What class of functions can an SNN represent or approximate?</li>
</ul>
<div class="fragment">
<p>The <span class="text-orange"><strong>Universal Approximation Theorem</strong></span> provides an answer.</p>
<div title="Universal Approximation Theorem">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Universal Approximation Theorem</strong></p>
</div>
<div class="callout-content">
<p>Any continuous function on a compact subset of <span class="math inline">\(\mathbb{R}^n\)</span> can be approximated to arbitrary precision by a shallow neural network with any non-polynomial activation function.</p>
<p><strong>Proof:</strong> See <span class="citation" data-cites="universal">Cybenko (<a href="#/references" role="doc-biblioref" onclick="">1989</a>)</span>, <span class="citation" data-cites="HORNIK">Hornik (<a href="#/references" role="doc-biblioref" onclick="">1991</a>)</span>, and <span class="citation" data-cites="leshno1993multilayer">Leshno et al. (<a href="#/references" role="doc-biblioref" onclick="">1993</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>This result is closely related to the <span class="text-blue"><strong>Option Spanning Theorem</strong></span> of <span class="citation" data-cites="spanning_theorem">Ross (<a href="#/references" role="doc-biblioref" onclick="">1976</a>)</span></p>
<ul>
<li>The result states that options portfolios can replicate any continuous payoff function.</li>
<li>A ReLu SNN corresponds to a portfolio of call/put options.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
</div>
<div class="fragment">
<p>The Universal Approximation Theorem establishes that shallow networks are theoretically sufficient</p>
<ul>
<li>But it does not imply that they are the most efficient way to approximate a function.</li>
<li>Achieving high accuracy may require an exponentially large number of hidden units.</li>
<li>We turn next to a richer class of models—<span class="text-green"><strong>deep neural networks</strong></span>—which can approximate complex functions more efficiently.</li>
</ul>
</div>
</section>
<section id="deep-neural-networks" class="slide level2 compact-slide">
<h2>Deep Neural Networks</h2>
<p>A shallow neural network contains a single hidden layer between the input and output layers.</p>
<ul>
<li><span class="text-orange"><strong>Deep neural networks</strong></span> (DNNs) extend this architecture by stacking multiple hidden layers on top of each other.</li>
<li>Each layer applies a linear transformation followed by a nonlinear activation.</li>
</ul>
<div class="fragment">
<p>Formally, let the <span class="math inline">\(n_l\)</span>-dimensional vector of <em>hidden units</em> at layer <span class="math inline">\(l\)</span> be defined recursively as <span class="math display">\[
\mathbf{h}_l(\mathbf{h}_{l-1}, \mathbf{\theta}_l) = \sigma(\mathbf{W}_l \mathbf{h}_{l-1} + \mathbf{b}_l), \qquad l = 1, \ldots, L-1,
\]</span> where <span class="math inline">\(\mathbf{\theta}_l = (\mathrm{vec}(\mathbf{W}_l)^{\top}, \mathbf{b}_l)^{\top}\)</span> collects all parameters of layer <span class="math inline">\(l\)</span>.</p>
</div>
<div class="fragment">
<div class="columns">
<div class="column" style="width:48%;">
<p><img data-src="Figures/deep_nn.png" style="width:100.0%"></p>
<!-- <p style="text-align: center !important; margin: 0.5em 0;">
**Architecture**
</p> -->
</div><div class="column" style="width:48%;">
<p>The first hidden layer operates directly on the input: <span class="math display">\[
\mathbf{h}_0(\mathbf{x}, \mathbf{\theta}_0) = \sigma(\mathbf{W}_0 \mathbf{x} + \mathbf{b}_0),
\]</span> and the output layer is given by <span class="math display">\[
f(\mathbf{x}, \mathbf{\theta}) = \mathbf{W}_L \mathbf{h}_L + \mathbf{b}_L,
\]</span> where <span class="math inline">\(\mathbf{\theta} = (\mathbf{\theta}_0^{\top}, \ldots, \mathbf{\theta}_L^{\top})^{\top}\)</span>.</p>
</div></div>
</div>
</section>
<section id="example-composition-of-functions" class="slide level2 compact-slide">
<h2>Example: Composition of Functions</h2>
<p>To understand the expressive power of deep neural networks, consider composing a simple shallow ReLU network with itself. Let <span class="math display">\[
  f_1(x) = 2\sigma(x) - 4\sigma(x - 0.5),
\]</span> which produces a triangular function with a single kink at <span class="math inline">\(x = 0.5\)</span>.</p>
<div class="columns">
<div class="column" style="width:48%;">
<p><img data-src="Figures/triangle_comp.png" style="width:100.0%"></p>
<p style="text-align: center !important; margin: 0.5em 0;">
<strong>Realized functions <span class="math inline">\(f_k(x)\)</span> for <span class="math inline">\(k = 1, 2, 3\)</span></strong>
</p>
</div><div class="column" style="width:48%;">
<p>Composing this function with itself yields <span class="math display">\[
  f_k(x) = 2\sigma(f_{k-1}(x)) - 4\sigma(f_{k-1}(x) - 0.5).
\]</span></p>
<p>We can represent <span class="math inline">\(f_k(x)\)</span> as a DNN with <span class="math inline">\(k\)</span> hidden layers: <span class="math display">\[
  \mathbf{h}_l(x) =
  \begin{bmatrix}
    \sigma(f_{l-1}(x)) \\
    \sigma(f_{l-1}(x) - 0.5)
  \end{bmatrix}.
\]</span></p>
<div style="margin-top: 0.5em;">

</div>
<p>The <span class="text-blue"><strong>number of parameters</strong></span> required to represent <span class="math inline">\(f_k(x)\)</span> is: <span class="math display">\[
  4 + (k-1)\times 6 + 3 = 6k + 1.
\]</span></p>
</div></div>
<div class="fragment">
<p>The same function can also be represented as a shallow neural network</p>
<ul>
<li>It would require <span class="math inline">\(2^k\)</span> hidden units to capture all linear regions.</li>
<li>Depth provides an <span class="text-green"><strong>exponential gain</strong></span> in expressive efficiency.</li>
</ul>
</div>
</section>
<section id="universal-approximation-theorem-revisited" class="slide level2 compact-slide">
<h2>Universal Approximation Theorem Revisited</h2>
<p>Shallow networks are universal approximators:</p>
<ul>
<li>With enough width, they can approximate any continuous function on a compact domain.</li>
<li>A complementary result establishes that <span class="text-orange"><strong>depth</strong></span> alone can also achieve universal approximation.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<div class="fragment">
<div title="Universal Approximation Theorem Revisited">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Universal Approximation Theorem Revisited</strong></p>
</div>
<div class="callout-content">
<p>For any Lebesgue-integrable function <span class="math inline">\(f \in L^1(\mathbb{R}^n)\)</span>, there exists a ReLU deep neural network with width at most <span class="math inline">\(n + 4\)</span> in every hidden layer that approximates <span class="math inline">\(f\)</span> to arbitrary precision.</p>
<p><strong>Proof:</strong> See <span class="citation" data-cites="lu2017expressive">Lu et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>The width theorem shows that a single wide layer suffices for universal approximation</p>
<ul>
<li>The depth theorem shows that even networks of modest width can achieve the same expressive power when sufficiently deep.</li>
<li>Depth provides an alternative, more parameter-efficient, route to functional richness.</li>
</ul>
</div>
</section>
<section id="julia-implementation" class="slide level2 compact-slide">
<h2>Julia Implementation</h2>
<p>We now implement a deep neural network in Julia using <code>Lux.jl</code>.</p>
<ul>
<li>To start, we consider the function <span class="math inline">\(f_1(x) = 2\sigma(x) - 4\sigma(x - 0.5)\)</span> defined above.</li>
</ul>
<div class="fragment">
<p>The function <code>Chain</code> defines a sequence of layers.</p>
<ul>
<li>The <code>model</code> object stores the network structure and provides the parameter count.</li>
</ul>
<div id="2" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a></a><span class="im">using</span> <span class="bu">Lux</span></span>
<span id="cb1-2"><a></a>model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb1-3"><a></a>    <span class="fu">Dense</span>(<span class="fl">1</span> <span class="op">=&gt;</span> <span class="fl">2</span>, Lux.relu), <span class="co"># single input, two hidden units</span></span>
<span id="cb1-4"><a></a>    <span class="fu">Dense</span>(<span class="fl">2</span> <span class="op">=&gt;</span> <span class="fl">1</span>, identity)  <span class="co"># two hidden units, one output</span></span>
<span id="cb1-5"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div class="ansi-escaped-output">
<pre>Chain(
    layer_1 = Dense(1 =&gt; 2, relu),                <span class="ansi-bright-black-fg"># 4 parameters</span>
    layer_2 = Dense(2 =&gt; 1),                      <span class="ansi-bright-black-fg"># 3 parameters</span>
) <span class="ansi-bright-black-fg">        # Total: </span>7 parameters,
<span class="ansi-bright-black-fg">          #        plus </span>0 states.</pre>
</div>
</div>
</div>
<div style="margin-top: 1.5em;">

</div>
</div>
<div class="fragment">
<p>Next, we initialize the parameters of the network using a random number generator.</p>
<ul>
<li><code>Lux</code> stores parameters explicitly as a <span class="text-orange"><strong>nested NamedTuple</strong></span>.</li>
<li>Each layer has its own sub-tuple, or <span class="text-green"><strong>leaf</strong></span>, containing its <code>weight</code> matrix and <code>bias</code> vector.</li>
</ul>
<div id="4" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><a></a><span class="im">using</span> <span class="bu">Random</span></span>
<span id="cb2-2"><a></a>rng <span class="op">=</span> <span class="bu">Random</span>.<span class="fu">Xoshiro</span>(<span class="fl">123</span>)</span>
<span id="cb2-3"><a></a>parameters, state <span class="op">=</span> Lux.<span class="fu">setup</span>(rng, model)</span>
<span id="cb2-4"><a></a>parameters</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>(layer_1 = (weight = Float32[-3.1280737; 0.14697331;;], bias = Float32[-0.3321283, 0.17361343]), layer_2 = (weight = Float32[-1.1964471 0.95745337], bias = Float32[0.5698812]))</code></pre>
</div>
</div>
</div>
</section>
<section id="updating-the-parameters" class="slide level2 compact-slide">
<h2>Updating the Parameters</h2>
<p>To reproduce the function <span class="math inline">\(f_1(x)\)</span>, we can manually assign new values to the network parameters.</p>
<ul>
<li><code>Lux</code> allows direct updates through a named tuple with the same structure as the initialized parameters:</li>
</ul>
<div id="6" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a></a>parameters <span class="op">=</span> (layer_1 <span class="op">=</span> (weight <span class="op">=</span> [<span class="fl">1.0</span>; <span class="fl">1.0</span>;;], bias <span class="op">=</span> [<span class="fl">0.0</span>, <span class="op">-</span><span class="fl">0.5</span>]), </span>
<span id="cb4-2"><a></a>              layer_2 <span class="op">=</span> (weight <span class="op">=</span> [<span class="fl">2.0</span> <span class="op">-</span><span class="fl">4.0</span>], bias <span class="op">=</span> [<span class="fl">0.0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>(layer_1 = (weight = [1.0; 1.0;;], bias = [0.0, -0.5]), layer_2 = (weight = [2.0 -4.0], bias = [0.0]))</code></pre>
</div>
</div>
<div class="fragment">
<div style="margin-top: 0.5em;">

</div>
<p>We can now evaluate the network on a grid of inputs:</p>
<div id="8" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a></a>xgrid <span class="op">=</span> <span class="fu">collect</span>(<span class="fu">range</span>(<span class="fl">0.0</span>, <span class="fl">1.0</span>, length<span class="op">=</span><span class="fl">9</span>))</span>
<span id="cb6-2"><a></a>ygrid <span class="op">=</span> <span class="fu">model</span>(xgrid<span class="op">'</span>, parameters, state)[<span class="fl">1</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>1×9 Matrix{Float64}:
 0.0  0.25  0.5  0.75  1.0  0.75  0.5  0.25  0.0</code></pre>
</div>
</div>
<div id="10" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a></a><span class="im">using</span> <span class="bu">Plots</span>, <span class="bu">LaTeXStrings</span></span>
<span id="cb8-2"><a></a><span class="fu">plot</span>(xgrid, ygrid<span class="op">'</span>, line <span class="op">=</span> <span class="fl">3</span>, xlabel <span class="op">=</span> L<span class="st">"x"</span>, ylabel <span class="op">=</span> L<span class="st">"f_1(x)"</span>, </span>
<span id="cb8-3"><a></a>     title <span class="op">=</span> <span class="st">"Shallow Neural Network"</span>, size <span class="op">=</span> (<span class="fl">400</span>, <span class="fl">275</span>), label <span class="op">=</span> <span class="st">""</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<img src="data:image/png;base64,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">
</div>
</div>
</div>
</section>
<section id="defining-a-deep-neural-network" class="slide level2 compact-slide">
<h2>Defining a Deep Neural Network</h2>
<p>To define a deeper network, we can stack multiple hidden layers.</p>
<div id="12" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a></a>model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb9-2"><a></a>    <span class="fu">Dense</span>(<span class="fl">1</span> <span class="op">=&gt;</span> <span class="fl">2</span>, Lux.relu),</span>
<span id="cb9-3"><a></a>    <span class="fu">Dense</span>(<span class="fl">2</span> <span class="op">=&gt;</span> <span class="fl">2</span>, Lux.relu),</span>
<span id="cb9-4"><a></a>    <span class="fu">Dense</span>(<span class="fl">2</span> <span class="op">=&gt;</span> <span class="fl">1</span>, identity)</span>
<span id="cb9-5"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div class="ansi-escaped-output">
<pre>Chain(
    layer_1 = Dense(1 =&gt; 2, relu),                <span class="ansi-bright-black-fg"># 4 parameters</span>
    layer_2 = Dense(2 =&gt; 2, relu),                <span class="ansi-bright-black-fg"># 6 parameters</span>
    layer_3 = Dense(2 =&gt; 1),                      <span class="ansi-bright-black-fg"># 3 parameters</span>
) <span class="ansi-bright-black-fg">        # Total: </span>13 parameters,
<span class="ansi-bright-black-fg">          #        plus </span>0 states.</pre>
</div>
</div>
</div>
<div class="fragment">
<div id="14" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb10-1"><a></a>parameters, state <span class="op">=</span> Lux.<span class="fu">setup</span>(rng, model)</span>
<span id="cb10-2"><a></a>parameters <span class="op">=</span> (layer_1 <span class="op">=</span> (weight <span class="op">=</span> [<span class="fl">1.0</span>; <span class="fl">1.0</span>;;], bias <span class="op">=</span> [<span class="fl">0.0</span>, <span class="op">-</span><span class="fl">0.5</span>]), </span>
<span id="cb10-3"><a></a>              layer_2 <span class="op">=</span> (weight <span class="op">=</span> [<span class="fl">2.0</span> <span class="op">-</span><span class="fl">4.0</span>; <span class="fl">2.0</span> <span class="op">-</span><span class="fl">4.0</span>], bias <span class="op">=</span> [<span class="fl">0.0</span>, <span class="op">-</span><span class="fl">0.5</span>]),</span>
<span id="cb10-4"><a></a>              layer_3 <span class="op">=</span> (weight <span class="op">=</span> [<span class="fl">2.0</span> <span class="op">-</span><span class="fl">4.0</span>], bias <span class="op">=</span> [<span class="fl">0.0</span>]))</span>
<span id="cb10-5"><a></a>xgrid <span class="op">=</span> <span class="fu">collect</span>(<span class="fu">range</span>(<span class="fl">0.0</span>, <span class="fl">1.0</span>, length<span class="op">=</span><span class="fl">9</span>))</span>
<span id="cb10-6"><a></a>ygrid <span class="op">=</span> <span class="fu">model</span>(xgrid<span class="op">'</span>, parameters, state)[<span class="fl">1</span>]<span class="ch">'</span></span>
<span id="cb10-7"><a></a><span class="fu">plot</span>(xgrid, ygrid, line <span class="op">=</span> <span class="fl">3</span>, xlabel <span class="op">=</span> L<span class="st">"x"</span>, ylabel <span class="op">=</span> L<span class="st">"f_2(x)"</span>, title <span class="op">=</span> <span class="st">"Deep Neural Network"</span>, size <span class="op">=</span> (<span class="fl">400</span>, <span class="fl">275</span>), label <span class="op">=</span> <span class="st">""</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<img src="data:image/png;base64,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">
</div>
</div>
</div>
</section></section>
<section>
<section id="iii.-gradient-descent-and-its-variants" class="title-slide slide level1 compact-slide center">
<h1>III. Gradient Descent and Its Variants</h1>

</section>
<section id="stochastic-gradient-descent" class="slide level2 compact-slide">
<h2>Stochastic Gradient Descent</h2>
<p>Having defined a neural network and its parameters, the next step is to <em>train</em> the model.</p>
<ul>
<li>This often requires finding thousands (or millions) of parameters, a challenging task.</li>
<li>We discuss how to train the network using <span class="text-orange"><strong>stochastic gradient descent</strong></span> (SGD) and its variants.</li>
</ul>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 1.5em 0;">

</div>
<p>We have seen how to use <span class="text-green"><strong>gradient descent</strong></span> to minimize a loss function.</p>
<ul>
<li><p>The idea is to update the parameters in the direction of the negative gradient of the loss function: <span class="math display">\[
\nabla \mathcal{L}(\mathbf{\theta})
= \frac{1}{I} \sum_{i=1}^I \big(f(\mathbf{x}_i, \mathbf{\theta}) - y_i\big)\,
\nabla_{\mathbf{\theta}} f(\mathbf{x}_i, \mathbf{\theta}).
\]</span></p></li>
<li><p>Computing this full gradient requires evaluating all <span class="math inline">\(I\)</span> data points at each iteration, which becomes prohibitive for large datasets.</p></li>
</ul>
<div style="border-top: 1px solid #ccc; margin: 1.5em 0;">

</div>
</div>
<div class="fragment">
<p>A simple and powerful idea is to approximate the gradient using a random subset of the data.</p>
<ul>
<li>Let <span class="math inline">\(\mathcal{B} \subset \{1, \ldots, I\}\)</span> denote a randomly drawn <span class="text-blue"><strong>mini-batch</strong></span> of size <span class="math inline">\(B \ll I\)</span>.</li>
<li>The <em>stochastic gradient descent</em> (SGD) update replaces the full gradient with <span class="math display">\[
\nabla_{\mathbf{\theta}} \hat{\mathcal{L}}(\mathbf{\theta}; \mathcal{B})
= \frac{1}{B} \sum_{i \in \mathcal{B}}
\big(f(\mathbf{x}_i, \mathbf{\theta}) - y_i\big)\,\nabla_{\mathbf{\theta}} f(\mathbf{x}_i, \mathbf{\theta})
\]</span></li>
</ul>
</div>
</section>
<section id="a-simple-example" class="slide level2 compact-slide">
<h2>A simple example</h2>
<p>Consider a toy model where <span class="math inline">\(y_i = \bar{\theta} + \epsilon_i\)</span>, with <span class="math inline">\(\epsilon_i\)</span> a mean-zero disturbance and <span class="math inline">\(f(\mathbf{x}_i, \mathbf{\theta}) = \mathbf{\theta}\)</span>.</p>
<ul>
<li>In this case, the full-batch and stochastic gradients simplify to <span class="math display">\[
\nabla \mathcal{L}(\mathbf{\theta})
= \frac{1}{I} \sum_{i=1}^I (\mathbf{\theta} - y_i),
\qquad
\nabla_{\mathbf{\theta}} \hat{\mathcal{L}}(\mathbf{\theta}; \mathcal{B})
= \frac{1}{B} \sum_{i \in \mathcal{B}} (\mathbf{\theta} - y_i).
\]</span></li>
</ul>
<p>Implementing this example in Julia is straightforward.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_sgd_example" data-code-line-numbers="1-20|1-4|6-8|11-13|15-18"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_sgd_example-1"><a href="#ls_sgd_example-1"></a><span class="co"># True parameter</span></span>
<span id="ls_sgd_example-2"><a href="#ls_sgd_example-2"></a>rng <span class="op">=</span> <span class="bu">Random</span>.<span class="fu">MersenneTwister</span>(<span class="fl">123</span>)</span>
<span id="ls_sgd_example-3"><a href="#ls_sgd_example-3"></a>θ_true, sample_size, batch_size <span class="op">=</span> <span class="fl">2.0</span>, <span class="fl">100_000</span>, <span class="fl">32</span></span>
<span id="ls_sgd_example-4"><a href="#ls_sgd_example-4"></a>noisy_sample <span class="op">=</span> θ_true <span class="op">.+</span> <span class="fl">0.5</span> <span class="op">.*</span> <span class="fu">randn</span>(rng, sample_size)</span>
<span id="ls_sgd_example-5"><a href="#ls_sgd_example-5"></a></span>
<span id="ls_sgd_example-6"><a href="#ls_sgd_example-6"></a><span class="co"># Gradient functions: function and mini-batch version</span></span>
<span id="ls_sgd_example-7"><a href="#ls_sgd_example-7"></a><span class="fu">grad_full</span>(θ)   <span class="op">=</span> <span class="fl">2</span> <span class="op">*</span> (θ <span class="op">-</span> <span class="fu">mean</span>(noisy_sample))</span>
<span id="ls_sgd_example-8"><a href="#ls_sgd_example-8"></a><span class="fu">grad_sgd</span>(θ, B) <span class="op">=</span> <span class="fl">2</span> <span class="op">*</span> (θ <span class="op">-</span> <span class="fu">mean</span>(noisy_sample[<span class="fu">rand</span>(rng, <span class="fl">1</span><span class="op">:</span>sample_size, B)]))</span>
<span id="ls_sgd_example-9"><a href="#ls_sgd_example-9"></a></span>
<span id="ls_sgd_example-10"><a href="#ls_sgd_example-10"></a><span class="co"># Training loop</span></span>
<span id="ls_sgd_example-11"><a href="#ls_sgd_example-11"></a>η <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="ls_sgd_example-12"><a href="#ls_sgd_example-12"></a>θ_full, θ_sgd <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="ls_sgd_example-13"><a href="#ls_sgd_example-13"></a>θ_path_full, θ_path_sgd <span class="op">=</span> <span class="dt">Float64</span>[], <span class="dt">Float64</span>[]</span>
<span id="ls_sgd_example-14"><a href="#ls_sgd_example-14"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">200</span></span>
<span id="ls_sgd_example-15"><a href="#ls_sgd_example-15"></a>    θ_full <span class="op">-=</span> η <span class="op">*</span> <span class="fu">grad_full</span>(θ_full)</span>
<span id="ls_sgd_example-16"><a href="#ls_sgd_example-16"></a>    θ_sgd  <span class="op">-=</span> η <span class="op">*</span> <span class="fu">grad_sgd</span>(θ_sgd, batch_size)</span>
<span id="ls_sgd_example-17"><a href="#ls_sgd_example-17"></a>    <span class="fu">push!</span>(θ_path_full, θ_full)</span>
<span id="ls_sgd_example-18"><a href="#ls_sgd_example-18"></a>    <span class="fu">push!</span>(θ_path_sgd, θ_sgd)</span>
<span id="ls_sgd_example-19"><a href="#ls_sgd_example-19"></a><span class="cf">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="full-batch-gradient-descent-vs.-sgd" class="slide level2 compact-slide">
<h2>Full-batch Gradient Descent vs.&nbsp;SGD</h2>
<p>Let’s compare the <span class="text-blue"><strong>full-batch GD</strong></span> and <span class="text-orange"><strong>SGD</strong></span> in our simple example.</p>
<ul>
<li>The figure shows the trajectory of the full-batch GD (blue) and SGD (orange).</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/sgd_example.png" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p>The full-batch GD converges monotonically to the true parameter.</p>
<ul>
<li>SGD introduces noise into the updates</li>
<li>But converges to the true parameter at roughly the same rate.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<p>SGD is much more efficient than full-batch GD.</p>
<ul>
<li>Full-batch GD requires <span class="math inline">\(I = 100,000\)</span> evaluations of the gradient</li>
<li>SGD only requires <span class="math inline">\(B = 32\)</span> evaluations per iteration.</li>
</ul>
</div></div>
</section>
<section id="non-convex-loss-landscape" class="slide level2 compact-slide">
<h2>Non-convex Loss Landscape</h2>
<p>The previous example considered a convex loss function, where both full-batch GD and SGD converge to the global minimum.</p>
<ul>
<li>An additional advantage of SGD emerges in non-convex problems</li>
<li>Its inherent randomness can help escape <span class="text-orange"><strong>local minima</strong></span>.</li>
</ul>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/sgd_nonconvex_animation.gif" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<div style="margin-top: 1.5em;">

</div>
<p>To illustrate the isse, consider the non-convex loss function:</p>
<p><span class="math display">\[
  L(\theta) = \theta^4 - 3\theta^2 + \theta,
\]</span></p>
<p>The figure shows the trajectory of full-batch GD and SGD.</p>
<ul>
<li>Full-batch GD gets stuck in a local minimum.</li>
<li>SGD escapes the local minimum due to its noise.</li>
</ul>
<p>This property is especially valuable in <span class="text-blue"><strong>neural-network training</strong></span>,</p>
<ul>
<li>Loss landscapes are typically high-dimensional and non-convex.</li>
</ul>
</div></div>
</div>
</section>
<section id="momentum" class="slide level2 compact-slide">
<h2>Momentum</h2>
<p>Gradient descent and SGD can struggle when the loss surface is <span class="text-orange"><strong>anisotropic</strong></span>,</p>
<ul>
<li>That is, steep in some directions and flat in others.</li>
<li>This can cause the algorithm to oscillate and slow down.</li>
</ul>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>To see this, consider a quadratic loss function and its gradient: <span class="math display">\[
  L(\mathbf{\theta}) = \frac{1}{2}\mathbf{\theta}^T \Lambda \mathbf{\theta}, \qquad \qquad \nabla L(\mathbf{\theta}) = \Lambda \mathbf{\theta}.
\]</span> where <span class="math inline">\(\Lambda = Diag(\lambda_1, \lambda_2)\)</span> and <span class="math inline">\(0 &lt; \lambda_1 \ll \lambda_2\)</span>.</p>
</div><div class="column" style="width:50%;">
<p>The update rule for gradient descent is: <span class="math display">\[
  \theta_{n+1,j} = (1 - \eta \lambda_j)\,\theta_{n,j}.
\]</span> Convergence requires <span class="math inline">\(|1 - \eta \lambda_j| &lt; 1\)</span>, i.e., <span class="math inline">\(0 &lt; \eta &lt; 2 / \lambda_j\)</span>.</p>
</div></div>
<p>If one eigenvalue is large, <span class="math inline">\(\eta\)</span> must be small to ensure stability—causing very slow convergence</p>
<ul>
<li>Larger learning rates accelerate convergence but introduce oscillations.</li>
</ul>
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
</div>
<div class="fragment">
<p>A simple yet powerful modification is to introduce <span class="text-blue"><strong>momentum</strong></span></p>
<ul>
<li>Let <span class="math inline">\(\mathbf{m}\)</span> be an exponentially weighted moving average of past gradients:</li>
</ul>
<p><span class="math display">\[
  \mathbf{m}_{n+1} = \beta \mathbf{m}_{n} + (1-\beta)\nabla L(\mathbf{\theta}_n).
\]</span></p>
<ul>
<li>The velocity vector <span class="math inline">\(\mathbf{m}\)</span> now plays the role of the gradient in the update rule:</li>
</ul>
<p><span class="math display">\[
  \mathbf{\theta}_{n+1} = \mathbf{\theta}_n - \eta \mathbf{m}_{n+1}.
\]</span></p>
</div>
</section>
<section id="momentum-vs.-gradient-descent" class="slide level2 compact-slide">
<h2>Momentum vs.&nbsp;Gradient Descent</h2>
<p>Consider the trajectory of <span class="text-blue"><strong>gradient descent</strong></span> (blue) and <span class="text-orange"><strong>momentum</strong></span> (orange).</p>
<ul>
<li>The loss surface has very different curvatures in different directions.</li>
<li>Gradient descent oscillates and converges slowly.</li>
<li>Momentum accelerates convergence by accumulating velocity in the direction of consistent gradients.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/momentum_animation.gif" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/momentum_loss.png" style="width:100.0%"></p>
</div></div>
<p>The loss plot confirms that momentum converges much faster than gradient descent.</p>
</section>
<section id="rmsprop" class="slide level2 compact-slide">
<h2>RMSProp</h2>
<p>When the curvature of the loss function varies across parameters, using a single learning rate <span class="math inline">\(\eta\)</span> can be inefficient.</p>
<ul>
<li><span class="text-orange"><strong>RMSProp</strong></span> addresses this issue by scaling the learning rate according to the magnitude of recent gradients.</li>
</ul>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>The <em>root mean square propagation</em> (RMSProp) algorithm defines: <span class="math display">\[
  \mathbf{v}_{n+1} = \rho \mathbf{v}_{n} + (1-\rho) (\nabla L(\mathbf{\theta}_n))^2,
\]</span> where <span class="math inline">\(\odot\)</span> denotes elementwise multiplication.</p>
</div><div class="column" style="width:50%;">
<p>The update rule for RMSProp is: <span class="math display">\[
  \mathbf{\theta}_{n+1} = \mathbf{\theta}_n -
  \eta \frac{\nabla L(\mathbf{\theta}_n)}{\sqrt{\mathbf{v}_{n+1}} + \epsilon},
\]</span></p>
</div></div>
<p>Intuitively, parameters with large gradients receive smaller updates.</p>
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
</div>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/rmsprop_animation.gif" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/rmsprop_loss.png" style="width:100.0%"></p>
</div></div>
</div>
</section>
<section id="adam" class="slide level2 compact-slide">
<h2>Adam</h2>
<p>Momentum and RMSProp can be combined to yield one of the most widely used optimization algorithms in deep learning:</p>
<ul>
<li><span class="text-orange"><strong>Adam (Adaptive Moment Estimation)</strong></span> maintains exponentially decaying averages of the first and second moments of the gradients.</li>
</ul>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>The <em>Adam</em> algorithm defines: <span class="math display">\[
  \begin{align}
  \mathbf{m}_{n+1} &amp;= \beta_1 \mathbf{m}_{n} + (1 - \beta_1)\,\nabla L(\mathbf{\theta}_n),\\
  \mathbf{v}_{n+1} &amp;= \beta_2 \mathbf{v}_{n} + (1 - \beta_2)\,(\nabla L(\mathbf{\theta}_n))^{\!\odot 2},
\end{align}
\]</span> where <span class="math inline">\(\odot\)</span> denotes elementwise multiplication.</p>
</div><div class="column" style="width:50%;">
<p>The update rule for Adam is: <span class="math display">\[
  \mathbf{\theta}_{n+1}
  = \mathbf{\theta}_n
  - \eta \frac{\hat{\mathbf{m}}_{n+1}}{\sqrt{\hat{\mathbf{v}}_{n+1}} + \epsilon},
\]</span> where <span class="math inline">\(\hat{\mathbf{m}}_{n} = \frac{\mathbf{m}_{n}}{1 - \beta_1^{n}}\)</span> and <span class="math inline">\(\hat{\mathbf{v}}_{n} = \frac{\mathbf{v}_{n}}{1 - \beta_2^{n}}\)</span> are bias corrections.</p>
</div></div>
<p>Adam is often the default optimiser in deep learning frameworks.</p>
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
</div>
<div class="fragment">
<div title="AdamW">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>AdamW</strong></p>
</div>
<div class="callout-content">
<p>In practice, weight regularization is often applied together with Adam.</p>
<ul>
<li>A naïve approach adds an <span class="math inline">\(L^2\)</span> penalty directly to the gradient, which scales the regularization term by <span class="math inline">\(1/\sqrt{\hat{\mathbf{v}}_{n+1}}\)</span>, causing parameter-dependent shrinkage.</li>
<li>The <em>AdamW</em> variant proposed by <span class="citation" data-cites="LoshchilovHutter2019AdamW">Loshchilov and Hutter (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> decouples weight decay from the adaptive rescaling: <span class="math display">\[
\mathbf{\theta}_{n+1} = (1 - \eta \lambda)\mathbf{\theta}_n
- \eta \frac{\hat{\mathbf{m}}_{n+1}}{\sqrt{\hat{\mathbf{v}}_{n+1}} + \epsilon},
\]</span> where <span class="math inline">\(\lambda\)</span> is the weight decay coefficient. This decoupled formulation has become the standard default in modern deep learning frameworks.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="julia-implementation-1" class="slide level2 compact-slide">
<h2>Julia Implementation</h2>
<p>We now illustrate how to use the optimisers discussed above in Julia.</p>
<ul>
<li>All examples use the <code>Optimisers.jl</code> library, which provides efficient implementations of the algorithms described above.</li>
</ul>
<p>As an illustrative example, we consider an <span class="text-orange"><strong>anisotropic non-convex loss function</strong></span>: <span class="math display">\[
  L(\mathbf{\theta}) = \sum_{i=1}^n w_i \,\ell(\theta_i), \qquad \qquad
  \ell(\theta) = \theta^4 - 3\theta^2 + \theta, \qquad \qquad w_i\ge 0 \text{ and } \sum_{i=1}^n w_i = 1.
\]</span></p>
<div class="fragment">
<p>In Julia, we can implement this loss function as follows:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_opt_loss"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_opt_loss-1"><a href="#ls_opt_loss-1"></a><span class="fu">ℓ</span>(θ) <span class="op">=</span> θ<span class="op">^</span><span class="fl">4</span> <span class="op">-</span> <span class="fl">3</span>θ<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> θ   <span class="co"># fourth-order polynomial</span></span>
<span id="ls_opt_loss-2"><a href="#ls_opt_loss-2"></a>weights <span class="op">=</span> <span class="fu">range</span>(<span class="fl">1</span>,<span class="fl">100</span>, length <span class="op">=</span> <span class="fl">10</span>)  <span class="co"># weights </span></span>
<span id="ls_opt_loss-3"><a href="#ls_opt_loss-3"></a><span class="fu">loss</span>(θ) <span class="op">=</span> <span class="fu">dot</span>(weights, <span class="fu">ℓ</span>.(θ))<span class="op">/</span><span class="fu">sum</span>(weights) <span class="co"># loss function</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="fragment">
<p>We can use gradient descent to minimise the loss function.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_opt_loss_optimiser" data-code-line-numbers="1-14|3|4|7-8|10-11"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_opt_loss_optimiser-1"><a href="#ls_opt_loss_optimiser-1"></a><span class="co"># Loss optimisation function</span></span>
<span id="ls_opt_loss_optimiser-2"><a href="#ls_opt_loss_optimiser-2"></a><span class="kw">function</span> <span class="fu">loss_optimiser</span>(loss, θ<span class="fl">0</span>, opt; steps<span class="op">=</span><span class="fl">1_000</span>, tol<span class="op">=</span><span class="fl">1e-8</span>)</span>
<span id="ls_opt_loss_optimiser-3"><a href="#ls_opt_loss_optimiser-3"></a>    θ <span class="op">=</span> <span class="fu">deepcopy</span>(θ<span class="fl">0</span>) <span class="co"># make a copy to avoid in-place modifications</span></span>
<span id="ls_opt_loss_optimiser-4"><a href="#ls_opt_loss_optimiser-4"></a>    st  <span class="op">=</span> Optimisers.<span class="fu">setup</span>(opt, θ)  <span class="co"># builds a “state tree”</span></span>
<span id="ls_opt_loss_optimiser-5"><a href="#ls_opt_loss_optimiser-5"></a>    losses <span class="op">=</span> <span class="dt">Float64</span>[]; gnorms <span class="op">=</span> <span class="dt">Float64</span>[] </span>
<span id="ls_opt_loss_optimiser-6"><a href="#ls_opt_loss_optimiser-6"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>steps</span>
<span id="ls_opt_loss_optimiser-7"><a href="#ls_opt_loss_optimiser-7"></a>        ℓ, back <span class="op">=</span> Zygote.<span class="fu">pullback</span>(loss, θ) <span class="co"># pullback of the loss</span></span>
<span id="ls_opt_loss_optimiser-8"><a href="#ls_opt_loss_optimiser-8"></a>        g <span class="op">=</span> <span class="fu">first</span>(<span class="fu">back</span>(<span class="fl">1.0</span>)) <span class="co"># compute gradient</span></span>
<span id="ls_opt_loss_optimiser-9"><a href="#ls_opt_loss_optimiser-9"></a>        <span class="fu">push!</span>(losses, ℓ); <span class="fu">push!</span>(gnorms, <span class="fu">norm</span>(g))</span>
<span id="ls_opt_loss_optimiser-10"><a href="#ls_opt_loss_optimiser-10"></a>        <span class="cf">if</span> gnorms[<span class="kw">end</span>] <span class="op">≤</span> tol; <span class="cf">break</span>; <span class="cf">end</span> </span>
<span id="ls_opt_loss_optimiser-11"><a href="#ls_opt_loss_optimiser-11"></a>        st, θ <span class="op">=</span> Optimisers.<span class="fu">update</span>(st, θ, g) <span class="co"># update state/params</span></span>
<span id="ls_opt_loss_optimiser-12"><a href="#ls_opt_loss_optimiser-12"></a>    <span class="cf">end</span></span>
<span id="ls_opt_loss_optimiser-13"><a href="#ls_opt_loss_optimiser-13"></a>    <span class="cf">return</span> θ, (losses<span class="op">=</span>losses, grad_norms<span class="op">=</span>gnorms)</span>
<span id="ls_opt_loss_optimiser-14"><a href="#ls_opt_loss_optimiser-14"></a><span class="kw">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="changing-optimisers" class="slide level2 compact-slide">
<h2>Changing optimisers</h2>
<p>To perform the optimisation, we first define the optimizer:</p>
<div id="18" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb11-1"><a></a>η   <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb11-2"><a></a>θ<span class="fl">0</span>  <span class="op">=</span> <span class="fu">randn</span>(rng, <span class="fl">10</span>)</span>
<span id="cb11-3"><a></a>opt <span class="op">=</span> Optimisers.<span class="fu">Descent</span>(η)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>Descent(0.05)</code></pre>
</div>
</div>
<div class="fragment">
<p>We can then call the <code>loss_optimiser</code> function to perform the optimisation:</p>
<div id="20" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb13-1"><a></a>θ_opt, stats <span class="op">=</span> <span class="fu">loss_optimiser</span>(loss, θ<span class="fl">0</span>, opt, steps <span class="op">=</span> <span class="fl">1000</span>)</span>
<span id="cb13-2"><a></a>θ_opt<span class="op">'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>1×10 adjoint(::Vector{Float64}) with eltype Float64:
 0.423922  -1.30084  -1.30084  -1.30084  …  1.1309  1.1309  -1.30084  1.1309</code></pre>
</div>
</div>
</div>
<div class="fragment">
<p>To switch to a different optimizer, we simply change the <code>opt</code> variable:</p>
<div id="22" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb15-1"><a></a>opt <span class="op">=</span> Optimisers.<span class="fu">Adam</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8)</code></pre>
</div>
</div>
<p>And call the <code>loss_optimiser</code> function again:</p>
<div id="24" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb17-1"><a></a>θ_opt, stats <span class="op">=</span> <span class="fu">loss_optimiser</span>(loss, θ<span class="fl">0</span>, opt, steps <span class="op">=</span> <span class="fl">1000</span>)</span>
<span id="cb17-2"><a></a>θ_opt<span class="op">'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>1×10 adjoint(::Vector{Float64}) with eltype Float64:
 1.1221  -1.36884  -1.15011  -1.26664  …  1.1309  1.13066  -1.34121  1.13151</code></pre>
</div>
</div>
</div>
</section>
<section id="comparing-optimisers" class="slide level2 compact-slide">
<h2>Comparing optimisers</h2>

<img data-src="Figures/optimisers_comparison.png" style="width:85.0%" class="r-stretch"></section>
<section id="fitting-a-dnn" class="slide level2 compact-slide">
<h2>Fitting a DNN</h2>
<p>As a final example, we fit a deep neural network (DNN) to a nonlinear, high-dimensional function.</p>
<ul>
<li>The goal is to illustrate how a DNN can learn a complex mapping and generalize beyond the training data.</li>
</ul>
<p>Define the multivariate function: <span class="math display">\[
  f(\mathbf{x}) = \frac{1}{n} \sum_{i=1}^n p(x_i), \qquad \qquad p(x) = \prod_{i=1}^5 (x - r_i),
\]</span> where the roots <span class="math inline">\(r_i\)</span> are drawn from a standard normal distribution.</p>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>We fix <span class="math inline">\(n = 10\)</span> and draw <span class="math inline">\(100,000\)</span> sample pairs <span class="math inline">\((\mathbf{x}_i, y_i)\)</span>, where <span class="math inline">\(y_i = f(\mathbf{x}_i)\)</span>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_dnn_polynomial"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_dnn_polynomial-1"><a href="#ls_dnn_polynomial-1"></a><span class="co"># Constructing function f</span></span>
<span id="ls_dnn_polynomial-2"><a href="#ls_dnn_polynomial-2"></a>rng  <span class="op">=</span> <span class="fu">Xoshiro</span>(<span class="fl">0</span>)           <span class="co"># pseudo random number generator</span></span>
<span id="ls_dnn_polynomial-3"><a href="#ls_dnn_polynomial-3"></a>roots <span class="op">=</span> <span class="fu">randn</span>(rng, <span class="fl">5</span>)       <span class="co"># polynomial roots</span></span>
<span id="ls_dnn_polynomial-4"><a href="#ls_dnn_polynomial-4"></a><span class="fu">p</span>(x) <span class="op">=</span> <span class="fu">prod</span>(x <span class="op">.-</span> roots)     <span class="co"># univariate polynomial</span></span>
<span id="ls_dnn_polynomial-5"><a href="#ls_dnn_polynomial-5"></a><span class="fu">f</span>(x) <span class="op">=</span> <span class="fu">mean</span>(<span class="fu">p</span>.(x))          <span class="co"># multivariate version</span></span>
<span id="ls_dnn_polynomial-6"><a href="#ls_dnn_polynomial-6"></a></span>
<span id="ls_dnn_polynomial-7"><a href="#ls_dnn_polynomial-7"></a><span class="co"># Random samples</span></span>
<span id="ls_dnn_polynomial-8"><a href="#ls_dnn_polynomial-8"></a>n_states, sample_size <span class="op">=</span> <span class="fl">10</span>, <span class="fl">100_000</span></span>
<span id="ls_dnn_polynomial-9"><a href="#ls_dnn_polynomial-9"></a>x_samples <span class="op">=</span> <span class="fu">rand</span>(rng, <span class="fu">Uniform</span>(<span class="op">-</span><span class="fl">1</span>,<span class="fl">1</span>), (n_states,sample_size))</span>
<span id="ls_dnn_polynomial-10"><a href="#ls_dnn_polynomial-10"></a>y_samples <span class="op">=</span> [<span class="fu">f</span>(x_samples[<span class="op">:</span>,i]) for i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>sample_size]<span class="ch">'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="dnn-architecture" class="slide level2 compact-slide">
<h2>DNN architecture</h2>
<p>We define a DNN with two hidden layers of 32 units each and GELU activations.</p>
<div id="28" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb19-1"><a></a><span class="co"># Architecture</span></span>
<span id="cb19-2"><a></a>layers <span class="op">=</span> [n_states, <span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">1</span>]</span>
<span id="cb19-3"><a></a>model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb19-4"><a></a>    <span class="fu">Dense</span>(layers[<span class="fl">1</span>] <span class="op">=&gt;</span> layers[<span class="fl">2</span>], Lux.gelu),</span>
<span id="cb19-5"><a></a>    <span class="fu">Dense</span>(layers[<span class="fl">2</span>] <span class="op">=&gt;</span> layers[<span class="fl">3</span>], Lux.gelu),</span>
<span id="cb19-6"><a></a>    <span class="fu">Dense</span>(layers[<span class="fl">3</span>] <span class="op">=&gt;</span> layers[<span class="fl">4</span>], identity)</span>
<span id="cb19-7"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div class="ansi-escaped-output">
<pre>Chain(
    layer_1 = Dense(10 =&gt; 32, gelu_tanh),         <span class="ansi-bright-black-fg"># 352 parameters</span>
    layer_2 = Dense(32 =&gt; 32, gelu_tanh),         <span class="ansi-bright-black-fg"># 1_056 parameters</span>
    layer_3 = Dense(32 =&gt; 1),                     <span class="ansi-bright-black-fg"># 33 parameters</span>
) <span class="ansi-bright-black-fg">        # Total: </span>1_441 parameters,
<span class="ansi-bright-black-fg">          #        plus </span>0 states.</pre>
</div>
</div>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>We then initialise the parameters and choose the optimiser:</p>
<div id="30" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb20-1"><a></a>parameters, layer_states <span class="op">=</span> Lux.<span class="fu">setup</span>(rng, model)</span>
<span id="cb20-2"><a></a>opt <span class="op">=</span> Optimisers.<span class="fu">Adam</span>()</span>
<span id="cb20-3"><a></a>opt_state <span class="op">=</span> Optimisers.<span class="fu">setup</span>(opt, parameters)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div class="ansi-escaped-output">
<pre>(layer_1 = (weight = <span class="ansi-green-fg">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="ansi-green-fg">)</span>, bias = <span class="ansi-green-fg">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="ansi-green-fg">)</span>), layer_2 = (weight = <span class="ansi-green-fg">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="ansi-green-fg">)</span>, bias = <span class="ansi-green-fg">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="ansi-green-fg">)</span>), layer_3 = (weight = <span class="ansi-green-fg">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="ansi-green-fg">)</span>, bias = <span class="ansi-green-fg">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0], Float32[0.0], (0.9, 0.999))<span class="ansi-green-fg">)</span>))</pre>
</div>
</div>
</div>
</div>
</section>
<section id="dnn-training-loop" class="slide level2 compact-slide">
<h2>DNN training loop</h2>
<p>We then define the loss function:</p>
<div id="32" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb21-1"><a></a><span class="kw">function</span> <span class="fu">loss_fn</span>(parameters, layer_states)</span>
<span id="cb21-2"><a></a>    y_prediction, layer_states <span class="op">=</span> <span class="fu">model</span>(x_samples, parameters, layer_states)</span>
<span id="cb21-3"><a></a>    loss <span class="op">=</span> <span class="fu">mean</span>(abs2, y_prediction <span class="op">-</span> y_samples)</span>
<span id="cb21-4"><a></a>    <span class="cf">return</span> loss, layer_states</span>
<span id="cb21-5"><a></a><span class="kw">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>loss_fn (generic function with 1 method)</code></pre>
</div>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>We finally run the training loop:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_dnn_loop" data-code-line-numbers="1-12"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_dnn_loop-1"><a href="#ls_dnn_loop-1"></a><span class="co"># train loop</span></span>
<span id="ls_dnn_loop-2"><a href="#ls_dnn_loop-2"></a>loss_history <span class="op">=</span> []</span>
<span id="ls_dnn_loop-3"><a href="#ls_dnn_loop-3"></a>n_steps <span class="op">=</span> <span class="fl">300_000</span></span>
<span id="ls_dnn_loop-4"><a href="#ls_dnn_loop-4"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>n_steps</span>
<span id="ls_dnn_loop-5"><a href="#ls_dnn_loop-5"></a>    loss, layer_states <span class="op">=</span> <span class="fu">loss_fn</span>(parameters, layer_states)</span>
<span id="ls_dnn_loop-6"><a href="#ls_dnn_loop-6"></a>    grad <span class="op">=</span> <span class="fu">gradient</span>(<span class="fu">p-&gt;loss_fn</span>(p, layer_states)[<span class="fl">1</span>], parameters)[<span class="fl">1</span>]</span>
<span id="ls_dnn_loop-7"><a href="#ls_dnn_loop-7"></a>    opt_state, parameters <span class="op">=</span> Optimisers.<span class="fu">update</span>(opt_state, parameters, grad)</span>
<span id="ls_dnn_loop-8"><a href="#ls_dnn_loop-8"></a>    <span class="fu">push!</span>(loss_history, loss)</span>
<span id="ls_dnn_loop-9"><a href="#ls_dnn_loop-9"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="fl">5000</span> <span class="op">==</span> <span class="fl">0</span></span>
<span id="ls_dnn_loop-10"><a href="#ls_dnn_loop-10"></a>        <span class="fu">println</span>(<span class="st">"Epoch: </span><span class="sc">$</span>epoch<span class="st">, Loss: </span><span class="sc">$</span>loss<span class="st">"</span>)</span>
<span id="ls_dnn_loop-11"><a href="#ls_dnn_loop-11"></a>    <span class="cf">end</span></span>
<span id="ls_dnn_loop-12"><a href="#ls_dnn_loop-12"></a><span class="cf">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="data-and-prediction-for-the-dnn" class="slide level2 compact-slide">
<h2>Data and prediction for the DNN</h2>
<p>The figure below presents the results.</p>
<ul>
<li>The left panel shows a random subsample of 256 data points (blue) and the corresponding DNN predictions (red)</li>
<li>The two are nearly indistinguishable, indicating an excellent in-sample fit.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/dnn_scatter.png" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/dnn_polynomial.png" style="width:100.0%"></p>
</div></div>
<div class="fragment">
<p>A natural concern is that the network might simply be <em>memorizing</em> the training data.</p>
<ul>
<li>To test this, we evaluate the network in the special case <span class="math inline">\(x_1 = x_2 = \cdots = x_n = x\)</span></li>
<li>Then, <span class="math inline">\(f(\mathbf{x})\)</span> reduces to <span class="math inline">\(p(x)\)</span>, a configuration that never appears in the random training sample.</li>
</ul>
<p>Even without seeing this zero-probability case during training, the network’s predictions closely match <span class="math inline">\(f(\mathbf{x})\)</span>.</p>
</div>
</section></section>
<section>
<section id="iv.-automatic-differentiation-and-backpropagation" class="title-slide slide level1 center">
<h1>IV. Automatic Differentiation and Backpropagation</h1>

</section>
<section id="automatic-differentiation" class="slide level2 compact-slide">
<h2>Automatic Differentiation</h2>
<p>We have introduced several optimisation algorithms for training neural networks.</p>
<ul>
<li>All these methods are <em>gradient-based</em> — they rely on the derivatives of the loss function.</li>
<li>Efficient and accurate computation of gradients is a central ingredient in modern machine learning.</li>
</ul>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p><span class="text-green"><strong>Automatic differentiation</strong></span> provides a systematic way to compute these gradients.</p>
<ul>
<li>AD applies the chain rule algorithmically on a sequence of elementary operations.</li>
<li>This makes it both exact (up to machine precision) and computationally efficient.</li>
</ul>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>There are two main modes of automatic differentiation:</p>
<ul>
<li><span class="text-orange"><strong>Forward mode</strong></span> — derivatives are propagated forward from inputs to outputs.</li>
<li><span class="text-blue"><strong>Reverse mode</strong></span> — derivatives are propagated backward from outputs to inputs.</li>
</ul>
<p>Reverse mode is also known as <em>backpropagation</em> and forms the foundation of training algorithms for neural networks.</p>
<ul>
<li>We begin with forward mode AD, which is conceptually simpler and helps to build intuition.</li>
</ul>
</div>
</section>
<section id="forward-mode-automatic-differentiation" class="slide level2 compact-slide">
<h2>Forward-Mode Automatic Differentiation</h2>
<p>To build intuition for forward-mode AD, consider the first-order approximation of a function <span class="math inline">\(f(x)\)</span> around a point <span class="math inline">\(x_0\)</span>: <span class="math display">\[
  f(x) = f(x_0) + f'(x_0)\,\epsilon + \mathcal{O}(\epsilon^2),
\]</span> where <span class="math inline">\(x = x_0 + \epsilon\)</span> for a small perturbation <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(\mathcal{O}(\epsilon^2)\)</span> collects higher-order terms.</p>
<div style="margin-top: 0.5em;">

</div>
<p>A first-order approximation is therefore characterized by two quantities:</p>
<ol type="1">
<li>the function value <span class="math inline">\(f(x_0)\)</span> and</li>
<li>its derivative <span class="math inline">\(f'(x_0)\)</span>.</li>
</ol>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>The Product Rule</strong></p>
<p>Suppose we know the linear approximations of two functions, <span class="math display">\[
  f(x) = f(x_0) + f'(x_0)\,\epsilon + \mathcal{O}(\epsilon^2),
\]</span> <span class="math display">\[
  g(x) = g(x_0) + g'(x_0)\,\epsilon + \mathcal{O}(\epsilon^2).
\]</span></p>
<p>Consider the product <span class="math inline">\(h(x) = f(x)g(x)\)</span>: <span class="math display">\[
  h(x) =
  \underbrace{f(x_0) g(x_0)}_{\color{#0072b2}{h(x_0)}} +
  \underbrace{[f'(x_0) g(x_0) + f(x_0) g'(x_0)]}_{\color{#d55e00}{h'(x_0)}}\,\epsilon
  + \mathcal{O}(\epsilon^2).
\]</span></p>
</div><div class="column" style="width:50%;">
<p><strong>The Chain Rule</strong></p>
<p>Now consider a composition <span class="math inline">\(h(x) = g(f(x))\)</span>.</p>
<ul>
<li>Substituting the linear approximation of <span class="math inline">\(f(x)\)</span> into <span class="math inline">\(g(\cdot)\)</span> gives: <span class="math display">\[
\begin{align}
h(x)
&amp;= g(f(x_0) + f'(x_0)\,\epsilon + \mathcal{O}(\epsilon^2)) \\
&amp;=
\underbrace{g(f(x_0))}_{\color{#0072b2}{h(x_0)}} +
\underbrace{g'(f(x_0)) f'(x_0)}_{\color{#d55e00}{h'(x_0)}}\,\epsilon
+ \mathcal{O}(\epsilon^2),
\end{align}
\]</span> which recovers the familiar <em>chain rule</em>.</li>
</ul>
</div></div>
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
</div>
</section>
<section id="dual-numbers" class="slide level2 compact-slide">
<h2>Dual numbers</h2>
<p>Combining a few simple rules, we can compute the linear approximation of arbitrarily complex fucntion</p>
<ul>
<li>This idea can be implemented in a computer using the concept of <span class="text-orange"><strong>dual numbers</strong></span>.</li>
</ul>
<div style="margin-top: 1.5em;">

</div>
<div class="fragment">
<p>We represent a dual number as a pair of numbers: <span class="math display">\[
  u = a + b\,\epsilon,
\]</span> where the <span class="text-blue"><strong>primal part</strong></span> <span class="math inline">\(a\)</span> stores the function value and the <span class="text-green"><strong>dual part</strong></span> <span class="math inline">\(b\)</span> stores its derivative.</p>
<div style="margin-top: 1.5em;">

</div>
<p>This construction is analogous to complex numbers</p>
<ul>
<li>For a complex number <span class="math inline">\(z = a + b i\)</span>, the defining property is <span class="math inline">\(i^2 = -1\)</span>.</li>
<li>For a dual number, the defining property of the dual unit <span class="math inline">\(\epsilon\)</span> is <span class="math inline">\(\epsilon^2 = 0\)</span>.</li>
</ul>
</div>
<div class="fragment">
<div style="border-top: 1px solid #ccc; margin: 1.5em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>The Product of Dual Numbers</strong></p>
<p>Given two dual numbers, <span class="math inline">\(u = a + b\,\epsilon\)</span> and <span class="math inline">\(v = c + d\,\epsilon\)</span>, define: <span class="math display">\[
  u \times v = (a + b\,\epsilon) \times (c + d\,\epsilon) = ac + (ad + bc)\,\epsilon,
\]</span> which mirrors the product rule of derivatives.</p>
</div><div class="column" style="width:50%;">
<p><strong>Elementary Functions on Dual Numbers</strong></p>
<p>Given a real function <span class="math inline">\(g(x)\)</span>, we can extend it to dual numbers: <span class="math display">\[
  g(a + b\,\epsilon) = g(a) + g'(a)\,b\,\epsilon,
\]</span> which automatically implements the chain rule.</p>
</div></div>
<div style="border-top: 1px solid #ccc; margin: 0.5em 0;">

</div>
</div>
</section>
<section id="julia-implementation-2" class="slide level2 compact-slide">
<h2>Julia implementation</h2>
<p>We can implement a basic automatic differentiation engine in Julia with only a few lines of code.</p>
<ul>
<li>We start by defining the dual number type</li>
</ul>
<div id="34" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb23-1"><a></a><span class="co"># Define the dual number type</span></span>
<span id="cb23-2"><a></a><span class="kw">struct</span> D <span class="op">&lt;:</span><span class="dt"> Number</span></span>
<span id="cb23-3"><a></a>    v<span class="op">::</span><span class="dt">Real </span><span class="co"># primal part (value of the function)</span></span>
<span id="cb23-4"><a></a>    d<span class="op">::</span><span class="dt">Real </span><span class="co"># dual part (derivative of the function)</span></span>
<span id="cb23-5"><a></a><span class="kw">end</span></span>
<span id="cb23-6"><a></a></span>
<span id="cb23-7"><a></a>u <span class="op">=</span> <span class="fu">D</span>(<span class="fl">1.0</span>, <span class="fl">1.0</span>) <span class="co"># instantiate a dual number</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>D(1.0, 1.0)</code></pre>
</div>
</div>
<p>Given this new number type, we can now define the basic operations on dual numbers.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_dual_implementation" data-code-line-numbers="1-12|2|3-6|7-10|11-12"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_dual_implementation-1"><a href="#ls_dual_implementation-1"></a><span class="co"># How to perform basic operations on dual numbers</span></span>
<span id="ls_dual_implementation-2"><a href="#ls_dual_implementation-2"></a> <span class="im">import</span> <span class="bu">Base</span>: +,-,*,/, convert, promote_rule, exp, sin, cos, log</span>
<span id="ls_dual_implementation-3"><a href="#ls_dual_implementation-3"></a> <span class="fu">+</span>(x<span class="op">::</span><span class="dt">D</span>, y<span class="op">::</span><span class="dt">D</span>)  <span class="op">=</span> <span class="fu">D</span>(x.v <span class="op">+</span> y.v, x.d <span class="op">+</span> y.d)</span>
<span id="ls_dual_implementation-4"><a href="#ls_dual_implementation-4"></a> <span class="fu">-</span>(x<span class="op">::</span><span class="dt">D</span>, y<span class="op">::</span><span class="dt">D</span>)  <span class="op">=</span> <span class="fu">D</span>(x.v <span class="op">-</span> y.v, x.d <span class="op">-</span> y.d)</span>
<span id="ls_dual_implementation-5"><a href="#ls_dual_implementation-5"></a> <span class="fu">*</span>(x<span class="op">::</span><span class="dt">D</span>, y<span class="op">::</span><span class="dt">D</span>)  <span class="op">=</span> <span class="fu">D</span>(x.v <span class="op">*</span> y.v, x.d <span class="op">*</span> y.v <span class="op">+</span> x.v <span class="op">*</span> y.d)</span>
<span id="ls_dual_implementation-6"><a href="#ls_dual_implementation-6"></a> <span class="op">/</span>(x<span class="op">::</span><span class="dt">D</span>, y<span class="op">::</span><span class="dt">D</span>)  <span class="op">=</span> <span class="fu">D</span>(x.v <span class="op">/</span> y.v, (x.v <span class="op">*</span> y.d <span class="op">-</span> y.v <span class="op">*</span> x.d) <span class="op">/</span> y.v<span class="op">^</span><span class="fl">2</span>)</span>
<span id="ls_dual_implementation-7"><a href="#ls_dual_implementation-7"></a> <span class="fu">exp</span>(x<span class="op">::</span><span class="dt">D</span>)      <span class="op">=</span> <span class="fu">D</span>(<span class="fu">exp</span>(x.v), <span class="fu">exp</span>(x.v) <span class="op">*</span> x.d)</span>
<span id="ls_dual_implementation-8"><a href="#ls_dual_implementation-8"></a> <span class="fu">log</span>(x<span class="op">::</span><span class="dt">D</span>)      <span class="op">=</span> <span class="fu">D</span>(<span class="fu">log</span>(x.v), <span class="fl">1.0</span> <span class="op">/</span> x.v <span class="op">*</span> x.d)</span>
<span id="ls_dual_implementation-9"><a href="#ls_dual_implementation-9"></a> <span class="fu">sin</span>(x<span class="op">::</span><span class="dt">D</span>)      <span class="op">=</span> <span class="fu">D</span>(<span class="fu">sin</span>(x.v), <span class="fu">cos</span>(x.v) <span class="op">*</span> x.d)</span>
<span id="ls_dual_implementation-10"><a href="#ls_dual_implementation-10"></a> <span class="fu">cos</span>(x<span class="op">::</span><span class="dt">D</span>)      <span class="op">=</span> <span class="fu">D</span>(<span class="fu">cos</span>(x.v), <span class="fu">-sin</span>(x.v) <span class="op">*</span> x.d)</span>
<span id="ls_dual_implementation-11"><a href="#ls_dual_implementation-11"></a> <span class="fu">promote_rule</span>(<span class="op">::</span><span class="dt">Type{D}</span>, <span class="op">::</span><span class="dt">Type{&lt;:Number}</span>) <span class="op">=</span> D</span>
<span id="ls_dual_implementation-12"><a href="#ls_dual_implementation-12"></a> <span class="bu">Base</span>.<span class="fu">show</span>(io<span class="op">::</span><span class="dt">IO</span>,x<span class="op">::</span><span class="dt">D</span>) <span class="op">=</span> <span class="fu">print</span>(io,x.v,<span class="st">" + "</span>,x.d,<span class="st">" ϵ"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="computing-the-derivative-of-a-function" class="slide level2 compact-slide">
<h2>Computing the derivative of a function</h2>
<p>We can now use the dual numbers to compute the derivatives of functions.</p>
<ul>
<li>First, define our test functions</li>
</ul>
<div id="38" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb25-1"><a></a><span class="fu">f</span>(x) <span class="op">=</span> <span class="fu">exp</span>(x<span class="op">^</span><span class="fl">2</span>)</span>
<span id="cb25-2"><a></a><span class="fu">g</span>(x) <span class="op">=</span> <span class="fu">cos</span>(x<span class="op">^</span><span class="fl">3</span>)</span>
<span id="cb25-3"><a></a></span>
<span id="cb25-4"><a></a><span class="fu">fprime</span>(x) <span class="op">=</span> <span class="fu">exp</span>(x<span class="op">^</span><span class="fl">2</span>) <span class="op">*</span> <span class="fl">2</span> <span class="op">*</span> x</span>
<span id="cb25-5"><a></a><span class="fu">gprime</span>(x) <span class="op">=</span> <span class="fu">-sin</span>(x<span class="op">^</span><span class="fl">3</span>) <span class="op">*</span> <span class="fl">3</span> <span class="op">*</span> x<span class="op">^</span><span class="fl">2</span>;</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>We can now evaluate the functions at dual numbers</p>
<ul>
<li>To compute the derivative of a function <span class="math inline">\(f\)</span>, we construct <span class="math inline">\(u = x_0 + 1.0 * \epsilon\)</span>, evaluate <span class="math inline">\(f(u)\)</span>, and extract the dual part of the result.</li>
</ul>
<div id="40" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb26-1"><a></a>u, v <span class="op">=</span> <span class="fu">D</span>(<span class="fl">1.0</span>, <span class="fl">1.0</span>), <span class="fu">D</span>(<span class="fl">2.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb26-2"><a></a><span class="fu">println</span>(<span class="st">"f(u) = "</span>, <span class="fu">f</span>(u), <span class="st">" |  Exact derivative: "</span>, <span class="fu">fprime</span>(<span class="fl">1.0</span>))</span>
<span id="cb26-3"><a></a><span class="fu">println</span>(<span class="st">"g(v) = "</span>, <span class="fu">g</span>(v), <span class="st">" |  Exact derivative: "</span>, <span class="fu">gprime</span>(<span class="fl">2.0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>f(u) = 2.718281828459045 + 5.43656365691809 ϵ |  Exact derivative: 5.43656365691809
g(v) = -0.14550003380861354 + -11.872298959480581 ϵ |  Exact derivative: -11.872298959480581</code></pre>
</div>
</div>
</div>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>We can define a helper function that keeps the dual arithmetic hidden from the user.</p>
<div id="42" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb28-1"><a></a><span class="kw">function</span> <span class="fu">derivative</span>(f<span class="op">::</span><span class="dt">Function</span>, x<span class="op">::</span><span class="dt">Real</span>)</span>
<span id="cb28-2"><a></a>    u <span class="op">=</span> <span class="fu">D</span>(x, <span class="fl">1.0</span>)</span>
<span id="cb28-3"><a></a>    <span class="cf">return</span> <span class="fu">f</span>(u).d</span>
<span id="cb28-4"><a></a><span class="kw">end</span></span>
<span id="cb28-5"><a></a><span class="fu">derivative</span>(f, <span class="fl">1.0</span>), <span class="fu">derivative</span>(g, <span class="fl">2.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>(5.43656365691809, -11.872298959480581)</code></pre>
</div>
</div>
</div>
</section>
<section id="multiple-dimensions" class="slide level2 compact-slide">
<h2>Multiple dimensions</h2>
<p>We can extend the dual numbers to multiple dimensions.</p>
<ul>
<li>Suppose <span class="math inline">\(\mathbf{y} = \mathbf{f}(\mathbf{x}) \in \mathbb{R}^m\)</span> with <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. Introduce a vector dual number <span class="math display">\[
\mathbf{x} = \mathbf{x}_0 + \epsilon\,\mathbf{v},
\]</span> where <span class="math inline">\(\mathbf{x}_0 \in \mathbb{R}^n\)</span> is the primal part and <span class="math inline">\(\mathbf{v} \in \mathbb{R}^n\)</span> is the <em>tangent direction</em>.</li>
</ul>
<p>The first-order approximation of <span class="math inline">\(\mathbf{f}\)</span> is <span class="math display">\[
  \mathbf{f}(\mathbf{x})
  = \color{#0072b2}{\underbrace{\mathbf{f}(\mathbf{x}_0)}_{\text{primal}}}
    + \color{#d55e00}{\underbrace{\mathbf{Jf}(\mathbf{x}_0)\,\mathbf{v}}_{\text{Jacobian-vector product (JVP)}}}\,\epsilon
    + \mathcal{O}(\epsilon^2),
\]</span> where <span class="math inline">\(\mathbf{Jf}(\mathbf{x}_0)\in\mathbb{R}^{m\times n}\)</span> has <span class="math inline">\((i,j)\)</span> entry <span class="math inline">\(\partial f_i/\partial x_j(\mathbf{x}_0)\)</span>.</p>
<ul>
<li>Thus, applying <span class="math inline">\(\,\mathbf{f}\,\)</span> to <span class="math inline">\((\mathbf{x}_0,\mathbf{v})\)</span> returns both the value <span class="math inline">\(\mathbf{f}(\mathbf{x}_0)\)</span> and the directional derivative <span class="math inline">\(\mathbf{Jf}(\mathbf{x}_0)\mathbf{v}\)</span>.</li>
</ul>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<div title="Cost: JVP vs. full Jacobian.">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Cost: JVP vs.&nbsp;full Jacobian.</strong></p>
</div>
<div class="callout-content">
<p>For <span class="math inline">\(f:\mathbb{R}^n\to\mathbb{R}^m\)</span>, a JVP needs only a <em>single</em> forward pass with the tangent <span class="math inline">\(\mathbf{v}\)</span>: <span class="math display">\[
\text{cost(JVP)} \sim \mathcal{O}(\text{cost}(f)).
\]</span> Forming the full Jacobian by forward mode requires <span class="math inline">\(n\)</span> passes (one per input direction): <span class="math display">\[
\text{cost(Jacobian via forward mode)} \sim \mathcal{O}\!\left(n\,\text{cost}(f)\right).
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computational-efficiency-of-jvps-and-full-jacobians" class="slide level2 compact-slide">
<h2>Computational efficiency of JVPs and full Jacobians</h2>
<p>To illustrate the computational efficiency of JVPs and full Jacobians, we compare JVPs to full Jacobians on the test function <span class="math display">\[
  f_i(\mathbf{x}) = \exp\!\left(-\tfrac{1}{n}\sum_{j=1}^n \sqrt{x_j}\right) + i, \qquad \qquad
  \mathbf{f}(\mathbf{x}) = \big[f_1(\mathbf{x}), \ldots, f_m(\mathbf{x})\big]^\top \in \mathbb{R}^m.
\]</span></p>
<div class="fragment">
<p>We can implement the JVP and full Jacobian computations in Julia using the <code>ForwardDiff.jl</code> package.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_dual_jvp" data-code-line-numbers="1-11"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_dual_jvp-1"><a href="#ls_dual_jvp-1"></a><span class="co"># Test function</span></span>
<span id="ls_dual_jvp-2"><a href="#ls_dual_jvp-2"></a><span class="fu">f</span>(x; n <span class="op">=</span> <span class="fl">1</span>) <span class="op">=</span> [<span class="fu">exp</span>(<span class="fu">-mean</span>(<span class="fu">sqrt</span>.(x)))<span class="op">+</span>i for i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>n]</span>
<span id="ls_dual_jvp-3"><a href="#ls_dual_jvp-3"></a></span>
<span id="ls_dual_jvp-4"><a href="#ls_dual_jvp-4"></a><span class="co"># Compute JVP</span></span>
<span id="ls_dual_jvp-5"><a href="#ls_dual_jvp-5"></a>x, v <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>], [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>]</span>
<span id="ls_dual_jvp-6"><a href="#ls_dual_jvp-6"></a>xdual <span class="op">=</span> ForwardDiff.<span class="fu">Dual</span><span class="dt">{Float64}.</span>(x, v) <span class="co"># vector of dual numbers</span></span>
<span id="ls_dual_jvp-7"><a href="#ls_dual_jvp-7"></a>ydual <span class="op">=</span> <span class="fu">f</span>(xdual; n <span class="op">=</span> <span class="fl">2</span>) <span class="co"># evaluate function at dual numbers</span></span>
<span id="ls_dual_jvp-8"><a href="#ls_dual_jvp-8"></a>jvp <span class="op">=</span> ForwardDiff.<span class="fu">partials</span>.(ydual) <span class="co"># jvp</span></span>
<span id="ls_dual_jvp-9"><a href="#ls_dual_jvp-9"></a></span>
<span id="ls_dual_jvp-10"><a href="#ls_dual_jvp-10"></a><span class="co"># Compute Jacobian</span></span>
<span id="ls_dual_jvp-11"><a href="#ls_dual_jvp-11"></a>jac <span class="op">=</span> ForwardDiff.<span class="fu">jacobian</span>(<span class="fu">x-&gt;f</span>(x; n <span class="op">=</span> <span class="fl">2</span>), x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/forwarddiff_benchmark_input.png" style="width:80.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/forwarddiff_benchmark_output.png" style="width:80.0%"></p>
</div></div>
</div>
</section>
<section id="the-order-of-operations-matters" class="slide level2 compact-slide">
<h2>The order of operations matters</h2>
<p>The order of operations matters when computing the Jacobian of a function.</p>
<ul>
<li>Consider the composition of functions <span class="math display">\[
\mathbf{F}(\mathbf{x}) = \mathbf{f}(\mathbf{g}(\mathbf{h}(\mathbf{x}))),
\]</span> where <span class="math inline">\(\mathbf{h}:\mathbb{R}^n \!\to\! \mathbb{R}^p\)</span>, <span class="math inline">\(\mathbf{g}:\mathbb{R}^p \!\to\! \mathbb{R}^q\)</span>, and <span class="math inline">\(\mathbf{f}:\mathbb{R}^q \!\to\! \mathbb{R}^m\)</span>.</li>
</ul>
<p>The chain rule implies the Jacobian of <span class="math inline">\(\mathbf{F}\)</span> is the product of the Jacobians of <span class="math inline">\(\mathbf{f}\)</span>, <span class="math inline">\(\mathbf{g}\)</span>, and <span class="math inline">\(\mathbf{h}\)</span>: <span class="math display">\[
  \frac{\partial \mathbf{F}}{\partial \mathbf{x}^\top}
  = \color{#0072b2}{\underbrace{\frac{\partial \mathbf{f}}{\partial \mathbf{g}^\top}}_{m\times q}}
    \color{#d55e00}{\underbrace{\frac{\partial \mathbf{g}}{\partial \mathbf{h}^\top}}_{q\times p}}
    \color{#009e73}{\underbrace{\frac{\partial \mathbf{h}}{\partial \mathbf{x}^\top}}_{p\times n}}.
\]</span></p>
<div class="fragment">
<div style="margin-top: 1.5em;">

</div>
<p>There are two natural ways to evaluate the product.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Right-to-left (forward mode):</strong> <span class="math display">\[
\frac{\partial \mathbf{F}}{\partial \mathbf{x}^\top}
= \color{#0072b2}{\frac{\partial \mathbf{f}}{\partial \mathbf{g}^\top}}
  \left(\color{#d55e00}{\frac{\partial \mathbf{g}}{\partial \mathbf{h}^\top}}
  \color{#009e73}{\frac{\partial \mathbf{h}}{\partial \mathbf{x}^\top}}\right).
\]</span></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><strong>Left-to-right (reverse mode):</strong> <span class="math display">\[
\frac{\partial \mathbf{F}}{\partial \mathbf{x}^\top}
= \left(\color{#0072b2}{\frac{\partial \mathbf{f}}{\partial \mathbf{g}^\top}}
  \color{#d55e00}{\frac{\partial \mathbf{g}}{\partial \mathbf{h}^\top}}\right)
  \color{#009e73}{\frac{\partial \mathbf{h}}{\partial \mathbf{x}^\top}}.
\]</span></li>
</ul>
</div></div>
</div>
<div class="fragment">
<p>Consider the special case <span class="math inline">\(p = q = r\)</span> and <span class="math inline">\(m = 1\)</span> (a loss function for a DNN with two hidden layers of <span class="math inline">\(r\)</span> units each).</p>
<ul>
<li><em>Forward mode:</em> First term costs <span class="math inline">\(n\)</span> <em>Jacobian-vector products (JVPs)</em>, for a total cost of <span class="math inline">\(\mathcal{O}(r^2 n)\)</span> operations.</li>
<li><em>Reverse mode:</em> Total cost corresponds to two <em>vector-Jacobian products (VJPs)</em>, amounting to <span class="math inline">\(\mathcal{O}(r n)\)</span> operations.</li>
</ul>
</div>
</section>
<section id="the-computational-graph" class="slide level2 compact-slide">
<h2>The computational graph</h2>
<p>To understand how reverse mode works, it is useful to represent the function as a <em>computational graph</em>. Consider the function <span class="math display">\[
  f(x) = (x_1 + x_2)\,x_1^2.
\]</span></p>
<div class="fragment">
<p>Each operation in the function can be viewed as a node in a directed acyclic graph (DAG):</p>
<p><img data-src="Figures/comp_graph_example.png" style="width:80.0%"></p>
</div>
<div class="fragment">
<p>Reverse mode proceeds in two stages:</p>
<ol type="1">
<li>A <em>forward pass</em> to compute and store the value at each node.</li>
<li>A <em>backward pass</em> to accumulate gradients of the output with respect to each node.</li>
</ol>
</div>
</section>
<section id="forward-and-backward-passes" class="slide level2 compact-slide">
<h2>Forward and backward passes</h2>
<div class="columns">
<div class="column" style="width:35%;">
<p><img data-src="Figures/comp_graph_example.png" style="width:100.0%"></p>
</div><div class="column" style="width:65%;">
<p><span class="text-blue"><strong>Forward pass:</strong></span> <span class="math display">\[
  v_1 = x_1, \qquad v_2 = x_2, \qquad
  v_3 = v_1 + v_2, \qquad v_4 = v_1^2, \qquad v_5 = v_3 v_4.
\]</span></p>
<p>For <span class="math inline">\(x_1 = 1.0\)</span> and <span class="math inline">\(x_2 = 2.0\)</span>, we obtain <span class="math display">\[
  v_1 = 1.0, \qquad v_2 = 2.0, \qquad
  v_3 = 3.0, \qquad v_4 = 1.0, \qquad v_5 = 3.0.
\]</span></p>
</div></div>
<div class="fragment">
<p><span class="text-green"><strong>Backward pass:</strong></span></p>
<ul>
<li>We seek <span class="math inline">\(\nabla_{\mathbf{x}} f = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right)\)</span>. Define the adjoints as <span class="math inline">\(\overline{v}_i \equiv \frac{\partial f}{\partial v_i}\)</span> for <span class="math inline">\(i = 1, 2, 3, 4, 5\)</span>.</li>
<li>Starting from the output <span class="math inline">\(v_5\)</span>, we initialize its <em>adjoint</em> as <span class="math inline">\(\overline{v}_5 \equiv \frac{\partial f}{\partial v_5} = 1\)</span></li>
</ul>
<div style="border-top: 1px solid #ccc; margin: 0.0em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>The adjoints at the last nodes are <span class="math display">\[
  \overline{v}_3 = \overline{v}_5 \cdot v_4 = 1.0, \qquad
  \overline{v}_4 = \overline{v}_5 \cdot v_3 = 3.0.
\]</span></p>
</div><div class="column" style="width:50%;">
<p>Moving one step further back, the local derivatives are <span class="math display">\[\begin{equation}
    \frac{\partial v_3}{\partial v_1} = 1, \quad
    \frac{\partial v_3}{\partial v_2} = 1, \quad
    \frac{\partial v_4}{\partial v_1} = 2v_1, \quad
    \frac{\partial v_4}{\partial v_2} = 0.
  \end{equation}\]</span></p>
</div></div>
<div style="border-top: 1px solid #ccc; margin: 0.0em 0;">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>The adjoint for <span class="math inline">\(v_1\)</span> collects contributions from two branches: <span class="math display">\[
  \begin{align}
    \overline{v}_1 &amp;{+}= \overline{v}_3 \frac{\partial v_3}{\partial v_1} = 1 \cdot 1 = 1,\\
    \overline{v}_1 &amp;{+}= \overline{v}_4 \frac{\partial v_4}{\partial v_1} = 3 \cdot 2v_1 = 6.
  \end{align}
  \]</span></p>
</div><div class="column" style="width:50%;">
<p>Variable <span class="math inline">\(v_2\)</span> affects only <span class="math inline">\(v_3\)</span>, so <span class="math display">\[
    \overline{v}_2 = \overline{v}_3 \frac{\partial v_3}{\partial v_2} = 1 \cdot 1 = 1.
  \]</span> <span class="math display">\[
    \frac{\partial f}{\partial x_1} = \overline{v}_1 = 7, \qquad
    \frac{\partial f}{\partial x_2} = \overline{v}_2 = 1.
  \]</span></p>
</div></div>
<div style="border-top: 1px solid #ccc; margin: 0.0em 0;">

</div>
</div>
</section>
<section id="julia-implementation-3" class="slide level2 compact-slide">
<h2>Julia implementation</h2>
<p>We can implement reverse-mode automatic differentiation in Julia using the <code>Zygote.jl</code> package.</p>
<div id="44" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb30-1"><a></a><span class="im">using</span> <span class="bu">Zygote</span></span>
<span id="cb30-2"><a></a><span class="fu">f</span>(x₁, x₂) <span class="op">=</span> (x₁ <span class="op">+</span> x₂) <span class="op">*</span> x₁<span class="op">^</span><span class="fl">2</span></span>
<span id="cb30-3"><a></a>y, back <span class="op">=</span> Zygote.<span class="fu">pullback</span>(f, <span class="fl">1.0</span>, <span class="fl">2.0</span>)</span>
<span id="cb30-4"><a></a><span class="fu">back</span>(<span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>(7.0, 1.0)</code></pre>
</div>
</div>
<div style="margin-top: 1.5em;">

</div>
<p>Internally, <code>Zygote</code> constructs the computational graph at compile time</p>
<ul>
<li>Then, it performs the backward pass to propagate adjoints through the graph.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="ls_reverse_zygote_example" data-code-line-numbers="1-14"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="ls_reverse_zygote_example-1"><a href="#ls_reverse_zygote_example-1"></a><span class="kw">function</span> <span class="fu">pullback_manual</span>(x1, x2)</span>
<span id="ls_reverse_zygote_example-2"><a href="#ls_reverse_zygote_example-2"></a>    v1, v2 <span class="op">=</span> x1, x2</span>
<span id="ls_reverse_zygote_example-3"><a href="#ls_reverse_zygote_example-3"></a>    v3 <span class="op">=</span> v1 <span class="op">+</span> v2</span>
<span id="ls_reverse_zygote_example-4"><a href="#ls_reverse_zygote_example-4"></a>    v4 <span class="op">=</span> v1<span class="op">^</span><span class="fl">2</span></span>
<span id="ls_reverse_zygote_example-5"><a href="#ls_reverse_zygote_example-5"></a>    v5 <span class="op">=</span> v3 <span class="op">*</span> v4</span>
<span id="ls_reverse_zygote_example-6"><a href="#ls_reverse_zygote_example-6"></a>    <span class="kw">function</span> <span class="fu">back</span>(v̅<span class="fl">5</span>)</span>
<span id="ls_reverse_zygote_example-7"><a href="#ls_reverse_zygote_example-7"></a>        v̅<span class="fl">3</span> <span class="op">=</span> v̅<span class="fl">5</span> <span class="op">*</span> v4</span>
<span id="ls_reverse_zygote_example-8"><a href="#ls_reverse_zygote_example-8"></a>        v̅<span class="fl">4</span> <span class="op">=</span> v̅<span class="fl">5</span> <span class="op">*</span> v3</span>
<span id="ls_reverse_zygote_example-9"><a href="#ls_reverse_zygote_example-9"></a>        v̅<span class="fl">1</span> <span class="op">=</span> v̅<span class="fl">3</span> <span class="op">*</span> <span class="fl">1</span> <span class="op">+</span> v̅<span class="fl">4</span> <span class="op">*</span> (<span class="fl">2</span>v1)</span>
<span id="ls_reverse_zygote_example-10"><a href="#ls_reverse_zygote_example-10"></a>        v̅<span class="fl">2</span> <span class="op">=</span> v̅<span class="fl">3</span> <span class="op">*</span> <span class="fl">1</span></span>
<span id="ls_reverse_zygote_example-11"><a href="#ls_reverse_zygote_example-11"></a>        <span class="cf">return</span> (v̅<span class="fl">1</span>, v̅<span class="fl">2</span>)</span>
<span id="ls_reverse_zygote_example-12"><a href="#ls_reverse_zygote_example-12"></a>    <span class="kw">end</span></span>
<span id="ls_reverse_zygote_example-13"><a href="#ls_reverse_zygote_example-13"></a>    <span class="cf">return</span> v5, back</span>
<span id="ls_reverse_zygote_example-14"><a href="#ls_reverse_zygote_example-14"></a><span class="kw">end</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="multi-output-functions-and-vjps" class="slide level2 compact-slide">
<h2>Multi-output functions and VJPs</h2>
<p>Why does <code>Zygote.pullback</code> return a function rather than the gradient directly?</p>
<ul>
<li>This design becomes essential when dealing with multi-output functions.</li>
</ul>
<p>Consider the function <span class="math display">\[
  \mathbf{f}(\mathbf{x}) = (x_1 + x_2)x_1^2, x_1 x_2.
  \]</span> whose Jacobian is <span class="math display">\[
  \mathbf{Jf}(\mathbf{x}) = \begin{pmatrix}
    3 x_1^2 + 2 x_1 x_2 &amp; x_1^2 \\
    x_2 &amp; x_1
  \end{pmatrix}.
  \]</span></p>
<p>We can use <code>Zygote.pullback</code> to compute the gradient of <span class="math inline">\(\mathbf{f}\)</span> with respect to <span class="math inline">\(\mathbf{x}\)</span>:</p>
<div id="46" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb32-1"><a></a><span class="im">using</span> <span class="bu">Zygote</span></span>
<span id="cb32-2"><a></a><span class="fu">f</span>(x₁, x₂) <span class="op">=</span> [(x₁ <span class="op">+</span> x₂) <span class="op">*</span> x₁<span class="op">^</span><span class="fl">2</span>, x₁ <span class="op">*</span> x₂]</span>
<span id="cb32-3"><a></a>y, back <span class="op">=</span> Zygote.<span class="fu">pullback</span>(f, <span class="fl">1.0</span>, <span class="fl">2.0</span>)</span>
<span id="cb32-4"><a></a><span class="fu">back</span>([<span class="fl">1.0</span>, <span class="fl">0.0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>(7.0, 1.0)</code></pre>
</div>
</div>
<p>Here, <code>back</code> takes a vector of output adjoints and returns the <em>vector-Jacobian product</em> (VJP):</p>
<ul>
<li>This demostrates that reverse-mode AD computes VJPs efficiently with cost <span class="math inline">\(\mathcal{O}(\text{cost}(f))\)</span>—independent of the output dimension <span class="math inline">\(m\)</span>.</li>
</ul>
</section>
<section id="performance-comparison" class="slide level2 compact-slide">
<h2>Performance comparison</h2>
<p>We now illustrate the scaling properties of reverse-mode AD using <code>Zygote.jl</code>.</p>
<ul>
<li>We employ the same test functions used in the forward-mode AD performance comparison.</li>
</ul>
<p>For scalar-valued functions (left panel), the cost of a VJP is independent of the input dimension <span class="math inline">\(n\)</span></p>
<ul>
<li>For vector-valued functions (right panel), computing the full Jacobian requires one backward pass per output dimension.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="Figures/zygote_benchmark_input.png" style="width:80.0%"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="Figures/zygote_benchmark_output.png" style="width:80.0%"></p>
</div></div>
<div class="fragment">
<div title="Taking stock.">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Taking stock.</strong></p>
</div>
<div class="callout-content">
<p>Automatic differentiation provides an efficient way to compute gradients of functions.</p>
<ul>
<li>Forward-mode excels when the output dimension is large, while reverse-mode is better when the input dimension is large.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="slide level2 compact-slide smaller scrollable">
<h2>References</h2>
<div class="references">

</div>



<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-universal" class="csl-entry" role="listitem">
Cybenko, G. 1989. <span>“Approximation by Superposition of Sigmoidal Functions.”</span> <em>Mathematics of Control, Signals and Systems</em> 2 (4): 303–14.
</div>
<div id="ref-deeplearning" class="csl-entry" role="listitem">
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep <span>L</span>earning</em>. MIT Press.
</div>
<div id="ref-HastieTibshiraniFriedman2009ESL" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd ed. New York, NY: Springer. <a href="https://hastie.su.domains/ElemStatLearn/">https://hastie.su.domains/ElemStatLearn/</a>.
</div>
<div id="ref-HORNIK" class="csl-entry" role="listitem">
Hornik, Kurt. 1991. <span>“Approximation Capabilities of Multilayer Feedforward Networks.”</span> <em>Neural Networks</em> 4 (2): 251–57. https://doi.org/<a href="https://doi.org/10.1016/0893-6080(91)90009-T">https://doi.org/10.1016/0893-6080(91)90009-T</a>.
</div>
<div id="ref-imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems 25</em>, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097–1105. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.
</div>
<div id="ref-leshno1993multilayer" class="csl-entry" role="listitem">
Leshno, Moshe, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. 1993. <span>“Multilayer Feedforward Networks with a Nonpolynomial Activation Function Can Approximate Any Function.”</span> <em>Neural Networks</em> 6 (6): 861–67.
</div>
<div id="ref-LoshchilovHutter2019AdamW" class="csl-entry" role="listitem">
Loshchilov, Ilya, and Frank Hutter. 2019. <span>“Decoupled Weight Decay Regularization.”</span> In <em>International Conference on Learning Representations</em>. <a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>.
</div>
<div id="ref-lu2017expressive" class="csl-entry" role="listitem">
Lu, Zhou, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. <span>“The Expressive Power of Neural Networks: A View from the Width.”</span> In <em>Advances in Neural Information Processing Systems</em>, 30:6232–40.
</div>
<div id="ref-Prince2023UDL" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding Deep Learning</em>. Cambridge, UK: Cambridge University Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
<div id="ref-spanning_theorem" class="csl-entry" role="listitem">
Ross, Stephen A. 1976. <span>“Options and Efficiency.”</span> <em>Quarterly Journal of Economics</em> 90 (1): 75–89.
</div>
</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script>
    function greyOutPreviousFragments(slide) {
      if (!slide || slide.dataset.greyPrevious !== 'true') return;
      
      const fragments = Array
        .from(slide.querySelectorAll('.fragment[data-fragment-index]'))
        .sort(
          (a, b) =>
            (Number(a.dataset.fragmentIndex) || 0) -
            (Number(b.dataset.fragmentIndex) || 0)
        );
      
      if (!fragments.length) return;
      
      fragments.forEach(frag => {
        frag.style.transition = 'opacity 0.8s ease-in-out, color 0.8s ease-in-out';
      });
      
      const visibleFragments = fragments.filter(frag =>
        frag.classList.contains('visible') ||
        frag.classList.contains('current-fragment')
      );
      const maxVisibleIndex = visibleFragments.length
        ? Math.max(
            ...visibleFragments.map(frag => Number(frag.dataset.fragmentIndex) || 0)
          )
        : -1;
      
      fragments.forEach(frag => {
        const fragIndex = Number(frag.dataset.fragmentIndex) || 0;
        if (fragIndex < maxVisibleIndex) {
          frag.style.opacity = '0.4';
          frag.style.color = '#808080';
        } else {
          frag.style.opacity = '1';
          frag.style.color = '';
        }
      });
    }

    Reveal.addEventListener('slidechanged', function(event) {
      setTimeout(() => greyOutPreviousFragments(event.currentSlide), 100);
    });

    Reveal.addEventListener('fragmentshown', function(event) {
      const slide = event.fragment.closest('section');
      setTimeout(() => greyOutPreviousFragments(slide), 50);
    });

    Reveal.addEventListener('fragmenthidden', function(event) {
      const slide = event.fragment.closest('section');
      setTimeout(() => greyOutPreviousFragments(slide), 50);
    });

    // Also check on initial load
    Reveal.addEventListener('ready', function(event) {
      setTimeout(() => {
        document
          .querySelectorAll('section[data-grey-previous="true"]')
          .forEach(slide => greyOutPreviousFragments(slide));
      }, 100);
    });

    // Add line-by-line highlighting to code blocks
    function setupLineByLineHighlighting() {
      const allPre = document.querySelectorAll('pre code');
      allPre.forEach(code => {
        const pre = code.parentElement;
        if (pre && pre.tagName === 'PRE' && !pre.dataset.lineNumbersSetup) {
          let lineNumbers = null;
          
          // Check for vf_iteration function
          if (code.textContent.includes('function vf_iteration')) {
            lineNumbers = '1|2|3|4|5|6|7|8|9|10|11|12|13|14|15';
          }
          // Check for tauchen function
          else if (code.textContent.includes('function tauchen')) {
            lineNumbers = '1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32';
          }
          
          if (lineNumbers) {
            // Set data-line-numbers attribute (Reveal.js should handle this)
            pre.setAttribute('data-line-numbers', lineNumbers);
            
            // Add CSS for line highlighting (fallback if plugin doesn't work)
            if (!document.getElementById('line-highlight-style')) {
              const style = document.createElement('style');
              style.id = 'line-highlight-style';
              style.textContent = `
                pre[data-line-numbers] code {
                  counter-reset: line;
                }
                pre[data-line-numbers] code .hljs-line::before {
                  counter-increment: line;
                  content: counter(line);
                  display: inline-block;
                  width: 2em;
                  padding-right: 1em;
                  color: #999;
                  text-align: right;
                }
                /* Highlight current line based on fragment */
                pre[data-line-numbers] code .hljs-line.fragment.visible {
                  background-color: rgba(255, 255, 0, 0.2);
                }
              `;
              document.head.appendChild(style);
            }
            
            console.log('Set data-line-numbers attribute on code block');
            pre.dataset.lineNumbersSetup = 'true';
          }
        }
      });
    }

    // Try multiple times to catch the code when it's rendered
    function trySetupLineNumbers() {
      setupLineByLineHighlighting();
      setTimeout(setupLineByLineHighlighting, 200);
      setTimeout(setupLineByLineHighlighting, 500);
      setTimeout(setupLineByLineHighlighting, 1000);
      setTimeout(setupLineByLineHighlighting, 2000);
    }

    Reveal.addEventListener('ready', function(event) {
      console.log('Reveal.js ready, setting up line numbers');
      trySetupLineNumbers();
    });

    Reveal.addEventListener('slidechanged', function(event) {
      setTimeout(setupLineByLineHighlighting, 200);
    });

    // Also use MutationObserver to catch when code is added
    const observer = new MutationObserver(function(mutations) {
      setupLineByLineHighlighting();
    });

    observer.observe(document.body, {
      childList: true,
      subtree: true
    });

    // Add footer with link back to homepage
    function addHomepageFooter() {
      // Check if footer already exists
      if (document.getElementById('homepage-footer')) return;
      
      // All slides are in Module subdirectories, so go up one level to reach index.html
      const homePath = '../index.html';
      
      // Create footer element
      const footer = document.createElement('div');
      footer.id = 'homepage-footer';
      footer.innerHTML = '<a href="' + homePath + '" title="Back to homepage">🏠</a>';
      
      // Add to reveal container
      const revealContainer = document.querySelector('.reveal');
      if (revealContainer) {
        revealContainer.appendChild(footer);
      }
    }

    // Add footer when Reveal is ready
    if (typeof Reveal !== 'undefined') {
      Reveal.addEventListener('ready', function(event) {
        addHomepageFooter();
      });
    } else {
      // Fallback: wait for Reveal to be available
      window.addEventListener('load', function() {
        if (typeof Reveal !== 'undefined') {
          Reveal.addEventListener('ready', function(event) {
            addHomepageFooter();
          });
          // Try immediately in case Reveal is already ready
          setTimeout(addHomepageFooter, 100);
        }
      });
    }
    </script>

    <style>
    /* Footer with link back to homepage */
    .reveal #homepage-footer {
      position: fixed;
      bottom: 20px;
      right: 70px;
      z-index: 1000;
      font-size: 0.5em;
      opacity: 0.5;
      transition: opacity 0.3s ease;
    }

    .reveal #homepage-footer:hover {
      opacity: 1;
    }

    .reveal #homepage-footer a {
      color: #0072b2;
      text-decoration: none;
      padding: 3px 5px;
      background-color: rgba(255, 255, 255, 0.9);
      border-radius: 3px;
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
      display: inline-block;
      transition: background-color 0.3s ease, transform 0.2s ease;
      line-height: 1;
    }

    .reveal #homepage-footer a:hover {
      background-color: #0072b2;
      transform: scale(1.1);
      text-decoration: none;
    }

    /* Hide footer on title slide if desired */
    .reveal #title-slide ~ #homepage-footer,
    .reveal section#title-slide ~ #homepage-footer {
      display: none;
    }
    </style>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>
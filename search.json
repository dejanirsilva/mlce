[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Computational Economics",
    "section": "",
    "text": "Instructor: Dejanir Silva (Purdue University) üåê\nInstitution: EDHEC Business School",
    "crumbs": [
      "Home",
      "Machine Learning for Computational Economics"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Machine Learning for Computational Economics",
    "section": "Syllabus",
    "text": "Syllabus\nFor more details on the 2026 version of the course, you can view the syllabus here.",
    "crumbs": [
      "Home",
      "Machine Learning for Computational Economics"
    ]
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "Machine Learning for Computational Economics",
    "section": "Lecture Notes",
    "text": "Lecture Notes\nThe main reference are the lecture notes developed for this course. The notes go through the theory and implementation of the methods in detail. It includes a lot of examples and guided implementations of the different methods in Julia.\nYou can view the lecture notes in PDF format here.",
    "crumbs": [
      "Home",
      "Machine Learning for Computational Economics"
    ]
  },
  {
    "objectID": "index.html#course-slides",
    "href": "index.html#course-slides",
    "title": "Machine Learning for Computational Economics",
    "section": "Course Slides",
    "text": "Course Slides\nThe course is divided into the following modules:\n\nModule 1 ‚Äì Introduction\nModule 2 ‚Äì Discrete-Time Methods\nModule 3 ‚Äì Continuous-Time Methods\nModule 4 ‚Äì Fundamentals of Machine Learning\nModule 5 ‚Äì The Deep Policy Iteration Method",
    "crumbs": [
      "Home",
      "Machine Learning for Computational Economics"
    ]
  },
  {
    "objectID": "index.html#interactive-notebooks",
    "href": "index.html#interactive-notebooks",
    "title": "Machine Learning for Computational Economics",
    "section": "Interactive Notebooks",
    "text": "Interactive Notebooks\nInteractive Pluto notebooks with code examples and exercises:\n\nModule 2 ‚Äì Three Challenges\nModule 3 ‚Äì Black-Scholes\nModule 4 ‚Äì Fitting DNN\nModule 5 ‚Äì Two Trees",
    "crumbs": [
      "Home",
      "Machine Learning for Computational Economics"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#introduction",
    "href": "Module04/Module04_Slides.html#introduction",
    "title": "Machine Learning for Computational Economics",
    "section": "Introduction",
    "text": "Introduction\nIn this module, we introduce the fundamental concepts of machine learning.\n\nOur goal is not to provide a comprehensive review of the field\n\nSee, e.g., Hastie, Tibshirani, and Friedman (2009), Goodfellow, Bengio, and Courville (2016), and Prince (2023) for excellent reviews.\n\nBut rather to develop the core tools needed to study high-dimensional economic models.\n\n\n\n\nWe proceed in three steps.\n\nWe show how to represent functions using neural networks.\nWe explain how to estimate their parameters using stochastic gradient descent.\nWe describe how automatic differentiation enables efficient computation of the gradients.\n\n\n\n\nThe module is organized as follows:\n\nSupervised Learning and Neural Networks\nGradient Descent and Its Variants\nAutomatic Differentiation and Backpropagation",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#supervised-learning",
    "href": "Module04/Module04_Slides.html#supervised-learning",
    "title": "Machine Learning for Computational Economics",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nSupervised learning concerns the task of inferring a mapping from inputs to outputs using labeled data.\n\nIt seeks to learn a function \\(f\\) that maps inputs \\(\\mathbf{x} \\in \\mathbb{R}^d\\) to outputs \\(\\mathbf{y} \\in \\mathbb{R}^p\\), given a dataset of labeled pairs \\(\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^I\\).\nIn essence, this is a regression problem, but one that often involves extremely large input spaces and complex nonlinear dependencies.\n\n\n\n\n\nExample: Image classification.\n\nThe ImageNet dataset contains over 15 million labeled images spanning more than 22,000 categories.\nEach image is a \\(224\\times224\\times3\\) array of pixels, where each pixel takes values between 0 and 255.\n\n\n\nA linear model could, in principle, be used to predict the category of an image.\n\nHowever, such a model would treat each pixel independently: marginal effects are unrelated to neighboring pixels.\nImages, by contrast, contain strong spatial structure and nonlinear relationships between pixels across channels.\n\n\n\n\n\n\n\n\n\n\nAlexNet\n\n\nIn groundbreaking work, Krizhevsky, Sutskever, and Hinton (2012)‚Äîwidely known as AlexNet‚Äîintroduced a deep neural network architecture that achieved dramatic improvements in classification accuracy on ImageNet.\n\nTheir model had eight layers and over 60 million parameters, exceptionally large at the time.\nIt demonstrated that deep neural networks could solve complex visual tasks at scale.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#imagenet-image-and-its-rgb-channels",
    "href": "Module04/Module04_Slides.html#imagenet-image-and-its-rgb-channels",
    "title": "Machine Learning for Computational Economics",
    "section": "ImageNet Image and its RGB Channels",
    "text": "ImageNet Image and its RGB Channels",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#linear-regression",
    "href": "Module04/Module04_Slides.html#linear-regression",
    "title": "Machine Learning for Computational Economics",
    "section": "Linear Regression",
    "text": "Linear Regression\nTo illustrate the key concepts in supervised learning, it is useful to start with a familiar model: linear regression.\n\nSuppose we are given a dataset of \\(I\\) observations of the input \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and output \\(y_i \\in \\mathbb{R}\\) for \\(i = 1, \\ldots, I\\).\nGiven the input \\(\\mathbf{x}_i\\), the model prediction \\(\\hat{y}_i\\) is given by the function \\(f(\\mathbf{x}_i, \\mathbf{\\theta})\\), where \\(\\mathbf{\\theta} \\in \\mathbb{R}^d\\) is a vector of parameters. \\[\nf(\\mathbf{x}_i, \\mathbf{\\theta})\n= \\color{#0072b2}{\\underbrace{\\mathbf{w}^{\\top}}_{\\text{weights}}}\\mathbf{x}_i\n+ \\color{#d55e00}{\\underbrace{b}_{\\text{bias}}}\n\\] where \\(\\mathbf{\\theta} = (\\mathbf{w}^{\\top}, b)^{\\top}\\).\n\n\n\n\n\n\nThe function \\(f(\\mathbf{x}_i, \\mathbf{\\theta})\\) defines a family of linear functions\n\nWe obtain different functions by varying the parameters \\(\\mathbf{\\theta}\\).\n\n\n\n\nOur goal is to find the parameter values that best fit the data.\n\nTo do this, we define a loss function that measures model fit: \\[\n\\mathcal{L}(\\mathbf{\\theta}) = \\frac{1}{2I} \\sum_{i=1}^I (y_i - f(\\mathbf{x}_i, \\mathbf{\\theta}))^2.\n\\] This is the mean squared error (MSE) loss function.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#the-least-squares-estimator",
    "href": "Module04/Module04_Slides.html#the-least-squares-estimator",
    "title": "Machine Learning for Computational Economics",
    "section": "The Least Squares Estimator",
    "text": "The Least Squares Estimator\nOur goal is to find the parameters that minimize the loss function.\n\nIn the linear regression case, we can solve for the optimal parameters in closed form.\nWe need to set the gradient of the loss function to zero: \\[\n\\nabla \\mathcal{L}(\\mathbf{\\theta}) =\n\\begin{bmatrix}\n  \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} \\\\\n  \\frac{\\partial \\mathcal{L}}{\\partial b}\n\\end{bmatrix}\n= \\frac{1}{I}\\mathbf{X}^{\\top} (\\mathbf{X} \\mathbf{\\theta} - \\mathbf{y}) = \\mathbf{0}\n\\] where \\[\n\\mathbf{X} = \\begin{bmatrix}\n  \\mathbf{x}_1^{\\top} & 1 \\\\\n  \\mathbf{x}_2^{\\top} & 1 \\\\\n  \\vdots & \\vdots \\\\\n  \\mathbf{x}_I^{\\top} & 1\n\\end{bmatrix} \\in \\mathbb{R}^{I \\times d},\n\\qquad\n\\mathbf{y} =\n\\begin{bmatrix}\n  y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_I\n\\end{bmatrix} \\in \\mathbb{R}^I.\n\\]\n\n\nSolving the normal equations yields the least squares estimator: \\[\n\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}.\n\\]\n\n\n\n\n\n\n\nGradient\n\n\nWe denote the gradient of a function \\(f(\\mathbf{x})\\in \\mathbb{R}\\) with respect to \\(\\mathbf{x}\\in \\mathbb{R}^d\\) as \\(\\nabla f(\\mathbf{x})\\in \\mathbb{R}^d\\).\n\nThe gradient is a column vector, so its differential is written as \\(d f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^{\\top} d\\mathbf{x}\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#gradient-descent",
    "href": "Module04/Module04_Slides.html#gradient-descent",
    "title": "Machine Learning for Computational Economics",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nIn general, we cannot solve for the parameters in closed form.\n\nIn this case, we need to use an iterative method to find the parameters, such as gradient descent.\n\n\n\n\n\nStarting from an initial guess \\(\\mathbf{\\theta}^{(0)}\\), gradient descent updates \\(\\mathbf{\\theta}\\) in the direction opposite to the gradient of the loss function.\n\nTo see why, note that the change in the loss function is approximately given by: \\[\n\\mathcal{L}(\\mathbf{\\theta} + \\Delta \\mathbf{\\theta}) - \\mathcal{L}(\\mathbf{\\theta})\n\\approx \\nabla \\mathcal{L}(\\mathbf{\\theta})^{\\top} \\Delta \\mathbf{\\theta},\n\\qquad \\qquad\n|\\nabla \\mathcal{L}(\\mathbf{\\theta})^{\\top} \\Delta \\mathbf{\\theta}|\n\\leq \\|\\nabla \\mathcal{L}(\\mathbf{\\theta})\\|\\,\\|\\Delta \\mathbf{\\theta}\\|\n\\]\nThe largest reduction in the loss function is achieved when \\(\\Delta \\mathbf{\\theta}\\) is proportional to the negative gradient.\n\nGiven a learning rate \\(\\eta\\), the gradient descent update rule is: \\[\n\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} - \\eta \\nabla \\mathcal{L}(\\mathbf{\\theta})\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nComputational cost\n\n\nThe gradient of the loss function with respect to the parameters is given by: \\[\n\\nabla \\mathcal{L}(\\mathbf{\\theta})\n= \\frac{1}{I} \\sum_{i=1}^I\n\\big(f(\\mathbf{x}_i, \\mathbf{\\theta}) - y_i\\big)\\,\n\\nabla_{\\mathbf{\\theta}} f(\\mathbf{x}_i, \\mathbf{\\theta}).\n\\] Each iteration of gradient descent therefore requires evaluating \\(\\nabla_{\\mathbf{\\theta}} f(\\mathbf{x}_i, \\mathbf{\\theta})\\) for all \\(I\\) observations.\n\nFor large datasets, this can be computationally expensive",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#implementation-of-gradient-descent",
    "href": "Module04/Module04_Slides.html#implementation-of-gradient-descent",
    "title": "Machine Learning for Computational Economics",
    "section": "Implementation of Gradient Descent",
    "text": "Implementation of Gradient Descent\n\nAlgorithm: Gradient Descent\nInput: Initial guess \\(\\mathbf{\\theta}^{(0)}\\), learning rate \\(\\eta\\)\nOutput: Parameter estimate \\(\\mathbf{\\theta}\\)\nInitialize: \\(t \\gets 0\\)\nRepeat until \\(\\|\\nabla \\mathcal{L}(\\mathbf{\\theta}^{(t)})\\| &lt; \\epsilon\\):\n\nParameter update:\n\\(\\mathbf{\\theta}^{(t+1)} \\gets \\mathbf{\\theta}^{(t)} - \\eta \\nabla \\mathcal{L}(\\mathbf{\\theta}^{(t)})\\)\n\\(t \\gets t + 1\\)\n\nReturn: \\(\\mathbf{\\theta}^{(t)}\\)\n\n\nfunction ls_grad_descent(y::AbstractVector{&lt;:Real}, x::AbstractVector{&lt;:Real}, Œ∏‚ÇÄ::AbstractVector{&lt;:Real}; \n    learning_rate::Real=0.01, max_iter::Integer=100, Œµ::Real = 1e-4)\n    @assert length(x) == length(y)\n    I = length(x)\n    X = hcat(x, ones(I)) # design matrix\n    ‚àáf(Œ∏) = X' * (X * Œ∏ - y) / I # gradient\n    w_path, b_path =Float64[Œ∏‚ÇÄ[1]], Float64[Œ∏‚ÇÄ[2]] # initial values\n    for _ in 1:max_iter\n        g = ‚àáf([w_path[end], b_path[end]])\n        push!(w_path, w_path[end] - learning_rate * g[1])\n        push!(b_path, b_path[end] - learning_rate * g[2])\n        if norm(g) &lt; Œµ\n            break\n        end\n    end\n    return (;w = w_path, b = b_path)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#loss-surface",
    "href": "Module04/Module04_Slides.html#loss-surface",
    "title": "Machine Learning for Computational Economics",
    "section": "Loss Surface",
    "text": "Loss Surface\nThe loss surface shows the loss for different parameter values.\n\n\n\n\n\n\n\n\n\n\nWe can visualize the loss surface as a heatmap.\n\nThe minimum of the loss surface is the parameter values that best fit the data.\nThis corresponds to the darkest region in the heatmap.\n\n\n\n\nThe animation shows the gradient descent path being traced\n\nThe yellow path shows how the algorithm moves toward the minimum.\nThe red dot marks the true parameter values used to simulate the data.\nThe algorithm converges to the minimum of the loss surface.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#training-and-testing",
    "href": "Module04/Module04_Slides.html#training-and-testing",
    "title": "Machine Learning for Computational Economics",
    "section": "Training and Testing",
    "text": "Training and Testing\nIn the discussion above, we used all available data to estimate the parameters.\n\nIn practice, we typically split the data into a training set and a test set.\nThe training set is used to estimate the parameters, the test set is used to evaluate how well the model fits new data.\n\n\n\n\n\n\n\n\n\n\nConsider data generated by a noisy version of the Runge function: \\[\n  y_i = \\frac{1}{1+25 z_i^2} + \\epsilon_i,\n  \\qquad \\epsilon_i \\sim \\text{i.i.d. with } \\mathbb{E}[\\epsilon_i]=0,\n\\] with \\(z_i \\in [-1,1]\\).\n\nConsider a polynomial regression with regressors: \\[\n\\mathbf{x}_i = [z_i, z_i^2, \\ldots, z_i^{d-1}]\n\\]\nAs the degree \\(d\\) increases, the model becomes more flexible.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen \\(d = I\\), the model reaches the interpolation threshold.\n\nBut high-degree polynomial interpolants of the Runge function oscillate violently.\nThe model performs poorly on the test set for high \\(d\\).\n\nThe figure shows the training loss declines steadily as \\(d\\) increases\n\nBut the test loss eventually rises: this is the hallmark of overfitting",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#shallow-neural-networks",
    "href": "Module04/Module04_Slides.html#shallow-neural-networks",
    "title": "Machine Learning for Computational Economics",
    "section": "Shallow Neural Networks",
    "text": "Shallow Neural Networks\nWe now move from linear to nonlinear models by introducing the shallow neural network (SNN).\n\nAn SNN can be viewed as a generalization of the linear regression model\nA series of intermediate transformations combines a linear function with a nonlinear activation.\n\n\n\n\n\n\n\nHidden units: a nonlinear transformations of the inputs. \\[\nh_j(\\mathbf{x}, \\mathbf{\\theta}_j) = \\sigma(\\mathbf{w}_j^{\\top} \\mathbf{x} + b_j), \\qquad j = 0, \\ldots, n-1\n\\] where \\(\\sigma\\) is an activation function, \\(\\mathbf{\\theta}_j = (\\mathbf{w}_j^{\\top}, b_j)^{\\top}\\).\n\nThe SNN can be written as: \\[\nf(\\mathbf{x}, \\mathbf{\\theta}) = \\mathbf{w}_n^{\\top} \\mathbf{h}(\\mathbf{x}, \\mathbf{\\theta}) + b_n,\n\\] where \\(\\mathbf{\\theta} = (\\mathbf{\\theta}_0^{\\top}, \\ldots, \\mathbf{\\theta}_{n}^{\\top})^{\\top}\\) and \\(\\mathbf{\\theta}_n = (\\mathbf{w}_n^{\\top}, b_n)^{\\top}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nThe rectified linear unit (ReLU): \\[\n\\sigma(x) = \\max(0, x).\n\\]\nThis is the mostly used activation function in machine learning.\n\nWe will often focus on the ReLU.\n\n\n\n\n\n\n\n\n\nThe Gaussian Error Linear Unit (GELU): \\[\n\\sigma(x) = x \\Phi(x),\n\\] where \\(\\Phi(x)\\) is the standard normal cdf.\n\nIt has been shown to improve performance in many applications.\n\n\n\n\n\n\n\n\n\nThe hyperbolic tangent (tanh): \\[\n\\sigma(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}.\n\\] It is historically popular in recurrent networks.\n\nIt provides smooth, bounded outputs between -1 and 1.\n\n\n\n\n\n\n\n\n\nThe Sigmoid Linear Unit (SiLU): \\[\n\\sigma(x) = \\frac{x}{1 + e^{-x}}.\n\\] Also known as Swish.\n\nSiLU yields smooth gradients even for negative inputs.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#snns-architecture-and-implementation",
    "href": "Module04/Module04_Slides.html#snns-architecture-and-implementation",
    "title": "Machine Learning for Computational Economics",
    "section": "SNNs Architecture and Implementation",
    "text": "SNNs Architecture and Implementation\n\n\n\n\n\n\n\nThis figure illustrates the architecture of an SNN.\n\nGreen nodes: input layer.\nBlue nodes: hidden layer.\nOrange node: output layer.\n\nThe hidden units are also known as neurons.\n\nA neural network is a collection of neurons.\nA shallow neural network has a single hidden layer.\n\n\n\nfunction shallow_nn(x::AbstractVector{&lt;:Real}, \n    W::AbstractMatrix{&lt;:Real}, b::AbstractVector{&lt;:Real},\n    w‚Çô::AbstractVector{&lt;:Real}, b‚Çô::Real; œÉ::Function = x-&gt;max(0,x))\n    @assert size(W,2) == length(x)  # ncols of W = length of x\n    @assert size(W,1) == length(w‚Çô) # nrows of W = length of w‚Çô\n    @assert length(b) == size(W,1)  # biases for the hidden units\n    return w‚Çô' * œÉ.(W * x .+ b)+ b‚Çô\nend\n# Convenience: scalar input (d = 1); w is the column of W\nfunction shallow_nn(x::Real, \n    w::AbstractVector{&lt;:Real}, b::AbstractVector{&lt;:Real},\n    w‚Çô::AbstractVector{&lt;:Real}, b‚Çô::Real; œÉ::Function = x-&gt;max(0,x))\n    return shallow_nn([x], reshape(w,length(w),1), b, w‚Çô, b‚Çô, œÉ = œÉ)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#snns-as-piecewise-linear-functions",
    "href": "Module04/Module04_Slides.html#snns-as-piecewise-linear-functions",
    "title": "Machine Learning for Computational Economics",
    "section": "SNNs as Piecewise Linear Functions",
    "text": "SNNs as Piecewise Linear Functions\nTo illustrate the structure of an SNN, consider a one-dimensional input \\(x_i \\in [0,1]\\) and a ReLU activation function.\n\nAssume that all hidden-unit weights are \\(w_j = 1\\).\n\n\n\n\n\n\\[\nSNN: f(x_i, \\theta) = \\sum_{j=0}^{n-1} w_{n,j} \\max(0, x_i - \\hat{x}_j) + b_n,\n\\] where \\(\\hat{x}_j \\equiv - b_j\\) is a convenient reparametrization of the biases.\n\n\n\nOrdering the breakpoints as \\(0 = \\hat{x}_0 &lt; \\cdots &lt; \\hat{x}_{n-1} &lt; 1 \\equiv \\hat{x}_n\\), \\[\n    f(x_i, \\theta) = f(\\hat{x}_j, \\theta) + w_{n,j}(x_i - \\hat{x}_j),\n    \\qquad x_i \\in (\\hat{x}_j, \\hat{x}_{j+1}],\n\\] with \\(f(\\hat{x}_0, \\theta) = b_n\\).\n\n\nThe function is piecewise linear, with the breakpoints \\(\\hat{x}_j\\) learned by the model.\n\nIf we fix the breakpoints to be equally spaced, the network collapses to the locally linear interpolant from Module 3.\nHence, an SNN can be interpreted as a finite-difference method with an adaptive grid that learns where to place the nodes.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#the-adaptive-choice-of-breakpoints-in-action",
    "href": "Module04/Module04_Slides.html#the-adaptive-choice-of-breakpoints-in-action",
    "title": "Machine Learning for Computational Economics",
    "section": "The Adaptive Choice of Breakpoints in Action",
    "text": "The Adaptive Choice of Breakpoints in Action\nReLU networks adaptively place their breakpoints in regions where the target function exhibits strong nonlinearity.\n\nTo illustrate this, we fit a one‚Äìhidden‚Äìlayer ReLU network to the density of a Beta distribution\nWe choose parameters \\(\\alpha=2\\) and \\(\\beta=10\\), so the curvature is concentrated at low \\(x\\) values.\n\n\n\n\n\n\nModel fit (15 hidden units)\n\n\n\n\nBreakpoint histogram (15 hidden units)\n\n\n\n\n\n\nModel fit (30 hidden units)\n\n\n\n\nBreakpoint histogram (30 hidden units)\n\n\n\n\n\n\nModel fit (100 hidden units)\n\n\n\n\nBreakpoint histogram (100 hidden units)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#the-universal-approximation-theorem",
    "href": "Module04/Module04_Slides.html#the-universal-approximation-theorem",
    "title": "Machine Learning for Computational Economics",
    "section": "The Universal Approximation Theorem",
    "text": "The Universal Approximation Theorem\nThe piecewise‚Äìlinear structure of an SNN enables it to approximate nonlinear functions.\n\nA natural question is: how expressive is this model? What class of functions can an SNN represent or approximate?\n\n\nThe Universal Approximation Theorem provides an answer.\n\n\n\n\n\n\n\nUniversal Approximation Theorem\n\n\nAny continuous function on a compact subset of \\(\\mathbb{R}^n\\) can be approximated to arbitrary precision by a shallow neural network with any non-polynomial activation function.\nProof: See Cybenko (1989), Hornik (1991), and Leshno et al. (1993).\n\n\n\n\n\n\n\n\n\nThis result is closely related to the Option Spanning Theorem of Ross (1976)\n\nThe result states that options portfolios can replicate any continuous payoff function.\nA ReLu SNN corresponds to a portfolio of call/put options.\n\n\n\n\n\n\nThe Universal Approximation Theorem establishes that shallow networks are theoretically sufficient\n\nBut it does not imply that they are the most efficient way to approximate a function.\nAchieving high accuracy may require an exponentially large number of hidden units.\nWe turn next to a richer class of models‚Äîdeep neural networks‚Äîwhich can approximate complex functions more efficiently.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#deep-neural-networks",
    "href": "Module04/Module04_Slides.html#deep-neural-networks",
    "title": "Machine Learning for Computational Economics",
    "section": "Deep Neural Networks",
    "text": "Deep Neural Networks\nA shallow neural network contains a single hidden layer between the input and output layers.\n\nDeep neural networks (DNNs) extend this architecture by stacking multiple hidden layers on top of each other.\nEach layer applies a linear transformation followed by a nonlinear activation.\n\n\nFormally, let the \\(n_l\\)-dimensional vector of hidden units at layer \\(l\\) be defined recursively as \\[\n\\mathbf{h}_l(\\mathbf{h}_{l-1}, \\mathbf{\\theta}_l) = \\sigma(\\mathbf{W}_l \\mathbf{h}_{l-1} + \\mathbf{b}_l), \\qquad l = 1, \\ldots, L-1,\n\\] where \\(\\mathbf{\\theta}_l = (\\mathrm{vec}(\\mathbf{W}_l)^{\\top}, \\mathbf{b}_l)^{\\top}\\) collects all parameters of layer \\(l\\).\n\n\n\n\n\n\n\nThe first hidden layer operates directly on the input: \\[\n\\mathbf{h}_0(\\mathbf{x}, \\mathbf{\\theta}_0) = \\sigma(\\mathbf{W}_0 \\mathbf{x} + \\mathbf{b}_0),\n\\] and the output layer is given by \\[\nf(\\mathbf{x}, \\mathbf{\\theta}) = \\mathbf{W}_L \\mathbf{h}_L + \\mathbf{b}_L,\n\\] where \\(\\mathbf{\\theta} = (\\mathbf{\\theta}_0^{\\top}, \\ldots, \\mathbf{\\theta}_L^{\\top})^{\\top}\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#example-composition-of-functions",
    "href": "Module04/Module04_Slides.html#example-composition-of-functions",
    "title": "Machine Learning for Computational Economics",
    "section": "Example: Composition of Functions",
    "text": "Example: Composition of Functions\nTo understand the expressive power of deep neural networks, consider composing a simple shallow ReLU network with itself. Let \\[\n  f_1(x) = 2\\sigma(x) - 4\\sigma(x - 0.5),\n\\] which produces a triangular function with a single kink at \\(x = 0.5\\).\n\n\n\n\nRealized functions \\(f_k(x)\\) for \\(k = 1, 2, 3\\)\n\n\nComposing this function with itself yields \\[\n  f_k(x) = 2\\sigma(f_{k-1}(x)) - 4\\sigma(f_{k-1}(x) - 0.5).\n\\]\nWe can represent \\(f_k(x)\\) as a DNN with \\(k\\) hidden layers: \\[\n  \\mathbf{h}_l(x) =\n  \\begin{bmatrix}\n    \\sigma(f_{l-1}(x)) \\\\\n    \\sigma(f_{l-1}(x) - 0.5)\n  \\end{bmatrix}.\n\\]\n\n\n\nThe number of parameters required to represent \\(f_k(x)\\) is: \\[\n  4 + (k-1)\\times 6 + 3 = 6k + 1.\n\\]\n\n\nThe same function can also be represented as a shallow neural network\n\nIt would require \\(2^k\\) hidden units to capture all linear regions.\nDepth provides an exponential gain in expressive efficiency.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#universal-approximation-theorem-revisited",
    "href": "Module04/Module04_Slides.html#universal-approximation-theorem-revisited",
    "title": "Machine Learning for Computational Economics",
    "section": "Universal Approximation Theorem Revisited",
    "text": "Universal Approximation Theorem Revisited\nShallow networks are universal approximators:\n\nWith enough width, they can approximate any continuous function on a compact domain.\nA complementary result establishes that depth alone can also achieve universal approximation.\n\n\n\n\n\n\n\n\n\n\n\n\nUniversal Approximation Theorem Revisited\n\n\nFor any Lebesgue-integrable function \\(f \\in L^1(\\mathbb{R}^n)\\), there exists a ReLU deep neural network with width at most \\(n + 4\\) in every hidden layer that approximates \\(f\\) to arbitrary precision.\nProof: See Lu et al. (2017).\n\n\n\n\n\n\n\n\n\nThe width theorem shows that a single wide layer suffices for universal approximation\n\nThe depth theorem shows that even networks of modest width can achieve the same expressive power when sufficiently deep.\nDepth provides an alternative, more parameter-efficient, route to functional richness.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#julia-implementation",
    "href": "Module04/Module04_Slides.html#julia-implementation",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia Implementation",
    "text": "Julia Implementation\nWe now implement a deep neural network in Julia using Lux.jl.\n\nTo start, we consider the function \\(f_1(x) = 2\\sigma(x) - 4\\sigma(x - 0.5)\\) defined above.\n\n\nThe function Chain defines a sequence of layers.\n\nThe model object stores the network structure and provides the parameter count.\n\n\nusing Lux\nmodel = Chain(\n    Dense(1 =&gt; 2, Lux.relu), # single input, two hidden units\n    Dense(2 =&gt; 1, identity)  # two hidden units, one output\n)\n\n\nChain(\n    layer_1 = Dense(1 =&gt; 2, relu),                # 4 parameters\n    layer_2 = Dense(2 =&gt; 1),                      # 3 parameters\n)         # Total: 7 parameters,\n          #        plus 0 states.\n\n\n\n\n\n\n\n\nNext, we initialize the parameters of the network using a random number generator.\n\nLux stores parameters explicitly as a nested NamedTuple.\nEach layer has its own sub-tuple, or leaf, containing its weight matrix and bias vector.\n\n\nusing Random\nrng = Random.Xoshiro(123)\nparameters, state = Lux.setup(rng, model)\nparameters\n\n(layer_1 = (weight = Float32[-3.1280737; 0.14697331;;], bias = Float32[-0.3321283, 0.17361343]), layer_2 = (weight = Float32[-1.1964471 0.95745337], bias = Float32[0.5698812]))",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#updating-the-parameters",
    "href": "Module04/Module04_Slides.html#updating-the-parameters",
    "title": "Machine Learning for Computational Economics",
    "section": "Updating the Parameters",
    "text": "Updating the Parameters\nTo reproduce the function \\(f_1(x)\\), we can manually assign new values to the network parameters.\n\nLux allows direct updates through a named tuple with the same structure as the initialized parameters:\n\n\nparameters = (layer_1 = (weight = [1.0; 1.0;;], bias = [0.0, -0.5]), \n              layer_2 = (weight = [2.0 -4.0], bias = [0.0]))\n\n(layer_1 = (weight = [1.0; 1.0;;], bias = [0.0, -0.5]), layer_2 = (weight = [2.0 -4.0], bias = [0.0]))\n\n\n\n\n\n\nWe can now evaluate the network on a grid of inputs:\n\nxgrid = collect(range(0.0, 1.0, length=9))\nygrid = model(xgrid', parameters, state)[1]\n\n1√ó9 Matrix{Float64}:\n 0.0  0.25  0.5  0.75  1.0  0.75  0.5  0.25  0.0\n\n\n\nusing Plots, LaTeXStrings\nplot(xgrid, ygrid', line = 3, xlabel = L\"x\", ylabel = L\"f_1(x)\", \n     title = \"Shallow Neural Network\", size = (400, 275), label = \"\")",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#defining-a-deep-neural-network",
    "href": "Module04/Module04_Slides.html#defining-a-deep-neural-network",
    "title": "Machine Learning for Computational Economics",
    "section": "Defining a Deep Neural Network",
    "text": "Defining a Deep Neural Network\nTo define a deeper network, we can stack multiple hidden layers.\n\nmodel = Chain(\n    Dense(1 =&gt; 2, Lux.relu),\n    Dense(2 =&gt; 2, Lux.relu),\n    Dense(2 =&gt; 1, identity)\n)\n\n\nChain(\n    layer_1 = Dense(1 =&gt; 2, relu),                # 4 parameters\n    layer_2 = Dense(2 =&gt; 2, relu),                # 6 parameters\n    layer_3 = Dense(2 =&gt; 1),                      # 3 parameters\n)         # Total: 13 parameters,\n          #        plus 0 states.\n\n\n\n\n\nparameters, state = Lux.setup(rng, model)\nparameters = (layer_1 = (weight = [1.0; 1.0;;], bias = [0.0, -0.5]), \n              layer_2 = (weight = [2.0 -4.0; 2.0 -4.0], bias = [0.0, -0.5]),\n              layer_3 = (weight = [2.0 -4.0], bias = [0.0]))\nxgrid = collect(range(0.0, 1.0, length=9))\nygrid = model(xgrid', parameters, state)[1]'\nplot(xgrid, ygrid, line = 3, xlabel = L\"x\", ylabel = L\"f_2(x)\", title = \"Deep Neural Network\", size = (400, 275), label = \"\")",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#stochastic-gradient-descent",
    "href": "Module04/Module04_Slides.html#stochastic-gradient-descent",
    "title": "Machine Learning for Computational Economics",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nHaving defined a neural network and its parameters, the next step is to train the model.\n\nThis often requires finding thousands (or millions) of parameters, a challenging task.\nWe discuss how to train the network using stochastic gradient descent (SGD) and its variants.\n\n\n\n\n\nWe have seen how to use gradient descent to minimize a loss function.\n\nThe idea is to update the parameters in the direction of the negative gradient of the loss function: \\[\n\\nabla \\mathcal{L}(\\mathbf{\\theta})\n= \\frac{1}{I} \\sum_{i=1}^I \\big(f(\\mathbf{x}_i, \\mathbf{\\theta}) - y_i\\big)\\,\n\\nabla_{\\mathbf{\\theta}} f(\\mathbf{x}_i, \\mathbf{\\theta}).\n\\]\nComputing this full gradient requires evaluating all \\(I\\) data points at each iteration, which becomes prohibitive for large datasets.\n\n\n\n\n\n\nA simple and powerful idea is to approximate the gradient using a random subset of the data.\n\nLet \\(\\mathcal{B} \\subset \\{1, \\ldots, I\\}\\) denote a randomly drawn mini-batch of size \\(B \\ll I\\).\nThe stochastic gradient descent (SGD) update replaces the full gradient with \\[\n\\nabla_{\\mathbf{\\theta}} \\hat{\\mathcal{L}}(\\mathbf{\\theta}; \\mathcal{B})\n= \\frac{1}{B} \\sum_{i \\in \\mathcal{B}}\n\\big(f(\\mathbf{x}_i, \\mathbf{\\theta}) - y_i\\big)\\,\\nabla_{\\mathbf{\\theta}} f(\\mathbf{x}_i, \\mathbf{\\theta})\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#a-simple-example",
    "href": "Module04/Module04_Slides.html#a-simple-example",
    "title": "Machine Learning for Computational Economics",
    "section": "A simple example",
    "text": "A simple example\nConsider a toy model where \\(y_i = \\bar{\\theta} + \\epsilon_i\\), with \\(\\epsilon_i\\) a mean-zero disturbance and \\(f(\\mathbf{x}_i, \\mathbf{\\theta}) = \\mathbf{\\theta}\\).\n\nIn this case, the full-batch and stochastic gradients simplify to \\[\n\\nabla \\mathcal{L}(\\mathbf{\\theta})\n= \\frac{1}{I} \\sum_{i=1}^I (\\mathbf{\\theta} - y_i),\n\\qquad\n\\nabla_{\\mathbf{\\theta}} \\hat{\\mathcal{L}}(\\mathbf{\\theta}; \\mathcal{B})\n= \\frac{1}{B} \\sum_{i \\in \\mathcal{B}} (\\mathbf{\\theta} - y_i).\n\\]\n\nImplementing this example in Julia is straightforward.\n# True parameter\nrng = Random.MersenneTwister(123)\nŒ∏_true, sample_size, batch_size = 2.0, 100_000, 32\nnoisy_sample = Œ∏_true .+ 0.5 .* randn(rng, sample_size)\n\n# Gradient functions: function and mini-batch version\ngrad_full(Œ∏)   = 2 * (Œ∏ - mean(noisy_sample))\ngrad_sgd(Œ∏, B) = 2 * (Œ∏ - mean(noisy_sample[rand(rng, 1:sample_size, B)]))\n\n# Training loop\nŒ∑ = 0.05\nŒ∏_full, Œ∏_sgd = 0.0, 0.0\nŒ∏_path_full, Œ∏_path_sgd = Float64[], Float64[]\nfor t in 1:200\n    Œ∏_full -= Œ∑ * grad_full(Œ∏_full)\n    Œ∏_sgd  -= Œ∑ * grad_sgd(Œ∏_sgd, batch_size)\n    push!(Œ∏_path_full, Œ∏_full)\n    push!(Œ∏_path_sgd, Œ∏_sgd)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#full-batch-gradient-descent-vs.-sgd",
    "href": "Module04/Module04_Slides.html#full-batch-gradient-descent-vs.-sgd",
    "title": "Machine Learning for Computational Economics",
    "section": "Full-batch Gradient Descent vs.¬†SGD",
    "text": "Full-batch Gradient Descent vs.¬†SGD\nLet‚Äôs compare the full-batch GD and SGD in our simple example.\n\nThe figure shows the trajectory of the full-batch GD (blue) and SGD (orange).\n\n\n\n\n\nThe full-batch GD converges monotonically to the true parameter.\n\nSGD introduces noise into the updates\nBut converges to the true parameter at roughly the same rate.\n\n\n\n\nSGD is much more efficient than full-batch GD.\n\nFull-batch GD requires \\(I = 100,000\\) evaluations of the gradient\nSGD only requires \\(B = 32\\) evaluations per iteration.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#non-convex-loss-landscape",
    "href": "Module04/Module04_Slides.html#non-convex-loss-landscape",
    "title": "Machine Learning for Computational Economics",
    "section": "Non-convex Loss Landscape",
    "text": "Non-convex Loss Landscape\nThe previous example considered a convex loss function, where both full-batch GD and SGD converge to the global minimum.\n\nAn additional advantage of SGD emerges in non-convex problems\nIts inherent randomness can help escape local minima.\n\n\n\n\n\n\n\n\n\nTo illustrate the isse, consider the non-convex loss function:\n\\[\n  L(\\theta) = \\theta^4 - 3\\theta^2 + \\theta,\n\\]\nThe figure shows the trajectory of full-batch GD and SGD.\n\nFull-batch GD gets stuck in a local minimum.\nSGD escapes the local minimum due to its noise.\n\nThis property is especially valuable in neural-network training,\n\nLoss landscapes are typically high-dimensional and non-convex.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#momentum",
    "href": "Module04/Module04_Slides.html#momentum",
    "title": "Machine Learning for Computational Economics",
    "section": "Momentum",
    "text": "Momentum\nGradient descent and SGD can struggle when the loss surface is anisotropic,\n\nThat is, steep in some directions and flat in others.\nThis can cause the algorithm to oscillate and slow down.\n\n\n\n\n\n\n\nTo see this, consider a quadratic loss function and its gradient: \\[\n  L(\\mathbf{\\theta}) = \\frac{1}{2}\\mathbf{\\theta}^T \\Lambda \\mathbf{\\theta}, \\qquad \\qquad \\nabla L(\\mathbf{\\theta}) = \\Lambda \\mathbf{\\theta}.\n\\] where \\(\\Lambda = Diag(\\lambda_1, \\lambda_2)\\) and \\(0 &lt; \\lambda_1 \\ll \\lambda_2\\).\n\nThe update rule for gradient descent is: \\[\n  \\theta_{n+1,j} = (1 - \\eta \\lambda_j)\\,\\theta_{n,j}.\n\\] Convergence requires \\(|1 - \\eta \\lambda_j| &lt; 1\\), i.e., \\(0 &lt; \\eta &lt; 2 / \\lambda_j\\).\n\nIf one eigenvalue is large, \\(\\eta\\) must be small to ensure stability‚Äîcausing very slow convergence\n\nLarger learning rates accelerate convergence but introduce oscillations.\n\n\n\n\n\n\nA simple yet powerful modification is to introduce momentum\n\nLet \\(\\mathbf{m}\\) be an exponentially weighted moving average of past gradients:\n\n\\[\n  \\mathbf{m}_{n+1} = \\beta \\mathbf{m}_{n} + (1-\\beta)\\nabla L(\\mathbf{\\theta}_n).\n\\]\n\nThe velocity vector \\(\\mathbf{m}\\) now plays the role of the gradient in the update rule:\n\n\\[\n  \\mathbf{\\theta}_{n+1} = \\mathbf{\\theta}_n - \\eta \\mathbf{m}_{n+1}.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#momentum-vs.-gradient-descent",
    "href": "Module04/Module04_Slides.html#momentum-vs.-gradient-descent",
    "title": "Machine Learning for Computational Economics",
    "section": "Momentum vs.¬†Gradient Descent",
    "text": "Momentum vs.¬†Gradient Descent\nConsider the trajectory of gradient descent (blue) and momentum (orange).\n\nThe loss surface has very different curvatures in different directions.\nGradient descent oscillates and converges slowly.\nMomentum accelerates convergence by accumulating velocity in the direction of consistent gradients.\n\n\n\n\n\n\n\nThe loss plot confirms that momentum converges much faster than gradient descent.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#rmsprop",
    "href": "Module04/Module04_Slides.html#rmsprop",
    "title": "Machine Learning for Computational Economics",
    "section": "RMSProp",
    "text": "RMSProp\nWhen the curvature of the loss function varies across parameters, using a single learning rate \\(\\eta\\) can be inefficient.\n\nRMSProp addresses this issue by scaling the learning rate according to the magnitude of recent gradients.\n\n\n\n\n\n\n\nThe root mean square propagation (RMSProp) algorithm defines: \\[\n  \\mathbf{v}_{n+1} = \\rho \\mathbf{v}_{n} + (1-\\rho) (\\nabla L(\\mathbf{\\theta}_n))^2,\n\\] where \\(\\odot\\) denotes elementwise multiplication.\n\nThe update rule for RMSProp is: \\[\n  \\mathbf{\\theta}_{n+1} = \\mathbf{\\theta}_n -\n  \\eta \\frac{\\nabla L(\\mathbf{\\theta}_n)}{\\sqrt{\\mathbf{v}_{n+1}} + \\epsilon},\n\\]\n\nIntuitively, parameters with large gradients receive smaller updates.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#adam",
    "href": "Module04/Module04_Slides.html#adam",
    "title": "Machine Learning for Computational Economics",
    "section": "Adam",
    "text": "Adam\nMomentum and RMSProp can be combined to yield one of the most widely used optimization algorithms in deep learning:\n\nAdam (Adaptive Moment Estimation) maintains exponentially decaying averages of the first and second moments of the gradients.\n\n\n\n\n\n\n\nThe Adam algorithm defines: \\[\n  \\begin{align}\n  \\mathbf{m}_{n+1} &= \\beta_1 \\mathbf{m}_{n} + (1 - \\beta_1)\\,\\nabla L(\\mathbf{\\theta}_n),\\\\\n  \\mathbf{v}_{n+1} &= \\beta_2 \\mathbf{v}_{n} + (1 - \\beta_2)\\,(\\nabla L(\\mathbf{\\theta}_n))^{\\!\\odot 2},\n\\end{align}\n\\] where \\(\\odot\\) denotes elementwise multiplication.\n\nThe update rule for Adam is: \\[\n  \\mathbf{\\theta}_{n+1}\n  = \\mathbf{\\theta}_n\n  - \\eta \\frac{\\hat{\\mathbf{m}}_{n+1}}{\\sqrt{\\hat{\\mathbf{v}}_{n+1}} + \\epsilon},\n\\] where \\(\\hat{\\mathbf{m}}_{n} = \\frac{\\mathbf{m}_{n}}{1 - \\beta_1^{n}}\\) and \\(\\hat{\\mathbf{v}}_{n} = \\frac{\\mathbf{v}_{n}}{1 - \\beta_2^{n}}\\) are bias corrections.\n\nAdam is often the default optimiser in deep learning frameworks.\n\n\n\n\n\n\n\n\n\n\n\n\nAdamW\n\n\nIn practice, weight regularization is often applied together with Adam.\n\nA na√Øve approach adds an \\(L^2\\) penalty directly to the gradient, which scales the regularization term by \\(1/\\sqrt{\\hat{\\mathbf{v}}_{n+1}}\\), causing parameter-dependent shrinkage.\nThe AdamW variant proposed by Loshchilov and Hutter (2019) decouples weight decay from the adaptive rescaling: \\[\n\\mathbf{\\theta}_{n+1} = (1 - \\eta \\lambda)\\mathbf{\\theta}_n\n- \\eta \\frac{\\hat{\\mathbf{m}}_{n+1}}{\\sqrt{\\hat{\\mathbf{v}}_{n+1}} + \\epsilon},\n\\] where \\(\\lambda\\) is the weight decay coefficient. This decoupled formulation has become the standard default in modern deep learning frameworks.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#julia-implementation-1",
    "href": "Module04/Module04_Slides.html#julia-implementation-1",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia Implementation",
    "text": "Julia Implementation\nWe now illustrate how to use the optimisers discussed above in Julia.\n\nAll examples use the Optimisers.jl library, which provides efficient implementations of the algorithms described above.\n\nAs an illustrative example, we consider an anisotropic non-convex loss function: \\[\n  L(\\mathbf{\\theta}) = \\sum_{i=1}^n w_i \\,\\ell(\\theta_i), \\qquad \\qquad\n  \\ell(\\theta) = \\theta^4 - 3\\theta^2 + \\theta, \\qquad \\qquad w_i\\ge 0 \\text{ and } \\sum_{i=1}^n w_i = 1.\n\\]\n\nIn Julia, we can implement this loss function as follows:\n‚Ñì(Œ∏) = Œ∏^4 - 3Œ∏^2 + Œ∏   # fourth-order polynomial\nweights = range(1,100, length = 10)  # weights \nloss(Œ∏) = dot(weights, ‚Ñì.(Œ∏))/sum(weights) # loss function\n\n\nWe can use gradient descent to minimise the loss function.\n# Loss optimisation function\nfunction loss_optimiser(loss, Œ∏0, opt; steps=1_000, tol=1e-8)\n    Œ∏ = deepcopy(Œ∏0) # make a copy to avoid in-place modifications\n    st  = Optimisers.setup(opt, Œ∏)  # builds a ‚Äústate tree‚Äù\n    losses = Float64[]; gnorms = Float64[] \n    for _ in 1:steps\n        ‚Ñì, back = Zygote.pullback(loss, Œ∏) # pullback of the loss\n        g = first(back(1.0)) # compute gradient\n        push!(losses, ‚Ñì); push!(gnorms, norm(g))\n        if gnorms[end] ‚â§ tol; break; end \n        st, Œ∏ = Optimisers.update(st, Œ∏, g) # update state/params\n    end\n    return Œ∏, (losses=losses, grad_norms=gnorms)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#changing-optimisers",
    "href": "Module04/Module04_Slides.html#changing-optimisers",
    "title": "Machine Learning for Computational Economics",
    "section": "Changing optimisers",
    "text": "Changing optimisers\nTo perform the optimisation, we first define the optimizer:\n\nŒ∑   = 0.05\nŒ∏0  = randn(rng, 10)\nopt = Optimisers.Descent(Œ∑)\n\nDescent(0.05)\n\n\n\nWe can then call the loss_optimiser function to perform the optimisation:\n\nŒ∏_opt, stats = loss_optimiser(loss, Œ∏0, opt, steps = 1000)\nŒ∏_opt'\n\n1√ó10 adjoint(::Vector{Float64}) with eltype Float64:\n 1.15324  1.13091  1.1309  1.1309  ‚Ä¶  1.1309  -1.30084  1.1309  -1.30084\n\n\n\n\nTo switch to a different optimizer, we simply change the opt variable:\n\nopt = Optimisers.Adam()\n\nAdam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8)\n\n\nAnd call the loss_optimiser function again:\n\nŒ∏_opt, stats = loss_optimiser(loss, Œ∏0, opt, steps = 1000)\nŒ∏_opt'\n\n1√ó10 adjoint(::Vector{Float64}) with eltype Float64:\n 1.1309  1.40642  1.1309  1.12579  ‚Ä¶  1.1309  -1.16284  1.1216  -1.1874",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#comparing-optimisers",
    "href": "Module04/Module04_Slides.html#comparing-optimisers",
    "title": "Machine Learning for Computational Economics",
    "section": "Comparing optimisers",
    "text": "Comparing optimisers",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#fitting-a-dnn",
    "href": "Module04/Module04_Slides.html#fitting-a-dnn",
    "title": "Machine Learning for Computational Economics",
    "section": "Fitting a DNN",
    "text": "Fitting a DNN\nAs a final example, we fit a deep neural network (DNN) to a nonlinear, high-dimensional function.\n\nThe goal is to illustrate how a DNN can learn a complex mapping and generalize beyond the training data.\n\nDefine the multivariate function: \\[\n  f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^n p(x_i), \\qquad \\qquad p(x) = \\prod_{i=1}^5 (x - r_i),\n\\] where the roots \\(r_i\\) are drawn from a standard normal distribution.\n\n\n\n\nWe fix \\(n = 10\\) and draw \\(100,000\\) sample pairs \\((\\mathbf{x}_i, y_i)\\), where \\(y_i = f(\\mathbf{x}_i)\\).\n# Constructing function f\nrng  = Xoshiro(0)           # pseudo random number generator\nroots = randn(rng, 5)       # polynomial roots\np(x) = prod(x .- roots)     # univariate polynomial\nf(x) = mean(p.(x))          # multivariate version\n\n# Random samples\nn_states, sample_size = 10, 100_000\nx_samples = rand(rng, Uniform(-1,1), (n_states,sample_size))\ny_samples = [f(x_samples[:,i]) for i = 1:sample_size]'",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#dnn-architecture",
    "href": "Module04/Module04_Slides.html#dnn-architecture",
    "title": "Machine Learning for Computational Economics",
    "section": "DNN architecture",
    "text": "DNN architecture\nWe define a DNN with two hidden layers of 32 units each and GELU activations.\n\n# Architecture\nlayers = [n_states, 32, 32, 1]\nmodel = Chain(\n    Dense(layers[1] =&gt; layers[2], Lux.gelu),\n    Dense(layers[2] =&gt; layers[3], Lux.gelu),\n    Dense(layers[3] =&gt; layers[4], identity)\n)\n\n\nChain(\n    layer_1 = Dense(10 =&gt; 32, gelu_tanh),         # 352 parameters\n    layer_2 = Dense(32 =&gt; 32, gelu_tanh),         # 1_056 parameters\n    layer_3 = Dense(32 =&gt; 1),                     # 33 parameters\n)         # Total: 1_441 parameters,\n          #        plus 0 states.\n\n\n\n\n\n\n\nWe then initialise the parameters and choose the optimiser:\n\nparameters, layer_states = Lux.setup(rng, model)\nopt = Optimisers.Adam()\nopt_state = Optimisers.setup(opt, parameters)\n\n\n(layer_1 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (Float32[0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], Float32[0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999)))), layer_2 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (Float32[0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], Float32[0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999)))), layer_3 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (Float32[0.0 0.0 ‚Ä¶ 0.0 0.0], Float32[0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (Float32[0.0], Float32[0.0], (0.9, 0.999)))))",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#dnn-training-loop",
    "href": "Module04/Module04_Slides.html#dnn-training-loop",
    "title": "Machine Learning for Computational Economics",
    "section": "DNN training loop",
    "text": "DNN training loop\nWe then define the loss function:\n\nfunction loss_fn(parameters, layer_states)\n    y_prediction, layer_states = model(x_samples, parameters, layer_states)\n    loss = mean(abs2, y_prediction - y_samples)\n    return loss, layer_states\nend\n\nloss_fn (generic function with 1 method)\n\n\n\n\n\n\nWe finally run the training loop:\n# train loop\nloss_history = []\nn_steps = 300_000\nfor _ in 1:n_steps\n    loss, layer_states = loss_fn(parameters, layer_states)\n    grad = gradient(p-&gt;loss_fn(p, layer_states)[1], parameters)[1]\n    opt_state, parameters = Optimisers.update(opt_state, parameters, grad)\n    push!(loss_history, loss)\n    if epoch % 5000 == 0\n        println(\"Epoch: $epoch, Loss: $loss\")\n    end\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#data-and-prediction-for-the-dnn",
    "href": "Module04/Module04_Slides.html#data-and-prediction-for-the-dnn",
    "title": "Machine Learning for Computational Economics",
    "section": "Data and prediction for the DNN",
    "text": "Data and prediction for the DNN\nThe figure below presents the results.\n\nThe left panel shows a random subsample of 256 data points (blue) and the corresponding DNN predictions (red)\nThe two are nearly indistinguishable, indicating an excellent in-sample fit.\n\n\n\n\n\n\n\n\nA natural concern is that the network might simply be memorizing the training data.\n\nTo test this, we evaluate the network in the special case \\(x_1 = x_2 = \\cdots = x_n = x\\)\nThen, \\(f(\\mathbf{x})\\) reduces to \\(p(x)\\), a configuration that never appears in the random training sample.\n\nEven without seeing this zero-probability case during training, the network‚Äôs predictions closely match \\(f(\\mathbf{x})\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#automatic-differentiation",
    "href": "Module04/Module04_Slides.html#automatic-differentiation",
    "title": "Machine Learning for Computational Economics",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nWe have introduced several optimisation algorithms for training neural networks.\n\nAll these methods are gradient-based ‚Äî they rely on the derivatives of the loss function.\nEfficient and accurate computation of gradients is a central ingredient in modern machine learning.\n\n\n\n\n\nAutomatic differentiation provides a systematic way to compute these gradients.\n\nAD applies the chain rule algorithmically on a sequence of elementary operations.\nThis makes it both exact (up to machine precision) and computationally efficient.\n\n\n\n\n\n\nThere are two main modes of automatic differentiation:\n\nForward mode ‚Äî derivatives are propagated forward from inputs to outputs.\nReverse mode ‚Äî derivatives are propagated backward from outputs to inputs.\n\nReverse mode is also known as backpropagation and forms the foundation of training algorithms for neural networks.\n\nWe begin with forward mode AD, which is conceptually simpler and helps to build intuition.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#forward-mode-automatic-differentiation",
    "href": "Module04/Module04_Slides.html#forward-mode-automatic-differentiation",
    "title": "Machine Learning for Computational Economics",
    "section": "Forward-Mode Automatic Differentiation",
    "text": "Forward-Mode Automatic Differentiation\nTo build intuition for forward-mode AD, consider the first-order approximation of a function \\(f(x)\\) around a point \\(x_0\\): \\[\n  f(x) = f(x_0) + f'(x_0)\\,\\epsilon + \\mathcal{O}(\\epsilon^2),\n\\] where \\(x = x_0 + \\epsilon\\) for a small perturbation \\(\\epsilon\\), and \\(\\mathcal{O}(\\epsilon^2)\\) collects higher-order terms.\n\n\n\nA first-order approximation is therefore characterized by two quantities:\n\nthe function value \\(f(x_0)\\) and\nits derivative \\(f'(x_0)\\).\n\n\n\n\n\n\n\nThe Product Rule\nSuppose we know the linear approximations of two functions, \\[\n  f(x) = f(x_0) + f'(x_0)\\,\\epsilon + \\mathcal{O}(\\epsilon^2),\n\\] \\[\n  g(x) = g(x_0) + g'(x_0)\\,\\epsilon + \\mathcal{O}(\\epsilon^2).\n\\]\nConsider the product \\(h(x) = f(x)g(x)\\): \\[\n  h(x) =\n  \\underbrace{f(x_0) g(x_0)}_{\\color{#0072b2}{h(x_0)}} +\n  \\underbrace{[f'(x_0) g(x_0) + f(x_0) g'(x_0)]}_{\\color{#d55e00}{h'(x_0)}}\\,\\epsilon\n  + \\mathcal{O}(\\epsilon^2).\n\\]\n\nThe Chain Rule\nNow consider a composition \\(h(x) = g(f(x))\\).\n\nSubstituting the linear approximation of \\(f(x)\\) into \\(g(\\cdot)\\) gives: \\[\n\\begin{align}\nh(x)\n&= g(f(x_0) + f'(x_0)\\,\\epsilon + \\mathcal{O}(\\epsilon^2)) \\\\\n&=\n\\underbrace{g(f(x_0))}_{\\color{#0072b2}{h(x_0)}} +\n\\underbrace{g'(f(x_0)) f'(x_0)}_{\\color{#d55e00}{h'(x_0)}}\\,\\epsilon\n+ \\mathcal{O}(\\epsilon^2),\n\\end{align}\n\\] which recovers the familiar chain rule.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#dual-numbers",
    "href": "Module04/Module04_Slides.html#dual-numbers",
    "title": "Machine Learning for Computational Economics",
    "section": "Dual numbers",
    "text": "Dual numbers\nCombining a few simple rules, we can compute the linear approximation of arbitrarily complex fucntion\n\nThis idea can be implemented in a computer using the concept of dual numbers.\n\n\n\n\n\nWe represent a dual number as a pair of numbers: \\[\n  u = a + b\\,\\epsilon,\n\\] where the primal part \\(a\\) stores the function value and the dual part \\(b\\) stores its derivative.\n\n\n\nThis construction is analogous to complex numbers\n\nFor a complex number \\(z = a + b i\\), the defining property is \\(i^2 = -1\\).\nFor a dual number, the defining property of the dual unit \\(\\epsilon\\) is \\(\\epsilon^2 = 0\\).\n\n\n\n\n\n\n\n\nThe Product of Dual Numbers\nGiven two dual numbers, \\(u = a + b\\,\\epsilon\\) and \\(v = c + d\\,\\epsilon\\), define: \\[\n  u \\times v = (a + b\\,\\epsilon) \\times (c + d\\,\\epsilon) = ac + (ad + bc)\\,\\epsilon,\n\\] which mirrors the product rule of derivatives.\n\nElementary Functions on Dual Numbers\nGiven a real function \\(g(x)\\), we can extend it to dual numbers: \\[\n  g(a + b\\,\\epsilon) = g(a) + g'(a)\\,b\\,\\epsilon,\n\\] which automatically implements the chain rule.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#julia-implementation-2",
    "href": "Module04/Module04_Slides.html#julia-implementation-2",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia implementation",
    "text": "Julia implementation\nWe can implement a basic automatic differentiation engine in Julia with only a few lines of code.\n\nWe start by defining the dual number type\n\n\n# Define the dual number type\nstruct D &lt;: Number\n    v::Real # primal part (value of the function)\n    d::Real # dual part (derivative of the function)\nend\n\nu = D(1.0, 1.0) # instantiate a dual number\n\nD(1.0, 1.0)\n\n\nGiven this new number type, we can now define the basic operations on dual numbers.\n# How to perform basic operations on dual numbers\n import Base: +,-,*,/, convert, promote_rule, exp, sin, cos, log\n +(x::D, y::D)  = D(x.v + y.v, x.d + y.d)\n -(x::D, y::D)  = D(x.v - y.v, x.d - y.d)\n *(x::D, y::D)  = D(x.v * y.v, x.d * y.v + x.v * y.d)\n /(x::D, y::D)  = D(x.v / y.v, (x.v * y.d - y.v * x.d) / y.v^2)\n exp(x::D)      = D(exp(x.v), exp(x.v) * x.d)\n log(x::D)      = D(log(x.v), 1.0 / x.v * x.d)\n sin(x::D)      = D(sin(x.v), cos(x.v) * x.d)\n cos(x::D)      = D(cos(x.v), -sin(x.v) * x.d)\n promote_rule(::Type{D}, ::Type{&lt;:Number}) = D\n Base.show(io::IO,x::D) = print(io,x.v,\" + \",x.d,\" œµ\")",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#computing-the-derivative-of-a-function",
    "href": "Module04/Module04_Slides.html#computing-the-derivative-of-a-function",
    "title": "Machine Learning for Computational Economics",
    "section": "Computing the derivative of a function",
    "text": "Computing the derivative of a function\nWe can now use the dual numbers to compute the derivatives of functions.\n\nFirst, define our test functions\n\n\nf(x) = exp(x^2)\ng(x) = cos(x^3)\n\nfprime(x) = exp(x^2) * 2 * x\ngprime(x) = -sin(x^3) * 3 * x^2;\n\n\n\n\n\nWe can now evaluate the functions at dual numbers\n\nTo compute the derivative of a function \\(f\\), we construct \\(u = x_0 + 1.0 * \\epsilon\\), evaluate \\(f(u)\\), and extract the dual part of the result.\n\n\nu, v = D(1.0, 1.0), D(2.0, 1.0)\nprintln(\"f(u) = \", f(u), \" |  Exact derivative: \", fprime(1.0))\nprintln(\"g(v) = \", g(v), \" |  Exact derivative: \", gprime(2.0))\n\nf(u) = 2.718281828459045 + 5.43656365691809 œµ |  Exact derivative: 5.43656365691809\ng(v) = -0.14550003380861354 + -11.872298959480581 œµ |  Exact derivative: -11.872298959480581\n\n\n\n\n\n\n\nWe can define a helper function that keeps the dual arithmetic hidden from the user.\n\nfunction derivative(f::Function, x::Real)\n    u = D(x, 1.0)\n    return f(u).d\nend\nderivative(f, 1.0), derivative(g, 2.0)\n\n(5.43656365691809, -11.872298959480581)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#multiple-dimensions",
    "href": "Module04/Module04_Slides.html#multiple-dimensions",
    "title": "Machine Learning for Computational Economics",
    "section": "Multiple dimensions",
    "text": "Multiple dimensions\nWe can extend the dual numbers to multiple dimensions.\n\nSuppose \\(\\mathbf{y} = \\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^m\\) with \\(\\mathbf{x} \\in \\mathbb{R}^n\\). Introduce a vector dual number \\[\n\\mathbf{x} = \\mathbf{x}_0 + \\epsilon\\,\\mathbf{v},\n\\] where \\(\\mathbf{x}_0 \\in \\mathbb{R}^n\\) is the primal part and \\(\\mathbf{v} \\in \\mathbb{R}^n\\) is the tangent direction.\n\nThe first-order approximation of \\(\\mathbf{f}\\) is \\[\n  \\mathbf{f}(\\mathbf{x})\n  = \\color{#0072b2}{\\underbrace{\\mathbf{f}(\\mathbf{x}_0)}_{\\text{primal}}}\n    + \\color{#d55e00}{\\underbrace{\\mathbf{Jf}(\\mathbf{x}_0)\\,\\mathbf{v}}_{\\text{Jacobian-vector product (JVP)}}}\\,\\epsilon\n    + \\mathcal{O}(\\epsilon^2),\n\\] where \\(\\mathbf{Jf}(\\mathbf{x}_0)\\in\\mathbb{R}^{m\\times n}\\) has \\((i,j)\\) entry \\(\\partial f_i/\\partial x_j(\\mathbf{x}_0)\\).\n\nThus, applying \\(\\,\\mathbf{f}\\,\\) to \\((\\mathbf{x}_0,\\mathbf{v})\\) returns both the value \\(\\mathbf{f}(\\mathbf{x}_0)\\) and the directional derivative \\(\\mathbf{Jf}(\\mathbf{x}_0)\\mathbf{v}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCost: JVP vs.¬†full Jacobian.\n\n\nFor \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\), a JVP needs only a single forward pass with the tangent \\(\\mathbf{v}\\): \\[\n\\text{cost(JVP)} \\sim \\mathcal{O}(\\text{cost}(f)).\n\\] Forming the full Jacobian by forward mode requires \\(n\\) passes (one per input direction): \\[\n\\text{cost(Jacobian via forward mode)} \\sim \\mathcal{O}\\!\\left(n\\,\\text{cost}(f)\\right).\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#computational-efficiency-of-jvps-and-full-jacobians",
    "href": "Module04/Module04_Slides.html#computational-efficiency-of-jvps-and-full-jacobians",
    "title": "Machine Learning for Computational Economics",
    "section": "Computational efficiency of JVPs and full Jacobians",
    "text": "Computational efficiency of JVPs and full Jacobians\nTo illustrate the computational efficiency of JVPs and full Jacobians, we compare JVPs to full Jacobians on the test function \\[\n  f_i(\\mathbf{x}) = \\exp\\!\\left(-\\tfrac{1}{n}\\sum_{j=1}^n \\sqrt{x_j}\\right) + i, \\qquad \\qquad\n  \\mathbf{f}(\\mathbf{x}) = \\big[f_1(\\mathbf{x}), \\ldots, f_m(\\mathbf{x})\\big]^\\top \\in \\mathbb{R}^m.\n\\]\n\nWe can implement the JVP and full Jacobian computations in Julia using the ForwardDiff.jl package.\n# Test function\nf(x; n = 1) = [exp(-mean(sqrt.(x)))+i for i = 1:n]\n\n# Compute JVP\nx, v = [1.0, 2.0, 3.0], [0.1, 0.2, 0.3]\nxdual = ForwardDiff.Dual{Float64}.(x, v) # vector of dual numbers\nydual = f(xdual; n = 2) # evaluate function at dual numbers\njvp = ForwardDiff.partials.(ydual) # jvp\n\n# Compute Jacobian\njac = ForwardDiff.jacobian(x-&gt;f(x; n = 2), x)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#the-order-of-operations-matters",
    "href": "Module04/Module04_Slides.html#the-order-of-operations-matters",
    "title": "Machine Learning for Computational Economics",
    "section": "The order of operations matters",
    "text": "The order of operations matters\nThe order of operations matters when computing the Jacobian of a function.\n\nConsider the composition of functions \\[\n\\mathbf{F}(\\mathbf{x}) = \\mathbf{f}(\\mathbf{g}(\\mathbf{h}(\\mathbf{x}))),\n\\] where \\(\\mathbf{h}:\\mathbb{R}^n \\!\\to\\! \\mathbb{R}^p\\), \\(\\mathbf{g}:\\mathbb{R}^p \\!\\to\\! \\mathbb{R}^q\\), and \\(\\mathbf{f}:\\mathbb{R}^q \\!\\to\\! \\mathbb{R}^m\\).\n\nThe chain rule implies the Jacobian of \\(\\mathbf{F}\\) is the product of the Jacobians of \\(\\mathbf{f}\\), \\(\\mathbf{g}\\), and \\(\\mathbf{h}\\): \\[\n  \\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{x}^\\top}\n  = \\color{#0072b2}{\\underbrace{\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{g}^\\top}}_{m\\times q}}\n    \\color{#d55e00}{\\underbrace{\\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{h}^\\top}}_{q\\times p}}\n    \\color{#009e73}{\\underbrace{\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}^\\top}}_{p\\times n}}.\n\\]\n\n\n\n\nThere are two natural ways to evaluate the product.\n\n\n\nRight-to-left (forward mode): \\[\n\\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{x}^\\top}\n= \\color{#0072b2}{\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{g}^\\top}}\n  \\left(\\color{#d55e00}{\\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{h}^\\top}}\n  \\color{#009e73}{\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}^\\top}}\\right).\n\\]\n\n\n\nLeft-to-right (reverse mode): \\[\n\\frac{\\partial \\mathbf{F}}{\\partial \\mathbf{x}^\\top}\n= \\left(\\color{#0072b2}{\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{g}^\\top}}\n  \\color{#d55e00}{\\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{h}^\\top}}\\right)\n  \\color{#009e73}{\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}^\\top}}.\n\\]\n\n\n\n\nConsider the special case \\(p = q = r\\) and \\(m = 1\\) (a loss function for a DNN with two hidden layers of \\(r\\) units each).\n\nForward mode: First term costs \\(n\\) Jacobian-vector products (JVPs), for a total cost of \\(\\mathcal{O}(r^2 n)\\) operations.\nReverse mode: Total cost corresponds to two vector-Jacobian products (VJPs), amounting to \\(\\mathcal{O}(r n)\\) operations.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#the-computational-graph",
    "href": "Module04/Module04_Slides.html#the-computational-graph",
    "title": "Machine Learning for Computational Economics",
    "section": "The computational graph",
    "text": "The computational graph\nTo understand how reverse mode works, it is useful to represent the function as a computational graph. Consider the function \\[\n  f(x) = (x_1 + x_2)\\,x_1^2.\n\\]\n\nEach operation in the function can be viewed as a node in a directed acyclic graph (DAG):\n\n\n\nReverse mode proceeds in two stages:\n\nA forward pass to compute and store the value at each node.\nA backward pass to accumulate gradients of the output with respect to each node.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#forward-and-backward-passes",
    "href": "Module04/Module04_Slides.html#forward-and-backward-passes",
    "title": "Machine Learning for Computational Economics",
    "section": "Forward and backward passes",
    "text": "Forward and backward passes\n\n\n\n\nForward pass: \\[\n  v_1 = x_1, \\qquad v_2 = x_2, \\qquad\n  v_3 = v_1 + v_2, \\qquad v_4 = v_1^2, \\qquad v_5 = v_3 v_4.\n\\]\nFor \\(x_1 = 1.0\\) and \\(x_2 = 2.0\\), we obtain \\[\n  v_1 = 1.0, \\qquad v_2 = 2.0, \\qquad\n  v_3 = 3.0, \\qquad v_4 = 1.0, \\qquad v_5 = 3.0.\n\\]\n\n\nBackward pass:\n\nWe seek \\(\\nabla_{\\mathbf{x}} f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}\\right)\\). Define the adjoints as \\(\\overline{v}_i \\equiv \\frac{\\partial f}{\\partial v_i}\\) for \\(i = 1, 2, 3, 4, 5\\).\nStarting from the output \\(v_5\\), we initialize its adjoint as \\(\\overline{v}_5 \\equiv \\frac{\\partial f}{\\partial v_5} = 1\\)\n\n\n\n\n\n\nThe adjoints at the last nodes are \\[\n  \\overline{v}_3 = \\overline{v}_5 \\cdot v_4 = 1.0, \\qquad\n  \\overline{v}_4 = \\overline{v}_5 \\cdot v_3 = 3.0.\n\\]\n\nMoving one step further back, the local derivatives are \\[\\begin{equation}\n    \\frac{\\partial v_3}{\\partial v_1} = 1, \\quad\n    \\frac{\\partial v_3}{\\partial v_2} = 1, \\quad\n    \\frac{\\partial v_4}{\\partial v_1} = 2v_1, \\quad\n    \\frac{\\partial v_4}{\\partial v_2} = 0.\n  \\end{equation}\\]\n\n\n\n\n\n\nThe adjoint for \\(v_1\\) collects contributions from two branches: \\[\n  \\begin{align}\n    \\overline{v}_1 &{+}= \\overline{v}_3 \\frac{\\partial v_3}{\\partial v_1} = 1 \\cdot 1 = 1,\\\\\n    \\overline{v}_1 &{+}= \\overline{v}_4 \\frac{\\partial v_4}{\\partial v_1} = 3 \\cdot 2v_1 = 6.\n  \\end{align}\n  \\]\n\nVariable \\(v_2\\) affects only \\(v_3\\), so \\[\n    \\overline{v}_2 = \\overline{v}_3 \\frac{\\partial v_3}{\\partial v_2} = 1 \\cdot 1 = 1.\n  \\] \\[\n    \\frac{\\partial f}{\\partial x_1} = \\overline{v}_1 = 7, \\qquad\n    \\frac{\\partial f}{\\partial x_2} = \\overline{v}_2 = 1.\n  \\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#julia-implementation-3",
    "href": "Module04/Module04_Slides.html#julia-implementation-3",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia implementation",
    "text": "Julia implementation\nWe can implement reverse-mode automatic differentiation in Julia using the Zygote.jl package.\n\nusing Zygote\nf(x‚ÇÅ, x‚ÇÇ) = (x‚ÇÅ + x‚ÇÇ) * x‚ÇÅ^2\ny, back = Zygote.pullback(f, 1.0, 2.0)\nback(1.0)\n\n(7.0, 1.0)\n\n\n\n\n\nInternally, Zygote constructs the computational graph at compile time\n\nThen, it performs the backward pass to propagate adjoints through the graph.\n\nfunction pullback_manual(x1, x2)\n    v1, v2 = x1, x2\n    v3 = v1 + v2\n    v4 = v1^2\n    v5 = v3 * v4\n    function back(vÃÖ5)\n        vÃÖ3 = vÃÖ5 * v4\n        vÃÖ4 = vÃÖ5 * v3\n        vÃÖ1 = vÃÖ3 * 1 + vÃÖ4 * (2v1)\n        vÃÖ2 = vÃÖ3 * 1\n        return (vÃÖ1, vÃÖ2)\n    end\n    return v5, back\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#multi-output-functions-and-vjps",
    "href": "Module04/Module04_Slides.html#multi-output-functions-and-vjps",
    "title": "Machine Learning for Computational Economics",
    "section": "Multi-output functions and VJPs",
    "text": "Multi-output functions and VJPs\nWhy does Zygote.pullback return a function rather than the gradient directly?\n\nThis design becomes essential when dealing with multi-output functions.\n\nConsider the function \\[\n  \\mathbf{f}(\\mathbf{x}) = (x_1 + x_2)x_1^2, x_1 x_2.\n  \\] whose Jacobian is \\[\n  \\mathbf{Jf}(\\mathbf{x}) = \\begin{pmatrix}\n    3 x_1^2 + 2 x_1 x_2 & x_1^2 \\\\\n    x_2 & x_1\n  \\end{pmatrix}.\n  \\]\nWe can use Zygote.pullback to compute the gradient of \\(\\mathbf{f}\\) with respect to \\(\\mathbf{x}\\):\n\nusing Zygote\nf(x‚ÇÅ, x‚ÇÇ) = [(x‚ÇÅ + x‚ÇÇ) * x‚ÇÅ^2, x‚ÇÅ * x‚ÇÇ]\ny, back = Zygote.pullback(f, 1.0, 2.0)\nback([1.0, 0.0])\n\n(7.0, 1.0)\n\n\nHere, back takes a vector of output adjoints and returns the vector-Jacobian product (VJP):\n\nThis demostrates that reverse-mode AD computes VJPs efficiently with cost \\(\\mathcal{O}(\\text{cost}(f))\\)‚Äîindependent of the output dimension \\(m\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#performance-comparison",
    "href": "Module04/Module04_Slides.html#performance-comparison",
    "title": "Machine Learning for Computational Economics",
    "section": "Performance comparison",
    "text": "Performance comparison\nWe now illustrate the scaling properties of reverse-mode AD using Zygote.jl.\n\nWe employ the same test functions used in the forward-mode AD performance comparison.\n\nFor scalar-valued functions (left panel), the cost of a VJP is independent of the input dimension \\(n\\)\n\nFor vector-valued functions (right panel), computing the full Jacobian requires one backward pass per output dimension.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaking stock.\n\n\nAutomatic differentiation provides an efficient way to compute gradients of functions.\n\nForward-mode excels when the output dimension is large, while reverse-mode is better when the input dimension is large.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module04/Module04_Slides.html#references",
    "href": "Module04/Module04_Slides.html#references",
    "title": "Machine Learning for Computational Economics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nCybenko, G. 1989. ‚ÄúApproximation by Superposition of Sigmoidal Functions.‚Äù Mathematics of Control, Signals and Systems 2 (4): 303‚Äì14.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nHornik, Kurt. 1991. ‚ÄúApproximation Capabilities of Multilayer Feedforward Networks.‚Äù Neural Networks 4 (2): 251‚Äì57. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. ‚ÄúImageNet Classification with Deep Convolutional Neural Networks.‚Äù In Advances in Neural Information Processing Systems 25, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097‚Äì1105. Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nLeshno, Moshe, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. 1993. ‚ÄúMultilayer Feedforward Networks with a Nonpolynomial Activation Function Can Approximate Any Function.‚Äù Neural Networks 6 (6): 861‚Äì67.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2019. ‚ÄúDecoupled Weight Decay Regularization.‚Äù In International Conference on Learning Representations. https://arxiv.org/abs/1711.05101.\n\n\nLu, Zhou, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. ‚ÄúThe Expressive Power of Neural Networks: A View from the Width.‚Äù In Advances in Neural Information Processing Systems, 30:6232‚Äì40.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. Cambridge, UK: Cambridge University Press. https://udlbook.github.io/udlbook/.\n\n\nRoss, Stephen A. 1976. ‚ÄúOptions and Efficiency.‚Äù Quarterly Journal of Economics 90 (1): 75‚Äì89.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 4 ‚Äì Fundamentals of Machine Learning"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#introduction",
    "href": "Module02/Module02_Slides.html#introduction",
    "title": "Machine Learning for Computational Economics",
    "section": "Introduction",
    "text": "Introduction\nDiscrete-time methods are the natural starting point for dynamic programming.\n\nIn this module, we introduce classical numerical methods to solve such models.\nThese techniques provide a natural starting point for more advanced methods.\n\n\n\n\nWe illustrate these concepts with a consumption-savings problem\n\nThis model is the backbone of many applications in macroeconomics and finance.\nIt is a key building block for many heterogeneous-agent models.\n\n\n\n\nThis benchmark problem allows us to illustrate three core computational tasks:\n\nRepresenting value and policy functions on a grid\nComputing expectations in problems with uncertainty\nPerforming the maximization step efficiently",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#a-consumption-savings-problem",
    "href": "Module02/Module02_Slides.html#a-consumption-savings-problem",
    "title": "Machine Learning for Computational Economics",
    "section": "A Consumption-Savings Problem",
    "text": "A Consumption-Savings Problem\nConsider a household deciding how much to consume and save when income is uncertain.\n\nThe household enters each period with cash-on-hand \\(M_t\\), sum of financial wealth \\(W_t\\) and labor income \\(Y_t\\).\nThe household earns a risk-free return \\(R\\) on her savings\n\n\nCash-on-hand evolves according to: \\[  \n  M_{t+1} = \\color{#0072b2}{\\underbrace{R\\,(M_t - c_t)}_{\\text{return on savings}}} + \\color{#d55e00}{\\underbrace{Y_{t+1}}_{\\text{labor income}}}.\n\\]\n\n\nThe household‚Äôs problem is to choose a consumption plan \\(\\{c_t\\}_{t=0}^{T-1}\\) that maximizes expected lifetime utility: \\[  \n  V_T(M)\n  = \\max_{\\{c_t\\}_{t=0}^{T-1}}\n    \\mathbb{E}\\!\\left[\n      \\sum_{t=0}^{T-1} e^{-\\rho t}\\, u(c_t)\n      + e^{-\\rho T}\\, \\color{#009e73}{\\underbrace{V_0(M_T)}_{\\text{terminal payoff}}}\n    \\right],\n\\]\nsubject to the transition equation for cash-on-hand and the stochastic process for labor income, \\[  \n  \\log Y_{t+1} \\sim \\mathcal{N}\\!\\left(- \\tfrac{1}{2}\\sigma_y^2,\\, \\sigma_y^2\\right),\n\\] where the normalization ensures that \\(\\mathbb{E}[Y_t] = 1\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-recursive-representation",
    "href": "Module02/Module02_Slides.html#the-recursive-representation",
    "title": "Machine Learning for Computational Economics",
    "section": "The Recursive Representation",
    "text": "The Recursive Representation\nThe recursive problem is given by \\[  \n  V_t(M)\n  = \\max_{c}\n    \\left\\{\n      u(c) + e^{-\\rho} \\mathbb{E}\\!\\left[ V_{t-1}(M') \\right]\n    \\right\\},\n\\]\nsubject to \\[  \n  M' = R (M - c) + Y', \\qquad\n  \\log Y' \\sim \\mathcal{N}\\!\\left(- \\tfrac{1}{2}\\sigma_y^2,\\, \\sigma_y^2\\right).\n\\]\n\n\n\n\nPreferences and terminal payoff are of the constant relative risk aversion (CRRA) form: \\[  \n  u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}, \\qquad\n  V_0(M) = A \\frac{M^{1-\\gamma}}{1-\\gamma}.\n\\]\nwhere \\(\\gamma &gt; 0\\) is the coefficient of relative risk aversion and \\(A &gt; 0\\) is a scaling parameter.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe infinite-horizon problem, where \\(T \\to \\infty\\), corresponds to the stationary solution of the recursive problem. In this case, the value function satisfies the fixed-point condition \\(V_t(M) = V_{t-1}(M) \\equiv V(M)\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-action-value-function",
    "href": "Module02/Module02_Slides.html#the-action-value-function",
    "title": "Machine Learning for Computational Economics",
    "section": "The Action-Value Function",
    "text": "The Action-Value Function\nThe action-value function is given by \\[  \n  V_t(M, c)\n  \\equiv\n      u(c) + e^{-\\rho} \\mathbb{E}\\!\\left[ V_{t-1}(R (M- c) + Y') \\right].\n\\]\n\n\n\n\n\n\nTip\n\n\nThe action-value function corresponds to the value associated with a given action (no maximization step).\n\n\n\n\n\n\n\n\nThink of \\(V_t(M,c)\\) as a table\n\nColumns: actions \\(c\\)\nRows: states \\(M\\)\n\n\n\n\nOptimal consumption: \\[c_t(M) = \\arg\\max_{c} V_t(M,c)\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#value-function-iteration",
    "href": "Module02/Module02_Slides.html#value-function-iteration",
    "title": "Machine Learning for Computational Economics",
    "section": "Value Function Iteration",
    "text": "Value Function Iteration\nWe can solve the Bellman equation by value function iteration (VFI).\n\nGiven \\(V^{(0)}(M)\\), find the optimal consumption: \\(c^{(1)}(M) = \\arg\\max_{c} V^{(1)}(M,c)\\).\n\nUpdate the value function as \\(V_1(M) = V_1(M, c_1(M))\\).\n\n\n\nAlgorithm: Value Function Iteration (VFI)\nInput: Initial guess \\(V^{(0)}(M)\\), tolerance tol\nOutput: Value \\(V(M)\\), policy \\(c^*(M)\\)\nInitialize: \\(t \\gets 0\\)\nRepeat until \\(\\|V^{(t+1)}-V^{(t)}\\|_\\infty &lt; \\texttt{tol}\\):\n\nPolicy update: \\[c_{t+1}(M) \\gets \\arg\\max_{c}\\left\\{ u(c) + e^{-\\rho}\\,\\mathbb{E}[V_{t}(R(M-c)+Y')] \\right\\}\\]\nValue update: \\[V_{t+1}(M) \\gets u(c_{t+1}(M)) + e^{-\\rho}\\,\\mathbb{E}\\left[ V_{t}(R(M-c_{t+1}(M))+Y') \\right]\\]\n\\(t \\gets t+1\\)\n\nReturn: \\(V^{(t)},\\,c^{(t)}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#numerical-solution",
    "href": "Module02/Module02_Slides.html#numerical-solution",
    "title": "Machine Learning for Computational Economics",
    "section": "Numerical Solution",
    "text": "Numerical Solution\nThere is no known analytical solution to this problem.\n\nWe must then resort to numerical methods.\n\n\n\n\nTo solve the problem numerically, we must overcome three main challenges:\n\n\nRepresent the value function in the computer\n\n\n\n\nCompute expectations in problems with uncertainty\n\n\n\n\nPerform the maximization step efficiently",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#representing-the-value-function",
    "href": "Module02/Module02_Slides.html#representing-the-value-function",
    "title": "Machine Learning for Computational Economics",
    "section": "Representing the Value Function",
    "text": "Representing the Value Function\nWe need to find a way to represent the value function in the computer.\n\nWe need a finite representation of an infinite-dimensional object.\nIn our single-state problem, this can be done by representing the value function on a grid.\n\n\nWe begin by constructing a finite grid for \\(M\\): \\[\n  M \\in \\mathcal{M} = \\{M_{1}, M_{2}, \\ldots, M_{N}\\}.\n\\]\n\nMgrid = collect(range(0.0, 3.0, length=9)) # uniform grid for M\nMgrid'\n\n1√ó9 adjoint(::Vector{Float64}) with eltype Float64:\n 0.0  0.375  0.75  1.125  1.5  1.875  2.25  2.625  3.0\n\n\n\n\nAt each point on this grid, we store the corresponding value function as a vector: \\[\n  \\mathbf{V}_t = (V_{t,1}, V_{t,2}, \\ldots, V_{t,N})^\\top,\n  \\qquad \\text{where } V_{t,i} \\equiv V_t(M_i).\n\\]\n\n\nSimilarly, the optimal consumption policy can be represented as a vector: \\[\n  \\mathbf{c}_t = (c_{t,1}, c_{t,2}, \\ldots, c_{t,N})^\\top,\n  \\qquad \\text{where } c_{t,i} \\equiv c_t(M_i).\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#interpolating-the-value-function",
    "href": "Module02/Module02_Slides.html#interpolating-the-value-function",
    "title": "Machine Learning for Computational Economics",
    "section": "Interpolating the Value Function",
    "text": "Interpolating the Value Function\nSuppose we start with the initial value function represented on a grid: \\(\\mathbf{V}_0\\).\n\nTo compute \\(\\mathbb{E}[V_0(R(M-c)+Y')]\\), we need to evaluate the value function at points outside the grid.\nWe can use linear interpolation to approximate the value function between grid points.\n\n\n\n\n\nFor \\(M \\in [M_{i-1}, M_i]\\), we assume that \\(V_t(M)\\) is linear and approximate it as \\[\n  V_t(M)\n  \\approx\n  \\frac{M - M_{i-1}}{M_i - M_{i-1}}\\, V_{t,i}\n  + \\frac{M_i - M}{M_i - M_{i-1}}\\, V_{t,i-1}.\n\\]\n\n\n\n\n\nWe can implement linear interpolation using the Interpolations.jl package.\n\nusing Interpolations\nMgrid    = collect(range(0.0, 3.0, length=9)) # uniform grid for M\nV_0      = [M^(1-2)/(1-2) for M in Mgrid] # Œ≥ = 2\nV_interp = linear_interpolation(Mgrid, V_0)\nV_interp(œÄ/2) # evaluating at point outside the grid\n\n-0.6414946393618145",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#accuracy-of-a-linear-interpolation",
    "href": "Module02/Module02_Slides.html#accuracy-of-a-linear-interpolation",
    "title": "Machine Learning for Computational Economics",
    "section": "Accuracy of a Linear Interpolation",
    "text": "Accuracy of a Linear Interpolation\nThe accuracy of a linear interpolation depends on the grid size.\n\nWe can approximate even very non-linear functions this way.\nDepending on the function, this may require a very fine grid.\n\n\n\n\n\nQuality of approximation may not be uniform\n\nWorse approximation near the boundaries.\n\n\n\n\nUniform grid can be very inefficient\n\nIt does not allow to focus where it is needed",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#non-uniform-grids",
    "href": "Module02/Module02_Slides.html#non-uniform-grids",
    "title": "Machine Learning for Computational Economics",
    "section": "Non-Uniform Grids",
    "text": "Non-Uniform Grids\nAn alternative is to use a non-uniform grid for the state variable.\n\n\n\n\nA convenient choice is the double-exponential grid\n\nClusters grid points near the lower bound.\nLet \\(u^j\\) denote a uniform grid on the unit interval \\([0,1]\\).\nDefine the grid points as \\[\na_j = a_{\\min} + (a_{\\max} - a_{\\min}) \\frac{e^{\\,e^{\\alpha u^j}-1} - 1}{e^{\\,e^{\\alpha}-1} - 1},\n\\]\n\nThe parameter \\(\\alpha &gt; 0\\) controls the degree of clustering.\n\nAs \\(\\alpha \\to 0\\), the grid becomes approximately uniform.\nFor large \\(\\alpha\\), the grid is very dense near the lower bound.\n\n\nWe can implement this in Julia as follows:\nfunction make_grid(zmin, zmax, Nz; Œ± = 1.0)\n    u = range(0.0, 1.0, length=Nz)\n    double_exp = Œ± == 0 ? u : @. (exp(exp(Œ± * u) - 1.0) - 1.0) / (exp(exp(Œ±) - 1.0) - 1.0)\n    return @. zmin + (zmax - zmin) * double_exp\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#computing-expectations",
    "href": "Module02/Module02_Slides.html#computing-expectations",
    "title": "Machine Learning for Computational Economics",
    "section": "Computing Expectations",
    "text": "Computing Expectations\nThe second issue we must address is how to compute expectations.\n\nIn our setting, this amounts to integrating over the stochastic labor income \\(Y'\\).\n\n\n\n\n\nThe expectation in the Bellman equation can be written as the integral: \\[\n  \\mathbb{E}\\!\\left[V_{t}(M')\\right] = \\int_{-\\infty}^{\\infty} V_t(R(M - c_t(M)) + e^{y'}) \\phi(y') d y',\n\\] where \\(\\phi(y')\\) is the probability density function of a normal random variable.\n\n\n\n\n\nComputing this expectation numerically requires approximating the integral.\n\nA common approach is to discretize the process for the log income \\(y_t \\equiv \\log Y_t\\)\nWe replace the continuous process by a finite-state Markov chain.\nNext: construct a grid and transition probabilities for the Markov chain.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-tauchen-method-constructing-the-grid",
    "href": "Module02/Module02_Slides.html#the-tauchen-method-constructing-the-grid",
    "title": "Machine Learning for Computational Economics",
    "section": "The Tauchen Method: Constructing the Grid",
    "text": "The Tauchen Method: Constructing the Grid\nThe Tauchen method is a common approach to discretize the process for an AR(1) process: \\[\n  z_{t+1} = \\mu + \\rho_z (z_t - \\mu) + \\varepsilon_{t+1},\n  \\qquad \\varepsilon_{t+1} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2),\n\\] where \\(|\\rho_z| &lt; 1\\) and \\(\\sigma_\\varepsilon &gt; 0\\) (in our case, \\(\\rho_z = 0\\) and \\(\\sigma_\\varepsilon = \\sigma_y\\)).\n\n\n\n\nWe start by constructing an uniform grid \\[\n  z \\in \\mathcal{Z} = \\{z_1, z_2, \\ldots, z_{N_z}\\},\n\\] with step size \\(\\Delta = \\frac{z_{N_z} - z_1}{N_z - 1}\\).\n\n\n\nA convenient choice for the endpoints is \\[\n  z_1 = \\mu - m\\,\\frac{\\sigma_z}{\\sqrt{1 - \\rho_z^2}},\n  \\qquad\n  z_{N_z} = \\mu + m\\,\\frac{\\sigma_z}{\\sqrt{1 - \\rho_z^2}},\n\\] where \\(m\\) is typically set to \\(3\\), ensuring that the grid spans \\(\\pm3\\) unconditional standard deviations of \\(z_t\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-tauchen-method-computing-the-transition-probabilities",
    "href": "Module02/Module02_Slides.html#the-tauchen-method-computing-the-transition-probabilities",
    "title": "Machine Learning for Computational Economics",
    "section": "The Tauchen Method: Computing the Transition Probabilities",
    "text": "The Tauchen Method: Computing the Transition Probabilities\nWe need to determine the transition probabilities \\(P_{ij} = \\Pr(z_{t+1} = z_j \\mid z_t = z_i)\\).\n\nConditional on \\(z_t = z_i\\), we have that \\(z_{t+1} \\sim \\mathcal{N}\\left(\\mu + \\rho_z (z_i - \\mu), \\sigma_\\varepsilon^2\\right)\\).\n\n\nThe transition probability \\(P_{ij}\\) equals the probability that \\(z_{t+1}\\) falls between the midpoints surrounding \\(z_j\\):\n\n\n\\[\\scriptsize\n  P_{ij} =\n  \\begin{cases}\n    \\Phi\\!\\left(\\dfrac{z_j - \\mu_i + \\tfrac{\\Delta}{2}}{\\sigma_\\varepsilon}\\right),\n      & j = 1, \\\\[0.75em]\n    \\Phi\\!\\left(\\dfrac{z_j - \\mu_i + \\tfrac{\\Delta}{2}}{\\sigma_\\varepsilon}\\right)\n      - \\Phi\\!\\left(\\dfrac{z_j - \\mu_i - \\tfrac{\\Delta}{2}}{\\sigma_\\varepsilon}\\right),\n      & 1 &lt; j &lt; N_z, \\\\[0.75em]\n    1 - \\Phi\\!\\left(\\dfrac{z_j - \\mu_i - \\tfrac{\\Delta}{2}}{\\sigma_\\varepsilon}\\right),\n      & j = N_z,\n  \\end{cases}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-tauchen-method-in-julia",
    "href": "Module02/Module02_Slides.html#the-tauchen-method-in-julia",
    "title": "Machine Learning for Computational Economics",
    "section": "The Tauchen Method in Julia",
    "text": "The Tauchen Method in Julia\n\"\"\"\nTauchen (1986) discretization of the AR(1) process\n    z_{t+1} = Œº + œÅ z_t + Œµ_{t+1},   Œµ ~ N(0, œÉœµ^2).\n\"\"\"\nfunction tauchen(M::Int, œÅ::Real, œÉœµ::Real; Œº::Real=0.0, m::Real=3.0)\n    @assert M ‚â• 2 \"Need at least M=2 grid points.\"\n    @assert abs(œÅ) &lt; 1 \"Require |œÅ|&lt;1 for stationary AR(1).\"\n    œÉz = œÉœµ / sqrt(1 - œÅ^2)                # unconditional std. dev.\n    zÃÑ = Œº / (1 - œÅ)                        # unconditional mean\n    if œÉœµ == 0.0 return (; z = [zÃÑ], P = [1.0]) end # degenerate case\n    zmin, zmax = zÃÑ - m*œÉz, zÃÑ + m*œÉz\n    Œî = (zmax - zmin) / (M - 1)\n    z = collect(range(zmin, zmax, length=M))\n    P = zeros(Float64, M, M)\n    for i in 1:M\n        mean_next = Œº + œÅ*z[i]\n        dist = Normal(mean_next, œÉœµ)\n        # First bin: (‚àí‚àû, midpoint_1]\n        P[i, 1] = cdf(dist, z[1] + Œî/2)\n        # Interior bins: (midpoint_{j-1}, midpoint_j]\n        for j in 2:M-1\n            upper = z[j] + Œî/2\n            lower = z[j] - Œî/2\n            P[i, j] = cdf(dist, upper) - cdf(dist, lower)\n        end\n        # Last bin: (midpoint_{N-1}, +‚àû)\n        P[i, M] = 1 - cdf(dist, z[M] - Œî/2)\n    end\n    return (;z, P)\nend;",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#conditional-moments-exact-vs.-markov-chain-approximation",
    "href": "Module02/Module02_Slides.html#conditional-moments-exact-vs.-markov-chain-approximation",
    "title": "Machine Learning for Computational Economics",
    "section": "Conditional Moments: Exact vs.¬†Markov Chain Approximation",
    "text": "Conditional Moments: Exact vs.¬†Markov Chain Approximation\n\n\n\n\n\n\n\nImportant\n\n\nTauchen‚Äôs method works particularly well for processes with moderate persistence. For highly persistent processes, the quadrature-based method of Tauchen and Hussey (1991) or the recursive scheme of Rouwenhorst (1995) provides greater accuracy.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-optimization-step",
    "href": "Module02/Module02_Slides.html#the-optimization-step",
    "title": "Machine Learning for Computational Economics",
    "section": "The optimization step",
    "text": "The optimization step\nThe third issue we must address is how to perform the maximization step efficiently.\n\nA direct approach is to specify a grid for the control variable \\(c \\in \\mathcal{C} = \\{c_1, c_2, \\ldots, c_{N_c}\\}\\)\nThe optimal consumption is given by \\(c_t(M) = \\arg\\max_{c} V_t(M, c)\\).\n\n\n\n\n\nThis brute-force approach can be computationally expensive\n\nAn alternative is to use the first-order condition and envelope condition:\n\n\\[\n\\color{#0072b2}{\\underbrace{u'(c_t(M)) = e^{-\\rho} R \\sum_{j=1}^{N_y} P_j\\, V_{t-1}'\\!\\left(R (M - c_t(M)) + Y'_j\\right)}_{\\text{first-order condition}}} , \\qquad \\qquad \\color{#d55e00}{\\underbrace{V_{t}'(M) = u'(c_{t}(M))}_{\\text{envelope condition}}}\n\\]\n\n\n\n\n\nCombining the two conditions, we obtain the consumption Euler equation: \\[\n  u'(c_t(M)) = e^{-\\rho} R \\sum_{j=1}^{N_y} P_j\\, u'\\!\\left(c_{t-1}\\!\\left(R (M - c_t(M)) + Y'_j\\right)\\right).\n\\]\nTo obtain \\(c_t(M)\\), we need to solve a nonlinear equation for each \\(M\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#egm-steps",
    "href": "Module02/Module02_Slides.html#egm-steps",
    "title": "Machine Learning for Computational Economics",
    "section": "The Endogenous Gridpoint Method",
    "text": "The Endogenous Gridpoint Method\nCarroll (2006) proposed the endogenous gridpoint method (EGM) that avoids the root-finding step:\n\n\nDefine a grid for the end-of-period assets \\(a_t(M) \\equiv M - c_t(M)\\): \\[\n   a_t(M) \\in \\mathcal{A} = \\{a_1, a_2, \\ldots, a_{N_a}\\}\n\\]\n\n\n\n\nSolve for consumption inverting the consumption Euler equation: \\[\n   c_{t,i} = u'^{-1}\\!\\left( e^{-\\rho} R \\sum_{j=1}^{N_y} P_j\\, u'\\!\\left(c_{t-1}\\!\\left(R a_i + Y'_j\\right)\\right) \\right).\n\\]\n\n\n\n\nInterpolate the new policy function over the endogenous grid: \\[\n   M_{t,i} = a_i + c_{t,i}.\n\\]\n\n\n\n\nUpdate the value function: \\[\n   V_t(M) = u(c_t(M)) + e^{-\\rho} \\sum_{j=1}^{N_y} P_j\\, V_{t-1}\\!\\left(R (M - c_t(M)) + Y'_j\\right).\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#model-struct",
    "href": "Module02/Module02_Slides.html#model-struct",
    "title": "Machine Learning for Computational Economics",
    "section": "Model Struct",
    "text": "Model Struct\nWe have seen how to represent the value function, compute expectations, and perform the maximization step.\n\nWe are now ready to solve for the value and policy functions.\nTo make the implementation modular and reusable, we encapsulate all parameters and grids in a Julia struct.\n\nBase.@kwdef struct ConsumptionSavingsDT\n    Œ≥::Float64 = 2.0        # CRRA coefficient\n    œÅ::Float64 = 0.05       # discount rate\n    A::Float64 = 1.00       # terminal value function parameter\n    R::Float64 = exp(œÅ)     # interest rate\n    œÉ::Float64 = 0.25       # standard deviation of log income\n    Z::NamedTuple = tauchen(9, 0.0, œÉ) # income process\n    Y::Vector{Float64} = exp.(Z.z)  # income levels\n    N::Int64  = 11          # number of grid points\n    Œ±::Float64 = 0.0        # grid spacing parameter\n    Mgrid::Vector{Float64} = make_grid(0.0, 2.5, N; Œ± = Œ±)\n    agrid::Vector{Float64} = make_grid(0.0, 1.0, N; Œ± = Œ±)\nend\n\nThe struct stores the parameters and grids for the model.\n\nThis lets us pass the entire model cleanly between solvers, simulators, and calibration routines\nMakes it easy to override the baseline calibration via keyword arguments\n\n\nm1 = ConsumptionSavingsDT()\nprintln(\"Risk aversion in model 1: $(m1.Œ≥)\", \" | \", \"Discount rate in model 1: $(m1.œÅ)\")\n\nm2 = ConsumptionSavingsDT(Œ≥ = 1.5)\nprintln(\"Risk aversion in model 2: $(m2.Œ≥)\", \" | \", \"Discount rate in model 2: $(m2.œÅ)\")\n\nRisk aversion in model 1: 2.0 | Discount rate in model 1: 0.05\nRisk aversion in model 2: 1.5 | Discount rate in model 2: 0.05",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#one-step-value-function-iteration",
    "href": "Module02/Module02_Slides.html#one-step-value-function-iteration",
    "title": "Machine Learning for Computational Economics",
    "section": "One-step Value Function Iteration",
    "text": "One-step Value Function Iteration\nWe now solve for \\(V_1(M)\\) and \\(c_1(M)\\), given the terminal condition \\(V_0(M) = \\frac{M^{1-\\gamma}}{1-\\gamma}\\).\n\nWe start with the brute-force approach.\nWe discretize the control space into \\(N_c\\) points and choose the maximizer of the action-value function.\n\n\nWe need to specify the admissible consumption set.\n\nThe lower bound is naturally \\(c=0\\).\nFor the upper bound, feasibility requires that next period‚Äôs cash-on-hand be nonnegative for all realizations of \\(Y'\\): \\[\nc_{\\max}(M) = M + \\frac{Y_{\\min}}{R},\n\\] since choosing \\(c=c_{\\max}(M)\\) leaves \\(M' = R(M-c) + Y_{\\min} = 0\\) next period.\n\n\n\nfunction vf_iteration(m::ConsumptionSavingsDT, V0::Function; NM::Int64 = 11, Nc::Int64 = 101)\n    (; Y, Z, R, Œ≥, œÅ) = m # unpack model parameters\n    Mgrid = collect(range(-Y[1]/R, 3.0, length=NM)) # grid for M\n    cgrid = [range(0.0, m + Y[1]/R, length=Nc) for m in Mgrid] # collection of grids for c\n    # Action-value function\n    V1(M, c) = c^(1-Œ≥)/(1-Œ≥) + exp(-œÅ) * sum(Z.P[1,j] * V0(m,R * (M-c) + Y[j]+1e-12) for j in eachindex(Y))\n    # Policy and value functions\n    C = [cgrid[j][argmax([V1(Mgrid[j], c) for c in cgrid[j]])] for j in eachindex(Mgrid)] \n    V = [V1(Mgrid[j], C[j]) for j in eachindex(Mgrid)]\n    return (; C, V, Mgrid) # return a named tuple\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#policy-and-value-functions",
    "href": "Module02/Module02_Slides.html#policy-and-value-functions",
    "title": "Machine Learning for Computational Economics",
    "section": "Policy and Value Functions",
    "text": "Policy and Value Functions\nTwo theoretical bounds for the policy function:\n\nLower bound: household receives the lowest income with certainty.\nUpper bound: household receives the average income with certainty.\n\n\n\n\n\n\na) Policy functions\n\n\n\n\nb) Value functions",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-marginal-propensity-to-consume",
    "href": "Module02/Module02_Slides.html#the-marginal-propensity-to-consume",
    "title": "Machine Learning for Computational Economics",
    "section": "The Marginal Propensity to Consume",
    "text": "The Marginal Propensity to Consume\nGiven the policy function \\(c_t(M)\\), we can obtain the marginal propensity to consume (MPC).\n\nThe MPC measures the change in consumption for a change in \\(M\\): \\(MPC(M) = \\frac{d c_t(M)}{d M}\\).\n\n\nCarroll and Kimball (1996) show that the MPC is decreasing in \\(M\\).\n\nBut the numerical solution does not necessarily satisfy this property.\n\n\n\n\n\na) MPCs for \\(N_M = 11\\)\n\n\n\n\nb) MPCs for \\(N_M = 101\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-egm-step",
    "href": "Module02/Module02_Slides.html#the-egm-step",
    "title": "Machine Learning for Computational Economics",
    "section": "The EGM Step",
    "text": "The EGM Step\nWe now solve for the value and policy functions using the endogenous gridpoint method (EGM).\n\nThis approaches approximates the policy function directly, rather than the value function.\nWe focus on the case the household can borrow up to the natural borrowing limit.\n\nThe limit varies with the horizon: \\(a_T(M) \\geq \\underline{a}_T\\): \\[\n  \\underline{a}_T = -\\sum_{t=1}^{T} \\frac{Y_{\\min}}{R^t}.\n\\]\nWe work with the transformed grid: \\[\n  \\tilde{a}_t(M) = a_t(M) - \\underline{a}_t,\n\\]\n\nfunction egm_step(m::ConsumptionSavingsDT, iter::Int, c0::Function)\n    (; agrid, Z, Y, R, Œ≥, œÅ) = m # unpack model parameters\n    agrid_shifted = -Y[1] * sum(R.^(-(1:iter))) .+ agrid \n    # compute the consumption policy\n    c1 = [sum(exp(-œÅ) * Z.P[1,j] * R * c0(R * a + Y[j]+1e-12)^(-Œ≥) \n        for j in eachindex(Y))^(-1/Œ≥) for a in agrid_shifted] \n    M1 = agrid_shifted .+ c1 # compute the cash-on-hand\n    return (; c = linear_interpolation(M1, c1; \n        extrapolation_bc=Line()), M = M1)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-egm-iteration",
    "href": "Module02/Module02_Slides.html#the-egm-iteration",
    "title": "Machine Learning for Computational Economics",
    "section": "The EGM Iteration",
    "text": "The EGM Iteration\nWe iterate the EGM step until the policy function converges.\n# Endogenous gridpoint method iteration\nm = ConsumptionSavingsDT(N = 100, Œ± = 1.5)\npolicies = [egm_step(m, 1, M-&gt;M)]\nfor i = 2:8\n    push!(policies, egm_step(m, i, M-&gt;policies[i-1][1](M)))\nend\n\nWe can compute the marginal propensity to consume (MPC) using the finite-difference method.\n# Compute finite difference derivative\nfunction fd_derivative(grid, x)\n    x_interp = linear_interpolation(grid, x)\n    return [Interpolations.gradient(x_interp, x)[1] for x in grid]\nend\n\n\n\n\n\n\na) Policy function\n\n\n\n\nb) Marginal propensity to consume",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-three-curses-of-dimensionality-revisited",
    "href": "Module02/Module02_Slides.html#the-three-curses-of-dimensionality-revisited",
    "title": "Machine Learning for Computational Economics",
    "section": "The Three Curses of Dimensionality Revisited",
    "text": "The Three Curses of Dimensionality Revisited\nThe methods discussed in this module are the backbone of modern computational economics.\n\nThey are very effective in one or two dimensions.\nHowever, they suffer from the three curses of dimensionality.\n\n\n1) The curse of representation\n\nWe represented the value and policy functions on a grid.\nThe number of grid points grows exponentially with the number of state variables.\n\n\n\n2) The curse of optimization\n\nThe EGM method enable us to avoid the costly root-finding step with a single control variable.\nBut this only works seamlessly in the case of a single control variable.\n\n\n\n3) The curse of expectation\n\nWe computed expectations using the Tauchen method.\nThe Markov chain approximation becomes increasingly costly with more shocks",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#the-way-forward",
    "href": "Module02/Module02_Slides.html#the-way-forward",
    "title": "Machine Learning for Computational Economics",
    "section": "The Way Forward",
    "text": "The Way Forward\nThe techniques discussed in this module suffer from the three curses of dimensionality.\n\nOvercoming these limitations requires new tools.\n\n\n\n\nWe will rely on a combination of two main ingredients:\n\nContinuous-time methods combined with machine learning techniques.\n\n\n\n\nBefore jumping into machine learning, we discuss next continuous-time methods.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module02/Module02_Slides.html#references",
    "href": "Module02/Module02_Slides.html#references",
    "title": "Machine Learning for Computational Economics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nCarroll, Christopher D. 2006. ‚ÄúThe Method of Endogenous Gridpoints for Solving Dynamic Stochastic Optimization Problems.‚Äù Economics Letters 91 (3): 312‚Äì20. https://doi.org/10.1016/j.econlet.2005.09.013.\n\n\nCarroll, Christopher D., and Miles S. Kimball. 1996. ‚ÄúOn the Concavity of the Consumption Function.‚Äù Econometrica 64 (4): 981‚Äì92. https://doi.org/10.2307/2171853.\n\n\nRouwenhorst, K. Geert. 1995. ‚ÄúAsset Pricing Implications of Equilibrium Business Cycle Models.‚Äù In Frontiers of Business Cycle Research, edited by Thomas F. Cooley, 294‚Äì330. Princeton, NJ: Princeton University Press.\n\n\nTauchen, George, and Robert Hussey. 1991. ‚ÄúQuadrature-Based Methods for Obtaining Approximate Solutions to Nonlinear Asset Pricing Models.‚Äù Econometrica 59 (2): 371‚Äì96. https://doi.org/10.2307/2938261.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 2 ‚Äì Discrete-Time Methods"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#about-me",
    "href": "Module01/Module01_Slides.html#about-me",
    "title": "Machine Learning for Computational Economics",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\n\n\nBrazilian economist\n\nMasters from University of Sao Paulo\n\nPhD in Economics from MIT\n\n\n\n\n\n\nResearch on macro-finance topics\n\nMonetary policy / financial intermediation\n\nApplications of ML to macro-finance\n\n\n\n\n\n\nProfessor at Purdue University üåê\n\nPreviously, Princeton and UIUC",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-need-for-high-dimensional-dynamic-problems",
    "href": "Module01/Module01_Slides.html#the-need-for-high-dimensional-dynamic-problems",
    "title": "Machine Learning for Computational Economics",
    "section": "The need for high-dimensional dynamic problems",
    "text": "The need for high-dimensional dynamic problems\nDynamic programming is one of the cornerstones of modern economics:\n\nIt is a central part of how we tackle many of the biggest topics in our field\ne.g.: consumption and firm behavior, asset prices, climate economics, public finance, etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeed to solve high-dimensional dynamic problems with many:\n\ninvestors / assets\ncountries / regions\nconsumers / goods\n\n\n\n\n\n\n Takeaway: realistic models must be high-dimensional.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-three-curses-of-dimensionality",
    "href": "Module01/Module01_Slides.html#the-three-curses-of-dimensionality",
    "title": "Machine Learning for Computational Economics",
    "section": "The three curses of dimensionality",
    "text": "The three curses of dimensionality\nChallenge: problem is plagued by the curse of dimensionality Powell (2011)\n\nComputational cost grows exponentially with dimensionality\nThis curse appears in three forms ‚Äî each requiring a different solution.\n\n\n\n\n\n\n1) The curse of representation\n\n\n\n2) The curse of optimization\n\n\n\n3) The curse of expectation",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-continuous-time-problem",
    "href": "Module01/Module01_Slides.html#the-continuous-time-problem",
    "title": "Machine Learning for Computational Economics",
    "section": "The continuous-time problem",
    "text": "The continuous-time problem\nOptimal control problem for an infinitely-lived agent:\n\nStates: \\(\\mathbf{s} \\in \\mathcal{S} \\subset \\mathbb{R}^n\\)\n\n\n\nControls: \\(\\mathbf{c} \\in \\Gamma(\\mathbf{s}) \\subset \\mathbb{R}^p\\)\n\n\n\n\nShocks: \\(\\mathbf{B}_t \\in \\mathbb{R}^m\\)\n\n\n\nOptimal control problem:\n\\[V(\\mathbf{s})  = \\max_{\\{\\mathbf{c}_t\\}_{t=0}^\\infty} \\mathbb{E}\\left[ \\left. \\int_{0}^\\infty e^{-\\rho t} u(\\mathbf{c}_t) dt  \\right| \\mathbf{s} \\right]\\]\nsubject to\n\n\\[\\begin{align*}\n  d \\mathbf{s}_t &= \\color{#d55e00}{\\underbrace{\\mathbf{f} (\\mathbf{s}_t, \\mathbf{c}_t)}_{\\text{drift }(n\\times 1)}} dt + \\color{#009e73}{\\underbrace{\\mathbf{g}(\\mathbf{s}_t, \\mathbf{c}_t)}_{\\text{diffusion }(n\\times m)}} d \\mathbf{B}_t\\\\\n  \\mathbf{c}_t &\\in \\Gamma(\\mathbf{s}_t), \\forall t \\in [0,\\infty),\n\\end{align*}\\]\n\ngiven \\(\\mathbf{s}_0 = \\mathbf{s}\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#example-ngm-with-o-u-productivity-shocks",
    "href": "Module01/Module01_Slides.html#example-ngm-with-o-u-productivity-shocks",
    "title": "Machine Learning for Computational Economics",
    "section": "Example: NGM with O-U productivity shocks",
    "text": "Example: NGM with O-U productivity shocks\nExample: neoclassical growth model (NGM)\n\nOrnstein-Uhlenbeck productivity process: \\[d A_t = - \\phi (A_t- \\overline{A}) dt + \\sigma_A d B_t.\\]\n\n\n\n\nMapping to general problem:\n\n\\(\\mathbf{s}_t = (k_t, A_t)'\\)\n\\(\\mathbf{c}_t = c_t\\)\n\\(\\mathbf{f}(\\mathbf{s}_t, \\mathbf{c}_t) = [A_t k_t^\\alpha - \\delta k_t - c_t, -\\phi(A_t -\\overline{A})]'\\)\n\\(\\mathbf{g}(\\mathbf{s}_t,\\mathbf{c}_t) = [0,\\sigma_A]'\\)\n\n\n\n\nFormulation is quite flexible\n\nIt accommodates multiple controls (e.g.¬†multiple assets/goods) or shocks\nDuarte, Duarte, Silva (2024) shows a range of macro-finance problems fit this framework",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-discrete-time-problem",
    "href": "Module01/Module01_Slides.html#the-discrete-time-problem",
    "title": "Machine Learning for Computational Economics",
    "section": "The discrete-time problem",
    "text": "The discrete-time problem\nDiscrete-time problem:\n\\[V(\\mathbf{s})  = \\max_{\\{\\mathbf{c}_t\\}_{t=0}^\\infty} \\mathbb{E}\\left[ \\left. \\sum_{k=0}^\\infty e^{-\\rho k \\Delta_t} u(\\mathbf{c}_k)\\Delta_t \\right| \\mathbf{s} \\right]\\]\nsubject to\n\\[\\begin{align*}\n  \\mathbf{s}_{t+1}-\\mathbf{s}_{t} &= \\mathbf{f} (\\mathbf{s}_t, \\mathbf{c}_t) \\Delta_t + \\mathbf{g}(\\mathbf{s}_t, \\mathbf{c}_t) \\sqrt{\\Delta_t} u_{t+1}\\\\\n  \\mathbf{c}_t &\\in \\Gamma(\\mathbf{s}_t), \\forall t \\in \\{0,1,\\ldots,\\infty\\}.\n\\end{align*}\\]\n\n\n\nBellman equation:\n\\[V(\\mathbf{s}) = \\max_{\\mathbf{c}\\in\\Gamma(\\mathbf{s})} u(\\mathbf{c})\\Delta_t + e^{-\\rho \\Delta_t} \\mathbb{E}\\left[ V(\\color{#009e73}{\\mathbf{s}'} )|\\mathbf{s} \\right],\\]\nwhere \\(\\mathbf{s}' =  \\mathbf{s} + \\mathbf{f} (\\mathbf{s}, \\mathbf{c}) \\Delta_t + \\mathbf{g}(\\mathbf{s}, \\mathbf{c}) \\sqrt{\\Delta_t} u'\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#vfi-and-pfi",
    "href": "Module01/Module01_Slides.html#vfi-and-pfi",
    "title": "Machine Learning for Computational Economics",
    "section": "VFI and PFI",
    "text": "VFI and PFI\nValue Function Iteration (VFI):\n\nGiven initial guess \\(V^0(\\mathbf{s})\\), iterate until convergence: \\[V^j(\\mathbf{s}) = \\max_{\\mathbf{c}\\in\\Gamma(\\mathbf{s})} u(\\mathbf{c})\\Delta_t + e^{-\\rho \\Delta_t} \\mathbb{E}\\left[ V^{j-1}(\\mathbf{s}')|\\mathbf{s} \\right] \\equiv T V^{j-1}(\\mathbf{s})\\]\n\n\n\n\nPolicy Function Iteration (PFI):\n\nGiven initial guess for policy function \\(\\pi(\\mathbf{s})\\), perform policy evaluation step: \\[V_{\\boldsymbol{\\pi}}(\\mathbf{s}) = u(\\boldsymbol{\\pi}(\\mathbf{s}))\\Delta_t + e^{-\\rho \\Delta_t} \\mathbb{E}\\left[ V_{\\boldsymbol{\\pi}}(\\mathbf{s}') \\right] \\equiv T V_{\\boldsymbol{\\pi}}(\\mathbf{s}).\\]\nThen, perform policy improvement step \\[\\pi'(\\mathbf{s}) = \\arg \\max_{\\mathbf{c}} \\left\\{ u(\\mathbf{c})\\Delta_t + e^{-\\rho \\Delta_t} \\mathbb{E}\\left[ V_{\\boldsymbol{\\pi}}(\\mathbf{s}') \\right] \\right\\},\\] and iterate until convergence.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-first-curse",
    "href": "Module01/Module01_Slides.html#the-first-curse",
    "title": "Machine Learning for Computational Economics",
    "section": "The first curse",
    "text": "The first curse\nSuppose that each state variable takes a finite number of values \\(S\\)\n\n# elements in state space: \\(|\\mathcal{S}| = S^n\\)\n\n\n\n\n\n\nExample: multi-region RBC\n\nModel with a region for each US state: \\(j = 1,\\ldots ,50\\)\nRegional state variables: \\((K_{jt}, A_{jt})\\) \\(\\Rightarrow\\) total \\(n = 100\\) state variables\nIf \\(S = 10\\), then \\(|\\mathcal{S}| = 10^{100}\\)\n\nA challenge even for sparse grid methods such as Smolyak\n\nNeeded: a parsimonious way of representing \\(V(\\mathbf{s})\\)\n\n\n\n\n\n1) The curse of representation\n\n‚ÄúThe state space explodes so quickly that even representing \\(V(\\mathbf{s})\\) becomes impossible.‚Äù",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-second-curse",
    "href": "Module01/Module01_Slides.html#the-second-curse",
    "title": "Machine Learning for Computational Economics",
    "section": "The second curse",
    "text": "The second curse\nFirst-order conditions:\n\\[\\nabla_{\\mathbf{c}} u(\\color{#0072b2}{\\mathbf{c}_{\\boldsymbol{\\pi}}(\\mathbf{s})}) + e^{-\\rho \\Delta t}  \\mathbb{E}\\left[ \\nabla_{\\mathbf{c}}V_{\\boldsymbol{\\pi}}(\\mathbf{s} + \\mathbf{f} (\\mathbf{s}, \\color{#0072b2}{\\mathbf{c}_{\\boldsymbol{\\pi}}(\\mathbf{s})}) \\Delta t + \\mathbf{g}(\\mathbf{s}, \\color{#0072b2}{\\mathbf{c}_{\\boldsymbol{\\pi}}(\\mathbf{s})}) \\sqrt{\\Delta_t} u') \\right] = 0,\\]\n\n\n\n\n\nBrute-force maximization infeasible with many controls\n\nTotal number of control vectors: \\(|\\mathcal{A}| = A^p\\)\n\nThe alternative is a costly non-linear root-finding step\n\nEndogenous gridpoint method (EGM) avoids such step\n\nBut only in the case of a single control\n\nNeeded: algorithm that avoids root-finding at every step\n\n\n2) The curse of optimization\n\n‚ÄúSolving the FOCs becomes a high-dimensional nonlinear root-finding problem.‚Äù",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-third-curse",
    "href": "Module01/Module01_Slides.html#the-third-curse",
    "title": "Machine Learning for Computational Economics",
    "section": "The third curse",
    "text": "The third curse\nWhen \\(u_{t+1}\\) is a \\(m-\\)dimensional vector of standard normal r.v., \\(\\mathbb{E}[V_{\\boldsymbol{\\pi}}(\\mathbf{s}')]\\) corresponds to the integral: \\[\\mathbb{E}[V_{\\boldsymbol{\\pi}}(\\mathbf{s}')] = \\int_{u_1} \\int_{u_2} \\ldots \\int_{u_m} V_{\\boldsymbol{\\pi}}(\\mathbf{s}')\\phi(\\mathbf{u}) du_1 du_2 \\ldots d u_m.\\]\n\n\n\n\nEvaluating integral is very costly with large number of shocks\n\nCost of quadrature solution increases exponentially with \\(m\\)\nWith \\(U\\) possible values, the total number of points is \\(|\\mathcal{U}| = U^m\\)\n\n\n\n\nConsider multi-region RBC example:\n\nWith a shock for every region, we have \\(m = 50\\)\nNeeded: an efficient way to compute expectations\n\n\n3) The curse of expectation\n\n‚ÄúComputing \\(\\mathbb{E}[V(\\mathbf{s}')]\\) requires integrating over a huge shock space.‚Äù",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#overcoming-the-curses",
    "href": "Module01/Module01_Slides.html#overcoming-the-curses",
    "title": "Machine Learning for Computational Economics",
    "section": "Overcoming the curses",
    "text": "Overcoming the curses\nThe goal of this course is to show how to overcome the three curses of dimensionality\n\nThe key will be to use machine learning techniques to handle each of the curses\nWe will learn how to represent functions, train models, and how to use automatic differentiation\n\n\n\n\n\nDeep neural network architecture\n\n\n\n\nTraining history",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#overview-of-the-course",
    "href": "Module01/Module01_Slides.html#overview-of-the-course",
    "title": "Machine Learning for Computational Economics",
    "section": "Overview of the course",
    "text": "Overview of the course\nBesides this introductory module, the course is organized into four modules.\n\nThe first two modules cover classical numerical methods\n\nThe last two modules cover machine learning techniques\n\n\n\n\n\n\n\nModule 02: Discrete-Time Methods\n\nTauchen‚Äôs discretization method\nValue function iteration\nEndogenous gridpoint method\n\n\nModule 03: Continuous-Time Methods\n\nFinite-difference methods\nStability/consistency/monotonicity\nSpectral methods\n\n\n\n\n\n\n\n\n\nModule 04: Fundamentals of Machine Learning\n\nSupervised learning and neural networks\nOptimization algorithms\nAutomatic differentiation\n\n\nModule 05: The Deep Policy Iteration (DPI) Method\n\nHyper-dual approach to It√¥‚Äôs lemma\nDeep policy iteration algorithm\nApplications",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#the-julia-programming-language",
    "href": "Module01/Module01_Slides.html#the-julia-programming-language",
    "title": "Machine Learning for Computational Economics",
    "section": "The Julia programming language",
    "text": "The Julia programming language\nThe course is meant to be practical and hands-on.\n\nThe course will teach you the theory\nBut also focus on the practical implementation\n\n\nWe will use the Julia programming language to implement the methods discussed in the course.\n\nJulia is a modern, high-level, high-performance programming language\nGreat resource to learn more about Julia: Quantitative Economics with Julia\n\n\n\nHere is an example of a Julia code block:\n\nusing Plots\nx = range(-1, 1, length = 100)\ny = x .^ 2\nplot(x, y, label = \"x^2\", line = 3, xlabel = \"x\", ylabel = \"y\", size = (300, 200))",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#conclusion",
    "href": "Module01/Module01_Slides.html#conclusion",
    "title": "Machine Learning for Computational Economics",
    "section": "Conclusion",
    "text": "Conclusion\nThere is a large gap between the models we want to solve and the models classical methods can handle.\n\nClassical numerical methods are limited by the three curses of dimensionality\nRelated to how to represent functions, how to find optimal decisions, and how to compute expectations\n\n\n\n\nThis course introduces techniques to allow us to bridge this gap.\n\nWe will learn how to use machine learning methods to overcome the three curses\nWe will learn how to use these methods to solve models that were previously intractable\n\n\n\n\nThe course will be a mix of theory and practice.\n\nWe will learn the theory behind the methods\nBut also focus on the practical implementation using Julia.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module01/Module01_Slides.html#references",
    "href": "Module01/Module01_Slides.html#references",
    "title": "Machine Learning for Computational Economics",
    "section": "References",
    "text": "References\n\n\nDuarte, Duarte, Silva. (2024). Machine learning for continuous-time finance. The Review of Financial Studies. 37. (11). :3217‚Äì3271 retrieved, from https://academic.oup.com/rfs/article/37/11/3217/7749384\n\n\nPowell. (2011). Approximate dynamic programming: Solving the curses of dimensionality.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 1 ‚Äì Introduction"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#introduction",
    "href": "Module03/Module03_Slides.html#introduction",
    "title": "Machine Learning for Computational Economics",
    "section": "Introduction",
    "text": "Introduction\nContinuous-time formulations provide an elegant counterpart to discrete-time dynamic programming.\n\nInstead of expectations over future states, they express optimal behavior through differential operators.\nThe solution is characterized by a partial differential equation (PDE).\n\n\n\n\nIn this module, we revisit the consumption-savings problem from Module 02.\n\nWe show how to derive the Hamilton-Jacobi-Bellman (HJB) equation.\nWe solve the HJB equation numerically using finite-difference schemes and spectral methods.\n\n\n\n\nThe module is organized as follows:\n\nDerivation of the HJB equation.\nSolution and stability of finite-difference schemes.\nApplication of spectral methods.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#a-portfolio-choice-problem-with-uninsurable-income",
    "href": "Module03/Module03_Slides.html#a-portfolio-choice-problem-with-uninsurable-income",
    "title": "Machine Learning for Computational Economics",
    "section": "A Portfolio Choice Problem with Uninsurable Income",
    "text": "A Portfolio Choice Problem with Uninsurable Income\nWe extend the consumption-savings problem from Module 02 in two dimensions:\n\nThe household faces a portfolio choice problem: she decides how much to save in a safe and a risky asset.\nLabor income follows a persistent Markov process rather than being i.i.d. over time.\n\n\n\nAsset returns:\nRiskless return over a small interval \\(\\Delta t\\) is \\[\n  R_f = 1 + r\\,\\Delta t,\n\\] and the return on the risky asset is: \\[\n  R_{r,t+\\Delta t} = 1 + \\mu_r\\,\\Delta t + \\sigma_r \\sqrt{\\Delta t}\\,\\varepsilon_{r,t+\\Delta t},\n\\] where \\(\\varepsilon_{r,t+\\Delta t}\\) is a unit-variance white noise.\n\nLabor income:\nLabor income follows a (discrete-state) Markov chain: \\[\n  Y_t \\in \\{Y_1, \\ldots, Y_{N_y}\\},\n\\] with transition probabilities \\[\n  P_{ij} = \\lambda_{ij}\\,\\Delta t, \\qquad j \\neq i,\n\\] and \\(P_{ii} = 1 - \\sum_{j \\neq i} P_{ij}\\).\n\n\n\n\n\n\n\n\nNote\n\n\nThe scaling ensures that mean and variance of the risky return are of order \\(\\Delta t\\). To see this, fix a horizon \\(T = n\\,\\Delta t\\) and note that \\[\n    \\operatorname{Var}\\!\\left[\\sum_{k=1}^{n} R_{r,t+k\\Delta t}\\right]\n    = n\\,\\sigma_r^2\\,\\Delta t\n    = \\sigma_r^2\\,T.\n  \\] Hence, as we partition the time interval into finer and finer subperiods, the total amount of risk over \\([t,\\,t+T]\\) remains unchanged.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-bellman-equation-with-time-step-delta-t",
    "href": "Module03/Module03_Slides.html#the-bellman-equation-with-time-step-delta-t",
    "title": "Machine Learning for Computational Economics",
    "section": "The Bellman Equation with Time Step \\(\\Delta t\\)",
    "text": "The Bellman Equation with Time Step \\(\\Delta t\\)\nThe Bellman equation for this discrete-time formulation is \\[\n  V_t(W, Y) = \\max_{c, \\alpha} \\left\\{ u(c)\\,\\Delta t + e^{-\\rho \\Delta t}\\, \\mathbb{E}\\left[V_{t-\\Delta t}(W', Y')\\right] \\right\\},\n\\] subject to the law of motion for wealth, \\[\n  W' = R_{p,t+\\Delta t}\\,\\bigl(W + (Y - c)\\,\\Delta t\\bigr),\n  \\qquad\n  R_{p,t+\\Delta t} = (1-\\alpha)\\,R_f + \\alpha\\,R_{r,t+\\Delta t},\n\\] and a borrowing limit \\(W \\geq \\underline{W}\\), given a transition matrix \\(\\Pr(Y' = Y_j \\mid Y = Y_i) = P_{ij}\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhen deriving the continuous-time limit, it is essential to distinguish between flow and stock variables. The cash-on-hand variable, \\[\n    M = W + Y\\,\\Delta t,\n  \\] combines a stock (wealth) with a flow (labor income). As \\(\\Delta t \\to 0\\), this distinction vanishes, and \\(M\\) and \\(W\\) coincide in the limit.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#state-dynamics-in-continuous-time",
    "href": "Module03/Module03_Slides.html#state-dynamics-in-continuous-time",
    "title": "Machine Learning for Computational Economics",
    "section": "State Dynamics in Continuous Time",
    "text": "State Dynamics in Continuous Time\nWealth dynamics.\n\nThe discrete-time wealth process can be expressed as \\[\nW_{t+\\Delta t} - W_t = [(1-\\alpha_t) r W_t + \\alpha_t \\mu_r W_t + Y_t - c_t]\\Delta t + \\alpha_t \\sigma_r W_t \\sqrt{\\Delta t}\\,\\varepsilon_{r,t+\\Delta t} + o(\\Delta t),\n\\]\n\nTaking the limit as \\(\\Delta t \\to 0\\), we obtain the continuous-time wealth dynamics: \\[\n  dW_t = [(1-\\alpha_t) r W_t + \\alpha_t \\mu_r W_t + Y_t - c_t] dt + \\alpha_t \\sigma_r W_t\\, dB_t,\n\\] where \\(B_t\\) is a standard Brownian motion.\n\n\n\n\nLabor income dynamics.\n\nThe expected change in income over a small interval \\(\\Delta t\\) is \\[\n\\mathbb{E}\\left[Y_{t+\\Delta t} - Y_t \\mid Y_t = Y_i\\right] = \\sum_{j \\neq i} \\lambda_{ij} (Y_j - Y_i)\\,\\Delta t.\n\\] In continuous time, this implies the following jump process for income: \\[\ndY_t = \\sum_{j} (Y_j - Y_i)\\, dN_{ij,t}, \\qquad \\mathbb{E}[dN_{ij,t}] = \\lambda_{ij}\\,dt,\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-hamilton-jacobi-bellman-equation",
    "href": "Module03/Module03_Slides.html#the-hamilton-jacobi-bellman-equation",
    "title": "Machine Learning for Computational Economics",
    "section": "The Hamilton-Jacobi-Bellman Equation",
    "text": "The Hamilton-Jacobi-Bellman Equation\nWe now derive the continuous-time version of the Bellman equation: \\[\n  0\n  = \\max_{c, \\alpha}\n    \\Big\\{\n      u(c)\n      + \\frac{\\mathbb{E}\\!\\left[V_{t-\\Delta t}(W',Y') - V_t(W,Y)\\,\\middle|\\,W,Y\\right]}{\\Delta t}\n      - \\rho\\,V_t(W,Y)\n    \\Big\\}\n    + O(\\Delta t).\n\\] subject to the wealth dynamics and labor income dynamics.\n\n\n\n\nTaking the limit as \\(\\Delta t \\to 0\\), we obtain the continuous-time Bellman equation: \\[\n  \\rho V_t(W,Y)\\,dt\n  = \\max_{c, \\alpha}\n    \\Big\\{\n      u(c)\\,dt\n      + \\color{#d55e00}{\\underbrace{\\mathbb{E}\\!\\left[dV_t(W,Y)\\right]}_{\\text{drift term}}}\n    \\Big\\}.\n\\]\n\n\n\n\n\nApplying It√¥‚Äôs lemma to \\(V_t(W,Y)\\), we obtain the Hamilton-Jacobi-Bellman (HJB) equation:\n\\[\n\\rho V_t(W,Y_i)\n  = \\max_{c,\\alpha}\n    \\Big\\{\n      u(c)\n      - \\frac{\\partial V_t}{\\partial t}\n      + \\mathcal{D}V_t(W,Y_i)\n      + \\sum_{j \\neq i} \\lambda_{ij}\\big[V_t(W,Y_j) - V_t(W,Y_i)\\big]\n    \\Big\\},\n\\] where \\(\\mathcal{D}\\) is the Dynkin operator: \\[\n  \\mathcal{D}V_t(W,Y)\n  = V_W\\,[r W + \\alpha (\\mu_r - r) W + Y - c_t]\n    + \\tfrac{1}{2} V_{WW}\\,(\\alpha \\sigma_r W)^2.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#a-blackscholesmerton-example",
    "href": "Module03/Module03_Slides.html#a-blackscholesmerton-example",
    "title": "Machine Learning for Computational Economics",
    "section": "A Black‚ÄìScholes‚ÄìMerton Example",
    "text": "A Black‚ÄìScholes‚ÄìMerton Example\nWe now show how to solve the HJB equation numerically using finite differences.\n\nAs a warm-up exercise, we consider an option pricing problem in the Black‚ÄìScholes‚ÄìMerton framework.\nThis will provide a simpler setting where we can illustrate the main idea of the finite-difference method.\n\n\n\n\n\nAsset dynamics:\n\nThe risk-free rate is constant and equal to \\(r\\),\nThe stock price follows a geometric Brownian motion: \\[\ndS_t = \\mu_S S_t\\,dt + \\sigma_S S_t\\,dB_t,\n\\] where \\(B_t\\) is a standard Brownian motion.\n\n\n\n\n\n\nNo arbitrage:\n\nWe are interested in pricing the option using the principle of no-arbitrage.\nThe absence of arbitrage opportunities implies the existence of a stochastic discount factor \\(\\pi_t\\): \\[\nd\\pi_t = -r\\,\\pi_t\\,dt - \\eta\\,\\pi_t\\,dB_t,\n\\] where \\(\\eta \\equiv \\frac{\\mu_S - r}{\\sigma_S}\\) is the market price of risk.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-blackscholesmerton-pde",
    "href": "Module03/Module03_Slides.html#the-blackscholesmerton-pde",
    "title": "Machine Learning for Computational Economics",
    "section": "The Black‚ÄìScholes‚ÄìMerton PDE",
    "text": "The Black‚ÄìScholes‚ÄìMerton PDE\nThe value of a call option is given by \\[\n  V_T(S) = \\mathbb{E}_0\\!\\left[ \\frac{\\pi_T}{\\pi_0}\\,\\max(S_T - K, 0) \\,\\middle|\\, S_0 = S \\right].\n\\] with terminal payoff \\(V_T(S) = \\max(S - K, 0)\\).\n\n\n\n\nThe HJB equation for this problem at time \\(t \\geq 0\\) is \\[\n  0 = \\mathbb{E}_t\\!\\left[\\, d \\bigl(\\pi_t\\,V_{T-t}(S_t)\\bigr) \\right].\n\\]\nApplying It√¥‚Äôs lemma to \\(\\pi_t V_{T-t}(S_t)\\), we obtain the Black‚ÄìScholes‚ÄìMerton PDE: \\[\n  -r\\,V_T(S)\n  - \\frac{\\partial V_T(S)}{\\partial T}\n  + r\\,S\\,\\frac{\\partial V_T(S)}{\\partial S}\n  + \\tfrac{1}{2}\\sigma_S^2 S^2\\,\\frac{\\partial^2 V_T(S)}{\\partial S^2} = 0,\n\\] with terminal condition \\(V_0(S) = \\max(S - K, 0)\\).\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIt is convenient to express the PDE in terms of the log stock price \\(s \\equiv \\log S\\): \\[\n  -r\\,v_T(s)\n  - \\frac{\\partial v_T(s)}{\\partial T}\n  + \\overline{r}\\,\\frac{\\partial v_T(s)}{\\partial s}\n  + \\frac{1}{2}\\sigma_S^2\\,\\frac{\\partial^2 v_T(s)}{\\partial s^2} = 0,\n\\] with terminal condition \\(v_0(s) = \\max(e^s - K, 0)\\), where \\(\\overline{r} \\equiv r - \\tfrac{1}{2}\\sigma_S^2\\) is the risk-adjusted drift.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#finite-difference-approximations",
    "href": "Module03/Module03_Slides.html#finite-difference-approximations",
    "title": "Machine Learning for Computational Economics",
    "section": "Finite-difference approximations",
    "text": "Finite-difference approximations\nWe can solve the PDE by discretizing both time and space (\\(t\\) and \\(s\\)).\n\nLet \\(t \\in \\{t_1, t_2, \\ldots, t_M\\}\\) be the time on a uniform grid.\nLet \\(s \\in \\{s_1, s_2, \\ldots, s_N\\}\\) be the log stock price on a uniform grid.\n\n\n\n\n\nWe can approximate the spatial derivatives in different ways:\n\n\nForward difference: \\[\n  \\frac{\\partial v_{t_n}(s_i)}{\\partial s} = \\frac{v_{i+1}^n - v_i^n}{\\Delta s},\n\\]\n\nBackward difference: \\[\n  \\frac{\\partial v_{t_n}(s_i)}{\\partial s} = \\frac{v_i^n - v_{i-1}^n}{\\Delta s},\n\\]\n\nCentered difference: \\[\n  \\frac{\\partial v_{t_n}(s_i)}{\\partial s} = \\frac{v_{i+1}^n - v_{i-1}^n}{2\\Delta s}.\n\\]\n\n\n\n\n\n\nThe second spatial derivative is approximated by a centered difference: \\[\n  \\frac{\\partial^2 v_{t_n}(s_i)}{\\partial s^2} = \\frac{v_{i+1}^n - 2v_i^n + v_{i-1}^n}{\\Delta s^2}.\n\\]\n\n\n\n\n\nThe time derivative is approximated by a forward Euler step: \\[\n  \\frac{\\partial v_{t_n}(s_i)}{\\partial T} = \\frac{v_i^{n+1} - v_i^n}{\\Delta t}.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#explicit-scheme",
    "href": "Module03/Module03_Slides.html#explicit-scheme",
    "title": "Machine Learning for Computational Economics",
    "section": "Explicit scheme",
    "text": "Explicit scheme\nWe fix a forward difference in time and consider two versions for the spatial derivative:\n\n\nThe forward-difference version is \\[\n  r\\,v_i^n\n  = -\\frac{v_i^{n+1} - v_i^n}{\\Delta t}\n    + \\overline{r}\\color{#d55e00}{\\,\\frac{v_{i+1}^n - v_i^n}{\\Delta s}}\n    + \\frac{\\sigma_S^2}{2}\\,\\frac{v_{i+1}^n - 2v_i^n + v_{i-1}^n}{\\Delta s^2},\n\\]\n\nThe backward-difference version is \\[\n  r\\,v_i^n\n  = -\\frac{v_i^{n+1} - v_i^n}{\\Delta t}\n    + \\overline{r}\\color{#009e73}{\\,\\frac{v_i^n - v_{i-1}^n}{\\Delta s}}\n    + \\frac{\\sigma_S^2}{2}\\,\\frac{v_{i+1}^n - 2v_i^n + v_{i-1}^n}{\\Delta s^2}.\n\\]\n\n\n\n\n\nRearranging either form gives a common update rule: \\[\n  v_i^{n+1}\n  = -r\\,\\Delta t\\,v_i^n\n    + p_u\\,v_{i+1}^n\n    + p_s\\,v_i^n\n    + p_d\\,v_{i-1}^n, \\qquad i=2, \\ldots, N-1,\n\\] where, letting \\(\\mathbf{1}_{\\text{F}}\\) and \\(\\mathbf{1}_{\\text{B}}\\) denote the forward and backward difference indicators,\n\\[\np_u = \\frac{\\overline{r}\\,\\Delta t}{\\Delta s}\\,\\mathbf{1}_{\\text{F}}\n       + \\frac{\\sigma_S^2\\,\\Delta t}{2\\Delta s^2}, \\qquad \\qquad\n       p_s = 1\n       - \\frac{\\overline{r}\\,\\Delta t}{\\Delta s}\\,(\\mathbf{1}_{\\text{F}} - \\mathbf{1}_{\\text{B}})\n       - \\frac{\\sigma_S^2\\,\\Delta t}{\\Delta s^2}, \\qquad \\qquad\n       p_d = -\\frac{\\overline{r}\\,\\Delta t}{\\Delta s}\\,\\mathbf{1}_{\\text{B}}\n       + \\frac{\\sigma_S^2\\,\\Delta t}{2\\Delta s^2}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nBoundary conditions\n\n\nWe handle boundaries using ghost nodes, i.e., values outside the grid.\n\n\nRight boundary. Option is deep in the money, so \\(\\frac{\\partial v_{t_n}(s_N)}{\\partial s} = e^{s_N}\\): \\[\n  v_{N+1}^n = v_N^n + e^{s_N}\\,\\Delta s.\n\\]\n\nLeft boundary. Option is far out of the money, so \\(\\frac{\\partial v_{t_n}(s_1)}{\\partial s} = 0\\): \\[\n  v_0^n = v_1^n.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#matrix-representation",
    "href": "Module03/Module03_Slides.html#matrix-representation",
    "title": "Machine Learning for Computational Economics",
    "section": "Matrix representation",
    "text": "Matrix representation\nDefine \\(\\mathbf{v}^n = [v_1^n, v_2^n, \\ldots, v_N^n]^\\top\\) and the tridiagonal matrix of coefficients: \\[\n  \\mathbf{P}\n  = \\begin{pmatrix}\n      p_s + p_d  & p_u & 0 & \\ldots & 0 \\\\\n      p_d & p_s  & p_u & \\ldots & 0 \\\\\n      \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n      0 & \\ldots & p_d & p_s  & p_u \\\\\n      0 & \\ldots & 0 & p_d & p_s + p_u\n    \\end{pmatrix}.\n\\] Then the law of motion for \\(\\mathbf{v}^n\\) is \\[\n  \\mathbf{v}^{n+1}\n  = \\mathbf{P}^r\\,\\mathbf{v}^n + \\mathbf{b},\n  \\qquad\n  \\mathbf{P}^r \\equiv \\mathbf{P} - r\\,\\Delta t\\,\\mathbf{I}_N,\n\\] with initial condition \\(\\mathbf{v}^0 = [v_0(s_1), v_0(s_2), \\ldots, v_0(s_N)]^\\top\\), and adjustment vector \\(\\mathbf{b} = [0, \\ldots, 0,\\, p_u\\,e^{s_N}\\,\\Delta s]^\\top\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#julia-implementation",
    "href": "Module03/Module03_Slides.html#julia-implementation",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia implementation",
    "text": "Julia implementation\nWe start by defining the model structure for the Black‚ÄìScholes‚ÄìMerton problem.\n@kwdef struct BlackScholesModel\n    œÉS::Float64 = 0.20\n    r::Float64 = 0.05\n    K::Float64 = 1.0\n    T::Float64 = 1.0\n    Ns::Int64 = 150\n    Nt::Int64 = 300\n    sgrid::LinRange{Float64} = range(-1.5, 1.5, length=Ns)\nend\n\n\n\n\nWe then implement the finite-difference scheme.\nfunction fd_scheme(m::BlackScholesModel; forward::Bool = true)\n    (; œÉS, r, K, T, Ns, Nt, sgrid) = m # unpack model parameters\n    Œîs, Œît = sgrid[2] - sgrid[1], T / (Nt - 1) # spatial/time steps\n    rÃÖ = r - 0.5 * œÉS^2 # risk-adjusted drift\n    # Coefficients for the tridiagonal matrix\n    pu = rÃÖ * Œît / Œîs * forward + œÉS^2 * Œît / (2 * Œîs^2)\n    ps = 1 - rÃÖ * Œît / Œîs * (2*forward-1) - œÉS^2 * Œît / (Œîs^2)\n    pd = -rÃÖ * Œît / Œîs * (1-forward) + œÉS^2 * Œît / (2 * Œîs^2)\n    e  = zeros(Ns)\n    e[1], e[end] = pd, pu # set boundary conditions\n    P = Tridiagonal(pd*ones(Ns-1), ps*ones(Ns)+e, pu*ones(Ns-1))\n    # Boundary conditions\n    b = zeros(Ns)\n    b[end] = pu * exp(sgrid[end]) * Œîs\n    # Initial condition\n    v = @. max(0.0, exp(sgrid) - K) # terminal condition\n    for n in 2:Nt\n        v = (P - r * Œît * I) * v + b # update rule\n    end\n    return (; P, b, v)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#forward-vs.-backward-differences",
    "href": "Module03/Module03_Slides.html#forward-vs.-backward-differences",
    "title": "Machine Learning for Computational Economics",
    "section": "Forward vs.¬†Backward Differences",
    "text": "Forward vs.¬†Backward Differences\n\n\nForward difference\n\n\n\nBackward difference\n\n\n\nTop row: positive drift. Bottom row: negative drift.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#markov-chain-approximation-mca",
    "href": "Module03/Module03_Slides.html#markov-chain-approximation-mca",
    "title": "Machine Learning for Computational Economics",
    "section": "Markov Chain Approximation (MCA)",
    "text": "Markov Chain Approximation (MCA)\nThe plots above motivate the use of an upwind scheme:\n\nUse a forward difference when the drift is positive.\nUse a backward difference when the drift is negative.\n\nThis choice suppresses numerical oscillations and improves stability of the explicit scheme.\n\n\n\n\nBut why does upwinding work?\n\nA useful perspective comes from the Markov Chain Approximation (MCA) of Kushner and Dupuis (2001).\n\nThe MCA approach starts from the discretized Bellman equation \\[\n  v_T(s) = (1 - r\\,\\Delta t)\\,\\mathbb{E}\\!\\left[\\,v_{T-\\Delta t}(s') \\,\\middle|\\, s\\,\\right],\n\\] under the risk‚Äìneutral log‚Äìprice dynamics with drift \\(\\overline r\\) and variance \\(\\sigma_S^2\\): \\[\n  \\mathbb{E}[\\,s' - s \\mid s\\,] = \\overline r\\,\\Delta t,\n  \\qquad\n  \\operatorname{Var}[\\,s' \\mid s\\,] = \\sigma_S^2\\,\\Delta t.\n\\]\n\n\n\n\n\nKey idea: Let‚Äôs assume \\(s\\) follows a Markov chain on the grid \\(\\{s_i\\}_{i=1}^N\\)\n\nStarting at \\(s_i\\), the log price can move only to \\(s_{i-1}\\), \\(s_i\\), or \\(s_{i+1}\\)\nThe transition probabilities \\((p_d, p_s, p_u)\\) are chosen to match the local mean and variance.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-cfl-condition",
    "href": "Module03/Module03_Slides.html#the-cfl-condition",
    "title": "Machine Learning for Computational Economics",
    "section": "The CFL Condition",
    "text": "The CFL Condition\nA locally consistent choice for the transition probabilities is \\[\n  p_u = \\frac{\\overline r\\,\\Delta t}{\\Delta s}\\,\\mathbf{1}_{\\{\\overline r \\ge 0\\}} + \\frac{\\sigma_S^2\\,\\Delta t}{2\\Delta s^2},\\quad\n  p_s = 1 - \\frac{|\\overline r|\\,\\Delta t}{\\Delta s} - \\frac{\\sigma_S^2\\,\\Delta t}{\\Delta s^2},\\quad\n  p_d = -\\,\\frac{\\overline r\\,\\Delta t}{\\Delta s}\\,\\mathbf{1}_{\\{\\overline r &lt; 0\\}} + \\frac{\\sigma_S^2\\,\\Delta t}{2\\Delta s^2}.\n\\]\n\n\n\n\nWe need to ensure that \\((p_d, p_s, p_u)\\) are non-negative and sum to 1.\n\nThese satisfy \\(p_u + p_s + p_d = 1\\) by construction.\nUpwinding ensures \\(p_u \\ge 0\\) and \\(p_d \\ge 0\\)\n\n\n\n\n\n\nTo ensure the nonnegativity of \\(p_s\\), we need to impose the Courant‚ÄìFriedrichs‚ÄìLewy (CFL) condition: \\[\n  1 - \\frac{|\\overline r|\\,\\Delta t}{\\Delta s} - \\frac{\\sigma_S^2\\,\\Delta t}{\\Delta s^2} \\ge 0\n  \\quad\\Longleftrightarrow\\quad\n  \\Delta t \\le \\frac{\\Delta s^2}{\\,|\\overline r|\\,\\Delta s + \\sigma_S^2\\,}.\n\\]\n\n\n\n\n\n\nImportant\n\n\nThe CFL condition is crucial to ensure that the transition probabilities are non-negative.\n\nViolating it produces instability and spurious oscillations in the numerical solution.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#discretization-error",
    "href": "Module03/Module03_Slides.html#discretization-error",
    "title": "Machine Learning for Computational Economics",
    "section": "Discretization Error",
    "text": "Discretization Error\nA key aspect of the CFL condition for the explicit scheme is a restriction on \\(\\Delta t\\)\n\nThe time step \\(\\Delta t\\) must shrink as the spatial step \\(\\Delta s\\) shrinks to maintain stability.\nWe can illustrate this with a heatmap of the discretization error.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-barles-souganidis-conditions",
    "href": "Module03/Module03_Slides.html#the-barles-souganidis-conditions",
    "title": "Machine Learning for Computational Economics",
    "section": "The Barles-Souganidis Conditions",
    "text": "The Barles-Souganidis Conditions\nThe analysis above provided a probabilistic intuition for the stability of the finite-difference scheme.\n\nA complementary and more general approach is given by the Barles and Souganidis (1991) theorem.\nThe theorem provides sufficient conditions for the convergence of the finite-difference schemes\n\n\nMain result. Consider a numerical scheme of the general form \\[\n  F^h(x, V(x), V(\\cdot)) = 0,\n\\] where \\(h\\) collects the discretization parameters (e.g., \\(\\Delta t\\), \\(\\Delta s\\)), and \\(V\\) is the numerical approximation to the value function.\nBarles and Souganidis (1991) provide conditions for the convergence to the viscosity solution of the HJB equation.\n\n\n\n\n\n\n\nBarles‚ÄìSouganidis theorem\n\n\nSuppose that the continuous-time problem admits a unique bounded viscosity solution. If a numerical scheme is:\n\nMonotone: The numerical operator \\(F^h\\) is non-decreasing in \\(V\\).\nStable: The sequence of numerical approximations \\(V^h\\) remains uniformly bounded.\nConsistent: As \\(h \\to 0\\), the discrete operator \\(F^h\\) converges to the continuous operator \\(F\\) defining the PDE.\n\nThen the scheme converges locally uniformly to the viscosity solution of the HJB equation.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nA viscosity solution is a generalized notion of solution to nonlinear PDEs like the HJB equation, which applies to problems where the value function may exhibit kinks or corners, as is typical in dynamic optimization problems with borrowing constraints or nonconvexities.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#connection-to-the-explicit-upwind-scheme",
    "href": "Module03/Module03_Slides.html#connection-to-the-explicit-upwind-scheme",
    "title": "Machine Learning for Computational Economics",
    "section": "Connection to the explicit upwind scheme",
    "text": "Connection to the explicit upwind scheme\nConsider our explicit upwind scheme: \\[\n  v_i^{n+1} = p_u^r\\,v_{i+1}^n + p_s^r\\,v_i^n + p_d^r\\,v_{i-1}^n + b_i^n,\n\\] with coefficients \\(p_u^r = p_u, p_s^r = p_s-r\\,\\Delta t, p_d^r = p_d\\).\nThe scheme is therefore:\n\nMonotone: if \\(p_u^r, p_s^r, p_d^r \\ge 0\\) ‚Äî that is, when upwinding and the CFL condition hold.\nStable: if \\(\\|v^n\\|\\) remains bounded, which again follows from \\(p_u^r + p_s^r + p_d^r = 1-r\\,\\Delta t \\leq 1\\) and nonnegativity.\nConsistent: because the finite-difference approximations converge to their continuous counterparts as \\(\\Delta s, \\Delta t \\to 0\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#implicit-schemes",
    "href": "Module03/Module03_Slides.html#implicit-schemes",
    "title": "Machine Learning for Computational Economics",
    "section": "Implicit schemes",
    "text": "Implicit schemes\nWe will use the Barles‚ÄìSouganidis theorem to analyze the stability of implicit schemes.\n\nThe implicit version of the discretization of the Black‚ÄìScholes‚ÄìMerton PDE is \\[\nr\\,v_i^{n+1}\n= -\\frac{v_i^{n+1} - v_i^n}{\\Delta t}\n  + \\overline{r} \\left[\\mathbf{1}_{\\{\\overline{r} \\ge 0\\}} \\,\\frac{v_{i+1}^{n+1} - v_i^{n+1}}{\\Delta s}\n  + \\mathbf{1}_{\\{\\overline{r} &lt; 0\\}} \\,\\frac{v_{i}^{n+1} - v_{i-1}^{n+1}}{\\Delta s}\\right]\n  + \\frac{\\sigma_S^2}{2}\\,\\frac{v_{i+1}^{n+1} - 2v_i^{n+1} + v_{i-1}^{n+1}}{\\Delta s^2},\n\\]\n\n\n\n\n\nWe can rewrite the equation above as \\[\n  v_i^{n+1} = v_i^n + p_u^r\\,v_{i+1}^{n+1} - \\delta^r\\,v_i^{n+1} + p_d^r\\,v_{i-1}^{n+1},\n\\] where \\(\\delta^r \\equiv p_u^r+p_d^r+r\\,\\Delta t\\).\n\n\n\n\n\nIn matrix form, we have \\[\n   \\mathbf{A} \\mathbf{v}^{n+1} = \\mathbf{v}^n+ \\mathbf{b},\n\\] where \\(\\mathbf{A}\\) is a tridiagonal matrix of coefficients: \\[\n  \\mathbf{A} = \\begin{pmatrix}\n    1+\\delta^r - p_d^r &-p_u^r & 0 & \\ldots & 0 \\\\\n    -p_d^r & 1+\\delta^r & -p_u^r & \\ldots & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\ldots & -p_d^r & 1+\\delta^r & -p_u^r \\\\\n    0 & \\ldots & 0 & -p_d^r & 1+\\delta^r -p_u^r\n  \\end{pmatrix}.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#checking-the-barles-souganidis-conditions",
    "href": "Module03/Module03_Slides.html#checking-the-barles-souganidis-conditions",
    "title": "Machine Learning for Computational Economics",
    "section": "Checking the Barles-Souganidis conditions",
    "text": "Checking the Barles-Souganidis conditions\nWe need to check that the scheme is monotone, stable, and consistent.\n\nThe discretization ensures consistency and \\(r&gt;0\\) ensures stability.\nThe main condition we need to check is monotonicity.\nMonotonicity requires that, if \\(\\mathbf{v}^n\\) and \\(\\mathbf{u}^{n}\\) satisfy the recursion, and \\(\\mathbf{v}^n \\ge \\mathbf{u}^{n}\\), then \\(\\mathbf{v}^{n+1} \\ge \\mathbf{u}^{n+1}\\).\n\n\n\n\n\nThis is equivalent to saying that \\(\\mathbf{A}\\) is a M-matrix.üåê\n\nSufficient condition: off-diagonal elements are non-positive and the diagonal is strictly dominant: \\(\\mathbf{A}_{ii} &gt; \\sum_{j \\neq i} |\\mathbf{A}_{ij}|\\).\nUpwinding ensures that the off-diagonal elements, \\(p_u^r\\) and \\(p_d^r\\), are non-positive.\n\n\n\n\n\n\nStrict diagonal dominance requires: \\[\n  1+\\delta^r &gt; |p_d^r| + |p_u^r| \\iff 1+r \\Delta t &gt; 0,\n\\] which holds for any \\(\\Delta t &gt; 0\\) provided \\(r &gt; 0\\).\n\n\n\n\n\n\nImportant\n\n\nThe condition above plays the same role as the CFL condition in ensuring monotonicity, but unlike in the explicit scheme, it imposes no restriction on \\(\\Delta t\\). This property is often referred to as unconditional stability.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#julia-implementation-1",
    "href": "Module03/Module03_Slides.html#julia-implementation-1",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia implementation",
    "text": "Julia implementation\nWe can implement the implicit scheme as follows:\nfunction fd_implicit(m::BlackScholesModel)\n    (; œÉS, r, K, T, Ns, Nt, sgrid) = m # unpack model parameters\n    Œîs, Œît = sgrid[2] - sgrid[1], T / (Nt - 1) # spatial/time steps\n    rÃÖ = r - 0.5 * œÉS^2 # risk-adjusted drift\n    # Coefficients for the tridiagonal matrix\n    forward = rÃÖ &gt; 0 ? 1 : 0\n    pu = rÃÖ * Œît / Œîs * forward + œÉS^2 * Œît / (2 * Œîs^2)\n    ps = 1 - rÃÖ * Œît / Œîs * (2*forward-1) - œÉS^2 * Œît / (Œîs^2)\n    pd = -rÃÖ * Œît / Œîs * (1-forward) + œÉS^2 * Œît / (2 * Œîs^2)\n    e  = zeros(Ns)\n    e[1], e[end] = pd, pu # set boundary conditions\n    P = Tridiagonal(pd*ones(Ns-1), ps*ones(Ns)+e, pu*ones(Ns-1))\n    A = (2+r * Œît) * I - P \n    # Boundary conditions\n    b = zeros(Ns)\n    b[end] = pu * exp(sgrid[end]) * Œîs\n    # Initial condition\n    v = @. max(0.0, exp(sgrid) - K) # terminal condition\n    for n in 2:Nt\n        v = A \\ (v + b) # update rule\n    end\n    return (; P, b, v)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-implicit-scheme-in-action",
    "href": "Module03/Module03_Slides.html#the-implicit-scheme-in-action",
    "title": "Machine Learning for Computational Economics",
    "section": "The Implicit Scheme in Action",
    "text": "The Implicit Scheme in Action\nWe can plot the solution of the Black‚ÄìScholes‚ÄìMerton PDE using the implicit scheme.\n\nThe left panel compares the numerical solution with the exact Black‚ÄìScholes‚ÄìMerton formula.\nThe right panel shows the error heatmap.\nIncreasing the number of grid points in the spatial dimension reduces the error.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#income-fluctuations-problem",
    "href": "Module03/Module03_Slides.html#income-fluctuations-problem",
    "title": "Machine Learning for Computational Economics",
    "section": "Income Fluctuations Problem",
    "text": "Income Fluctuations Problem\nWe now solve the income fluctuations problem using a finite-difference scheme.\n\nRelative to the option pricing problem, we must handle (i) the optimal consumption choice and (ii) the borrowing constraint.\nFollowing Achdou et al. (2022), we consider the case without portfolio choice and a two-state income process.\n\n\n\n\n\nHJB equation: \\[\n  \\rho\\,V_{j,t}(W)\n  \\;=\\;\n  u \\big(c_{j,t}(W)\\big)\n  \\;-\\; \\frac{\\partial V_{j,t}}{\\partial t}(W)\n  \\;+\\; \\frac{\\partial V_{j,t}}{\\partial W}(W)\\,\\big[rW + Y_j - c_{j,t}(W)\\big]\n  \\;+\\; \\lambda_j \\big[V_{-j,t}(W)-V_{j,t}(W)\\big],\n\\] where \\(c_{j,t}(W) = u'^{-1}\\!\\big(V_{j,t}'(W)\\big)\\) and \\(\\lambda_j\\in\\{\\lambda_1,\\lambda_2\\}\\) is the outgoing intensity from state \\(j\\).\n\n\n\n\n\nUpwinding. Define forward and backward differences for the wealth derivative: \\[\n  v_{i,j,F}^{\\,n} \\;=\\; \\frac{v_{i+1,j}^{\\,n}-v_{i,j}^{\\,n}}{\\Delta W},\n  \\qquad\n  v_{i,j,B}^{\\,n} \\;=\\; \\frac{v_{i,j}^{\\,n}-v_{i-1,j}^{\\,n}}{\\Delta W}.\n\\] The corresponding wealth drifts are \\[\n  \\mu_{i,j,F}^{\\,n} \\;=\\; r W_i + Y_j - u'^{-1}\\!\\big(v_{i,j,F}^{\\,n}\\big),\n  \\qquad\n  \\mu_{i,j,B}^{\\,n} \\;=\\; r W_i + Y_j - u'^{-1}\\!\\big(v_{i,j,B}^{\\,n}\\big).\n\\]\n\n\n\n\n\nThe upwinded derivative used in the drift: \\[\n  v_{i,j,W}^{\\,n}\n  \\;=\\;\n  v_{i,j,F}^{\\,n}\\,\\mathbf{1}_{\\{\\mu_{i,j,F}^{\\,n}&gt;0\\}}\n  \\;+\\;\n  v_{i,j,B}^{\\,n}\\,\\mathbf{1}_{\\{\\mu_{i,j,B}^{\\,n}&lt;0\\}}\n  \\;+\\;\n  \\overline v_{i,j}\\,\\mathbf{1}_{\\{\\mu_{i,j,F}^{\\,n}\\le 0 \\le \\mu_{i,j,B}^{\\,n}\\}},\n\\] where \\(\\overline v_{i,j} \\equiv u'(rW_i+Y_j)\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#semi-implicit-upwind-scheme",
    "href": "Module03/Module03_Slides.html#semi-implicit-upwind-scheme",
    "title": "Machine Learning for Computational Economics",
    "section": "Semi-implicit upwind scheme",
    "text": "Semi-implicit upwind scheme\nLet \\([x]^+=\\max\\{x,0\\}\\) and \\([x]^-=\\min\\{x,0\\}\\). The semi-implicit scheme is \\[\n  \\rho\\,v_{i,j}^{\\,n+1}\n  \\;=\\;\n  u_{i,j}^{\\,n}\n  \\;-\\; \\frac{v_{i,j}^{\\,n+1}-v_{i,j}^{\\,n}}{\\Delta t}\n  \\;+\\; \\frac{v_{i+1,j}^{\\,n+1}-v_{i,j}^{\\,n+1}}{\\Delta W}\\,[\\mu_{i,j,F}^{\\,n}]^{+}\n  \\;+\\; \\frac{v_{i,j}^{\\,n+1}-v_{i-1,j}^{\\,n+1}}{\\Delta W}\\,[\\mu_{i,j,B}^{\\,n}]^{-}\n  \\;+\\; \\lambda_j\\!\\left(v_{i,-j}^{\\,n+1}-v_{i,j}^{\\,n+1}\\right).\n\\] Rearranging, yields the linear system \\[\n  -\\ell_{i,j}^{\\,n}\\,v_{i-1,j}^{\\,n+1}\n  \\;+\\;\n  s_{i,j}^{\\,n}\\,v_{i,j}^{\\,n+1}\n  \\;-\\;\n  r_{i,j}^{\\,n}\\,v_{i+1,j}^{\\,n+1}\n  \\;-\\;\n  \\lambda_j\\,v_{i,-j}^{\\,n+1}\n  \\;=\\;\n  \\frac{1}{\\Delta t}\\,v_{i,j}^{\\,n} \\;+\\; u_{i,j}^{\\,n},\n  \\label{eq:semi_implicit_linear}\n\\] with nonnegative coefficients \\[\n\\begin{align}\n  \\ell_{i,j}^{\\,n} &\\equiv -\\frac{[\\mu_{i,j,B}^{\\,n}]^{-}}{\\Delta W} \\;\\ge 0,\\\\\n  r_{i,j}^{\\,n}    &\\equiv \\;\\frac{[\\mu_{i,j,F}^{\\,n}]^{+}}{\\Delta W} \\;\\ge 0,\\\\\n  s_{i,j}^{\\,n}    &\\equiv \\frac{1}{\\Delta t} \\;+\\; \\ell_{i,j}^{\\,n} \\;+\\; r_{i,j}^{\\,n} \\;+\\; \\rho+\\lambda_j.\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nBoundary conditions\n\n\nTo impose the no-borrowing condition at the lower boundary \\(W=\\underline W\\), and reflection at the upper boundary \\(W=\\overline W\\), set \\[\n  v_{1,j,B}^{\\,n} \\;=\\; u'(rW_1+Y_j), \\qquad \\qquad v_{N,j,F}^{\\,n} \\;=\\; u'(rW_N+Y_j).\n\\] Hence, by construction, \\(\\ell_{1,j}^{\\,n}=r_{N,j}^{\\,n}=0\\), so ghost nodes are never used.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#block-matrix-form",
    "href": "Module03/Module03_Slides.html#block-matrix-form",
    "title": "Machine Learning for Computational Economics",
    "section": "Block matrix form",
    "text": "Block matrix form\nStack the two income states as \\[\n  \\mathbf{v}_j^{\\,n} = \\big(v_{1,j}^{\\,n},\\dots,v_{N,j}^{\\,n}\\big)^\\top,\\quad\n  \\mathbf{v}^{\\,n} = \\big((\\mathbf{v}_1^{\\,n})^\\top,(\\mathbf{v}_2^{\\,n})^\\top\\big)^\\top,\\quad\n  \\mathbf{u}^{\\,n} = \\big(u(c_{1,1}^{\\,n}),\\dots,u(c_{N,2}^{\\,n})\\big)^\\top.\n\\] Then \\[\n  \\mathbf{A}^{\\,n}\\,\\mathbf{v}^{\\,n+1} \\;=\\; \\frac{1}{\\Delta t}\\,\\mathbf{v}^{\\,n} + \\mathbf{u}^{\\,n},\n\\] with the \\(2N\\times 2N\\) block matrix \\[\n  \\mathbf{A}^{\\,n} \\;=\\;\n  \\begin{pmatrix}\n    \\mathbf{A}_1^{\\,n}  & -\\,\\lambda_1\\, \\mathbf{I}_N \\\\\n    -\\,\\lambda_2\\, \\mathbf{I}_N    & \\mathbf{A}_2^{\\,n}\n  \\end{pmatrix}, \\qquad \\qquad\n  \\mathbf{A}_j^{\\,n} = \\begin{pmatrix}\n    s_{1,j}^{\\,n} & -r_{1,j}^{\\,n} & 0 & \\ldots & 0 \\\\\n    -\\ell_{2,j}^{\\,n} & s_{2,j}^{\\,n} & -r_{2,j}^{\\,n} & \\ldots & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\ldots & -\\ell_{N-1,j}^{\\,n} & s_{N-1,j}^{\\,n} & -r_{N-1,j}^{\\,n} \\\\\n    0 & \\ldots & 0 & -\\ell_{N,j}^{\\,n} & s_{N,j}^{\\,n}\n  \\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nUnconditional stability\n\n\nOff-diagonals of \\(\\mathbf{A}^{\\,n}\\) are nonpositive and \\((\\text{diag}) - \\sum_{\\text{offdiag}} |\\,\\cdot\\,|\n  = \\big[\\tfrac{1}{\\Delta t} + \\rho+\\lambda_j + \\ell + r\\big] - (\\ell + r + \\lambda_j)\n  = \\tfrac{1}{\\Delta t} + \\rho \\,&gt;\\, 0\\), for each row. Thus \\(\\mathbf{A}^{\\,n}\\) is an M-matrix, the scheme is monotone and convergent, and it is unconditionally stable (no CFL restriction on \\(\\Delta t\\)).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#finite-differences-as-policy-function-iteration",
    "href": "Module03/Module03_Slides.html#finite-differences-as-policy-function-iteration",
    "title": "Machine Learning for Computational Economics",
    "section": "Finite Differences as Policy Function Iteration",
    "text": "Finite Differences as Policy Function Iteration\nWe have seen that the finite-difference method can be interpreted as an application of the MCA method: \\[\n  \\mathbf{v}\n  = \\max_{\\mathbf{c}}\n    \\Big\\{ u(\\mathbf{c})\\,\\Delta t\n    + (1-\\rho \\Delta t)\\,\\mathbf{P}(\\mathbf{c})\\,\\mathbf{v} \\Big\\}\n  \\;\\equiv\\; \\mathbf{T}\\mathbf{v},\n\\]\n\nOne can solve this problem using any method for discrete-time dynamic programming.\n\nThe (semi-implicit) finite-difference scheme of Achdou et al. (2022) is closely related to policy function iteration (PFI).\nThis was pointed out by Phelan and Eslami (2022).\n\n\n\n\nAlgorithm: Policy Function Iteration (PFI)\nInput: Initial policy \\(\\mathbf{c}^{(0)}\\), tolerance tol\nOutput: Value \\(\\mathbf{v}\\), policy \\(\\mathbf{c}\\)\nInitialize: \\(n \\gets 0\\)\nRepeat until \\(\\|\\mathbf{c}^{(n+1)}-\\mathbf{c}^{(n)}\\| &lt; \\text{tol}\\):\n\nPolicy evaluation:\nSolve \\([I-(1-\\rho\\Delta t)P(\\mathbf{c}^{(n)})]\\mathbf{v}^{(n+1)} = \\Delta t\\,u(\\mathbf{c}^{(n)})\\).\nPolicy improvement:\n\\(\\mathbf{c}^{(n+1)} \\gets \\arg\\max_{\\mathbf{c}}\\{\\Delta t\\,u(\\mathbf{c})+(1-\\rho\\Delta t)P(\\mathbf{c})\\mathbf{v}^{(n)}\\}\\).\n\\(n \\gets n+1\\).\n\nReturn: \\(\\mathbf{v}^{(n)},\\,\\mathbf{c}^{(n)}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#model-struct",
    "href": "Module03/Module03_Slides.html#model-struct",
    "title": "Machine Learning for Computational Economics",
    "section": "Model struct",
    "text": "Model struct\nWe start by defining the model parameters and the grid.\n@kwdef struct IncomeFluctuationsModel\n    œÅ::Float64 = 0.05\n    r::Float64 = 0.03\n    Œ≥::Float64 = 2.0\n    Y::Vector{Float64} = [0.1, 0.2]\n    Œª::Vector{Float64} = [0.02, 0.03]\n    Wmin::Float64 = -0.02\n    Wmax::Float64 = 2.0\n    N::Int64 = 500\n    Wgrid::LinRange{Float64} = range(Wmin, Wmax, length = N)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#implicit-scheme-implementation",
    "href": "Module03/Module03_Slides.html#implicit-scheme-implementation",
    "title": "Machine Learning for Computational Economics",
    "section": "Implicit scheme implementation",
    "text": "Implicit scheme implementation\nfunction fd_implicit(m::IncomeFluctuationsModel; Œît::Float64 = Inf, \n    tol::Float64 = 1e-6, max_iter::Int64 = 100, print_residual::Bool = true)\n    (; œÅ, r, Œ≥, Y, Œª, Wgrid, Wmin, Wmax, N) = m # unpack parameters\n    ŒîW = Wgrid[2] - Wgrid[1]\n    # Initial guess\n    v = [1/œÅ * (y + r * w)^(1-Œ≥) / (1-Œ≥) for w in Wgrid, y in Y] \n    c, vW, residual = similar(v), similar(v), 0.0 # pre-allocation\n    for i = 1:max_iter\n        # Compute derivatives\n        Dv = (v[2:end,:] - v[1:end-1,:]) / ŒîW\n        vB = [(@. (r * Wmin + Y)^(-Œ≥))'; Dv] # backward difference\n        vF = [Dv; (@. (r * Wmax + Y)^(-Œ≥))'] # forward difference\n        vÃÖ  = (r * Wgrid .+ Y').^(-Œ≥) # zero-savings case\n        ŒºB = r * Wgrid .+ Y' - vB.^(-1/Œ≥) # backward drift\n        ŒºF = r * Wgrid .+ Y' - vF.^(-1/Œ≥) # forward drift\n        vW = ifelse.(ŒºF .&gt; 0.0, vF, ifelse.(ŒºB .&lt; 0.0, vB, vÃÖ))\n        # Assemble matrix\n        c  = vW.^(-1/Œ≥)          # consumption\n        u  = c.^(1-Œ≥) / (1-Œ≥)    # utility\n        L = -min.(ŒºB, 0) / ŒîW    # subdiagonal\n        R = max.(ŒºF, 0) / ŒîW     # superdiagonal\n        S = @. 1/Œît + L + R + œÅ + Œª' # diagonal\n        Aj = [Tridiagonal(-L[2:end,j], S[:,j], -R[1:end-1,j]) \n                for j in eachindex(Y)] # tridiagonal matrices\n        A = [sparse(Aj[1]) -Œª[1] * I\n            -Œª[2] * I sparse(Aj[2])] # block matrix\n        # Update\n        vp = A \\ (u + v/Œît)[:]\n        residual = sqrt(mean((vp - v[:]).^2)) \n        v = reshape(vp, N, length(Y)) # reshape vector to matrix\n        if print_residual println(\"Iteration $i, residual = $residual\") end\n        if residual &lt; tol\n            break\n        end\n    end\n    s = r * Wgrid .+ Y' - c   # savings\n    return (; v, vW, c, s, residual) # return solution\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-solution-to-the-income-fluctuations-problem",
    "href": "Module03/Module03_Slides.html#the-solution-to-the-income-fluctuations-problem",
    "title": "Machine Learning for Computational Economics",
    "section": "The Solution to the Income Fluctuations Problem",
    "text": "The Solution to the Income Fluctuations Problem",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#finite-differences-as-local-polynomial-approximations",
    "href": "Module03/Module03_Slides.html#finite-differences-as-local-polynomial-approximations",
    "title": "Machine Learning for Computational Economics",
    "section": "Finite differences as local polynomial approximations",
    "text": "Finite differences as local polynomial approximations\nWe have used finite differences to generate a local approximation of derivatives, based on function values at neighboring points.\n\nWe will see how to use spectral methods, which instead provide a global approximation of derivatives.\nWe are interested in computing the derivative of a function \\(v(x)\\) that we observe at discrete points \\(\\{x_i\\}_{i=1}^N\\).\n\n\n\n\n\n\n\nA natural approximation is the forward difference: \\[\n  v'(x_i) = \\frac{v_{i+1}-v_i}{\\Delta x}+ O(\\Delta x).\n\\]\n\nThis is equivalent to replacing \\(v(x)\\) locally by a linear interpolant:\n\\[\n\\tilde v(x) = v_i + Dv_i (x-x_i), \\qquad Dv_i = \\frac{v_{i+1}-v_i}{\\Delta x}.\n\\]\n\nDifferentiating the interpolant yields \\(\\tilde v'(x_i)=Dv_i\\), i.e., the forward difference.\n\n\n\n\n\n\n\nThe central difference formula yields \\[\n  v'(x_i) = \\frac{v_{i+1}-v_{i-1}}{2\\Delta x} + O(\\Delta x^2),\n\\]\n\nThis is equivalent to replacing \\(v(x)\\) by a quadratic interpolant:\n\\[\n\\tilde v(x) = \\ell_{i-1}(x)v_{i-1} + \\ell_i(x)v_i + \\ell_{i+1}(x)v_{i+1},\n\\]\n\nwhere the basis polynomials are \\(\\ell_{i-1}(x) = \\frac{(x-x_i)(x-x_{i+1})}{2\\Delta x^2}\\), \\(\\ell_i(x) = -\\frac{(x-x_i+\\Delta x)(x-x_i-\\Delta x)}{\\Delta x^2}\\), and \\(\\ell_{i+1}(x) = \\frac{(x-x_i+\\Delta x)(x-x_i)}{2\\Delta x^2}\\).\n\n\nDifferentiating and evaluating at \\(x_i\\) yields the central difference formulas: \\[\n  \\tilde v'(x_i) = \\frac{v_{i+1}-v_{i-1}}{2\\Delta x},\n  \\qquad\n  \\tilde v''(x_i) = \\frac{v_{i+1}-2v_i+v_{i-1}}{\\Delta x^2},\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#higher-order-finite-differences",
    "href": "Module03/Module03_Slides.html#higher-order-finite-differences",
    "title": "Machine Learning for Computational Economics",
    "section": "Higher-order finite differences",
    "text": "Higher-order finite differences\nWe can obtain higher-order finite-difference formulas by fitting higher-degree local polynomials and differentiating them.\n\nFor example, a quartic interpolant yields a fourth-order approximation to \\(v'(x_i)\\).\nWe can compute such higher-order approximations in Julia using the Polynomials.jl package.\n\n\n\n\n\"\"\"\n    finite_difference(f, x0, accuracy; scheme=:forward, Œîx=0.01)\n\nApproximate f'(x0) by differentiating a local interpolating \npolynomial built on a stencil that achieves the requested accuracy.\n\"\"\"\nfunction finite_difference(f::Function, x0::Float64, accuracy::Int;\n                    scheme::Symbol = :forward, Œîx::Float64 = 0.01)\n    @assert accuracy &gt; 0 \"accuracy order must be positive\"\n    steps = zeros(Int, accuracy+1)\n    if iseven(accuracy)\n        m = accuracy √∑ 2\n        steps = collect(-m:m)\n    else\n        q = accuracy\n        steps = scheme === :forward ? collect(0:q) : collect(-q:0)\n    end\n    x_points = x0 .+ Œîx .* steps\n    p    = Polynomials.fit(x_points, f.(x_points), length(x_points) - 1)\n    return Polynomials.derivative(p)(x0)\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#from-local-to-global-polynomials",
    "href": "Module03/Module03_Slides.html#from-local-to-global-polynomials",
    "title": "Machine Learning for Computational Economics",
    "section": "From local to global polynomials",
    "text": "From local to global polynomials\nSuppose we approximate a smooth function \\(v(x)\\) on \\([-1,1]\\) by a single polynomial of degree \\(N\\): \\[\\begin{equation}\n  v(x) \\approx \\tilde v_N(x) = \\sum_{k=0}^{N} a_k x^k,\n\\end{equation}\\] where the coefficients \\(a_k\\) are chosen so that \\(\\tilde v_N(x_i) = v(x_i)\\) at a set of interpolation nodes \\(\\{x_i\\}_{i=0}^N\\).\n\n\n\n\n\n\n\n\nChange of domain\n\n\nWhen approximating a function \\(v(x)\\) on a bounded domain \\([a,b]\\), we can always map it to the interval \\([-1,1]\\) via \\[\n    v(x) \\equiv V\\!\\left(\\tfrac{(b-a)x + (a+b)}{2}\\right),\n    \\qquad x \\in [-1,1].\n  \\] For a discussion of approximations on unbounded domains, see Chapter 17 of Boyd (2001).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinite vs.¬†spectral accuracy\n\n\nHigher-order finite differences improve accuracy algebraically, with error \\(O(\\Delta x^p)\\) for a \\(p\\)-th order scheme.\n\nIn contrast, spectral methods achieve exponential (or spectral) convergence for smooth functions, as we will see next.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-runge-phenomenon",
    "href": "Module03/Module03_Slides.html#the-runge-phenomenon",
    "title": "Machine Learning for Computational Economics",
    "section": "The Runge phenomenon",
    "text": "The Runge phenomenon\nThis global approach uses information from the entire domain to approximate derivatives or other operators.\n\nHowever, if the interpolation nodes \\(x_i\\) are equally spaced, the interpolant may oscillate violently near the boundaries as \\(N\\) increases\nA pathology known as the Runge phenomenon: consider the function \\(v(x) = \\frac{1}{1 + 25x^2}\\), for \\(x \\in [-1,1]\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-interpolation-error-theorem",
    "href": "Module03/Module03_Slides.html#the-interpolation-error-theorem",
    "title": "Machine Learning for Computational Economics",
    "section": "The interpolation error theorem",
    "text": "The interpolation error theorem\nThe interpolation error theorem üåê provides a useful decomposition of the interpolation error.\n\nLet \\(v(x)\\) have at least \\(N+1\\) derivatives on \\([-1,1]\\) and let \\(\\tilde v_N(x)\\) be the degree-\\(N\\) interpolant of \\(v(x)\\) at nodes \\(\\{x_i\\}_{i=0}^N\\).\nThen, the interpolation error is given by \\[\nv(x) - \\tilde v_N(x)\n= \\color{#0072b2}{\\frac{v^{(N+1)}(\\xi)}{(N+1)!}}\\,\\color{#009e73}{\\mathcal{P}_{N+1}(x)},\n\\qquad\n\\mathcal{P}_{N+1}(x) = \\prod_{i=0}^{N}(x - x_i),\n\\] for some \\(\\xi \\in [-1,1]\\).\n\n\n\n\n\nThe error therefore depends on two components:\n\nThe \\((N+1)\\)-st derivative \\(v^{(N+1)}(\\xi)\\), which depends only on the smoothness of \\(v\\); and\nThe node polynomial \\(\\mathcal{P}_{N+1}(x)\\), which depends solely on the choice of interpolation nodes.\n\n\n\nWe cannot control the smoothness of \\(v\\), but we can control the second term.\n\n\n\n\n\n\n\nWorst-case interpolation error\n\n\nThe worst-case (uniform) interpolation error is bounded by \\[\n  |v(x) - \\tilde v_N(x)|\n  \\le\n  \\frac{1}{(N+1)!}\\,\n  \\max_{x \\in [-1,1]} |v^{(N+1)}(x)|\\,\n  \\|\\mathcal{P}_{N+1} \\|_\\infty,\n  \\qquad\n  \\|\\mathcal{P}_{N+1} \\|_\\infty \\equiv \\max_{x \\in [-1,1]} |\\mathcal{P}_{N+1}(x)|.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#chebyshev-polynomials-and-the-minimax-property",
    "href": "Module03/Module03_Slides.html#chebyshev-polynomials-and-the-minimax-property",
    "title": "Machine Learning for Computational Economics",
    "section": "Chebyshev polynomials and the minimax property",
    "text": "Chebyshev polynomials and the minimax property\nOur goal is clear: choose interpolation nodes \\(\\{x_i\\}\\) that minimize \\(\\|\\mathcal{P}_{N+1}\\|_\\infty\\).\n\nThis leads directly to the Chebyshev minimal amplitude theorem.\n\n\n\n\n\n\n\n\n\nChebyshev minimal amplitude theorem\n\n\nDefine the Chebyshev nodesüåê as \\[\n  \\hat{x}_i = \\cos\\!\\left(\\frac{(2i+1)\\pi}{2(N+1)}\\right),\n  \\qquad i = 0,\\ldots,N.\n\\] The polynomial \\(\\mathcal{P}_{N+1}(x) = \\prod_{i=0}^{N} (x - x_i)\\) attains its smallest maximum amplitude on \\([-1,1]\\) when the \\(x_i\\) are the Chebyshev nodes: \\[\n  \\|\\mathcal{P}_{N+1}\\|_\\infty\n  = \\max_{x \\in [-1,1]} |\\mathcal{P}_{N+1}(x)|\n  \\geq\n  \\max_{x \\in [-1,1]} \\left|\\prod_{i=0}^{N} (x - \\hat{x}_i)\\right|.\n\\]\n\n\n\n\n\n\n\n\nChebyshev nodes are the roots of the Chebyshev polynomial üåê, \\[\n  T_{N+1}(x) = \\cos\\!\\big((N+1)\\arccos x\\big),\n\\] which oscillates between \\(-1\\) and \\(1\\) exactly \\(N{+}2\\) times on \\([-1,1]\\).\n\nChebyshev polynomials enjoy an orthogonality relation: \\[\n  \\int_{-1}^1 T_m(x)\\,T_n(x)\\,w(x)\\,dx =\n0, \\qquad m \\ne n, \\qquad w(x)=1/\\sqrt{1-x^2}.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#from-interpolation-to-differentiation",
    "href": "Module03/Module03_Slides.html#from-interpolation-to-differentiation",
    "title": "Machine Learning for Computational Economics",
    "section": "From interpolation to differentiation",
    "text": "From interpolation to differentiation\nLet \\(\\tilde v_N(x) = \\sum_{k=0}^N a_k T_k(x)\\) be the degree-\\(N\\) Chebyshev interpolant of \\(v(x)\\):\n\nGiven the interpolant, it is straightforward to compute its derivatives.\nOne can use recursion for Chebyshev polynomials of the second kind or the formulas in the lecture notes.\n\n\n\n\n\n\nConsider the smooth function \\(v(x) = e^{x^2} + 2\\sin(x)\\):\n\nThe figure shows the approximation error for the derivative\nWe compare finite differences and Chebyshev differentiation\n\n\n\n\nChebyshev differentiation reaches machine precision accuracy\n\nWith a modest number of nodes (around \\(N=20\\))\nEven with \\(N = 10\\), it is orders of magnitude more accurate than FD\n\n\n\n\n\n\n\n\n\n\n\nDerivatives of Chebyshev polynomials\n\n\nBecause the Chebyshev interpolation error decays exponentially for smooth functions, the same holds for the derivative approximation: \\[\n  \\|\\tilde v_N'(x) - v'(x)\\|_\\infty = O(e^{-\\alpha N}), \\qquad \\text{for some } \\alpha &gt;0.\n  \\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-two-trees-model",
    "href": "Module03/Module03_Slides.html#the-two-trees-model",
    "title": "Machine Learning for Computational Economics",
    "section": "The Two-trees Model",
    "text": "The Two-trees Model\nWe can use Chebyshev polynomials to solve PDEs like the HJB equation\n\nTo illustrate the method, we consider the two-trees model of Cochrane, Longstaff, and Santa-Clara (2008)\nThe model is a two-tree economy with dividends following a geometric Brownian motion: \\[\n\\frac{d D_{i,t}}{D_{i,t}} = \\mu dt + \\sigma dB_{i,t}, \\qquad i = 1,2.\n\\]\n\n\nAggregate consumption equals the sum of dividends from the two trees: \\(C_t = D_{1,t} + D_{2,t}\\).\n\nAssuming the representative household has log utility, the price of the first tree is given by: \\[\nP_{t} = \\mathbb{E}_t \\left[ \\int_{0}^{\\infty} e^{-\\rho s} \\frac{C_t}{C_{t+s}} D_{1,t+s} ds \\right] \\Rightarrow v_{t} = \\mathbb{E}_t \\left[ \\int_{0}^{\\infty} e^{-\\rho s} s_{t+s} ds \\right], \\qquad v_t \\equiv \\frac{P_t}{C_t}, \\qquad s_t \\equiv \\frac{D_{1,t}}{C_{t}}.\n\\]\n\n\n\n\n\n\n\n\n\n\nThe HJB equation\n\n\nThe price-consumption ratio \\(v_t\\) satisfies the HJB equation: \\[\n  \\rho v = s - v_s \\,2\\sigma^2 s(1-s)\\!\\left(s-\\tfrac12\\right)\n           + \\frac{1}{2} v_{ss}\\,\\big(2\\sigma^2 s^2(1-s)^2\\big),\n\\] with boundary conditions \\(v(0) = 0\\) and \\(v(1) = 1/\\rho\\), using \\(d s_t = - 2 \\sigma^2 s_t(1-s_t)(s_t -1/2) dt + \\sigma s_t(1-s_t)(dB_{1,t}- dB_{2,t})\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#solving-the-hjb-equation-with-chebyshev-collocation",
    "href": "Module03/Module03_Slides.html#solving-the-hjb-equation-with-chebyshev-collocation",
    "title": "Machine Learning for Computational Economics",
    "section": "Solving the HJB equation with Chebyshev collocation",
    "text": "Solving the HJB equation with Chebyshev collocation\nTo solve the HJB equation with Chebyshev collocation, we start with a series expansion of \\(v(s)\\): \\[\n  v(s) = \\sum_{n=0}^{\\infty} a_n \\tilde{T}_n(s) \\approx \\sum_{n=0}^{N} a_n \\tilde{T}_n(s), \\qquad \\qquad \\tilde{T}_n(s) = T_n(2s-1).\n\\]\n\n\n\n\n\n\n\nChange of domain\n\n\nThe mapping \\(s \\mapsto x=2s-1\\) rescales the domain from \\([0,1]\\) to \\([-1,1]\\), allowing the use of standard Chebyshev polynomials. Derivatives with respect to \\(s\\) follow from the chain rule, yielding \\(\\tilde T_n'(s)=2T_n'(2s-1)\\) and \\(\\tilde T_n''(s)=4T_n''(2s-1)\\).\n\n\n\n\n\nPlugging the expansion into the HJB equation and evaluating at the grid points, we obtain the linear system: \\[\n  \\mathbf{L} \\mathbf{a} = \\mathbf{b},\n\\] where \\(\\mathbf{L}\\) is a \\((N+1) \\times (N+1)\\) matrix and \\(\\mathbf{b}\\) is a \\((N+1)\\) vector.\n\n\nInterior points. For \\(i =2,\\ldots,N\\) and \\(j = 1,\\ldots,N+1\\), we have: \\[\n  \\mathbf{L}_{i,j} \\;=\\;\n\\rho\\,\\tilde T_{j-1}(s_i)\n\\;+\\; 2\\sigma^2 s_i(1-s_i)\\!\\left(s_i-\\tfrac12\\right)\\,\\tilde T_{j-1}'(s_i)\n\\;-\\; \\sigma^2 s_i^2(1-s_i)^2\\,\\tilde T_{j-1}''(s_i), \\qquad \\qquad b_i = s_i.\n\\]\n\n\n\n\n\n\n\n\n\nBoundary conditions\n\n\nThe first and last rows accommodate the boundary conditions: \\[\n\\begin{align}\n  \\mathbf{L}_{1,j} &= \\tilde{T}_{j-1}(0), \\qquad  \n  \\mathbf{L}_{N+1,j} = \\tilde{T}_{j-1}(1), \\qquad \\qquad  b_1 = 0, \\qquad b_{N+1} = 1/\\rho.\n\\end{align}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#julia-implementation-2",
    "href": "Module03/Module03_Slides.html#julia-implementation-2",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia Implementation",
    "text": "Julia Implementation\nWe start by defining the model structure for the two-trees model:\n@kwdef struct TwoTrees\n    œÅ::Float64 = 0.04\n    œÉ::Float64 = sqrt(0.04)\n    Œº::Float64 = 0.02\n    N::Int = 7\nend\n\nTo construct the matrix \\(\\mathbf{L}\\), we need to evaluate the Chebyshev polynomials and their derivatives at the grid points:\n\nWe use the implementation of Chebyshev polynomials and their derivatives from the Polynomials.jl package.\n\nfunction chebyshev_derivatives(n::Int, z::Real; \n        zmin::Real = -1.0, zmax::Real = 1.0)\n    a, b = 2 / (zmax - zmin), -(zmin + zmax) / (zmax - zmin)\n    x = a * z + b # Map to [-1,1]\n    p = ChebyshevT([zeros(n);1.0]) # Degree n Chebyshev polynomial\n    d1p = derivative(p) # First derivative\n    d2p = derivative(d1p) # Second derivative\n    return p(x), d1p(x) * a, d2p(x) * a^2\nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-chebyshev-solver",
    "href": "Module03/Module03_Slides.html#the-chebyshev-solver",
    "title": "Machine Learning for Computational Economics",
    "section": "The Chebyshev solver",
    "text": "The Chebyshev solver\nThe function chebyshev_solver solves for the coefficients \\(a_n\\) of the Chebyshev expansion of the price-consumption ratio \\(v(s)\\):\n\nThe function returns a NamedTuple with the function \\(v(s)\\) and the grid \\(s_i\\).\nNotice that the function \\(v(s)\\) can be evaluated at any point in the interval \\([z_{\\min},z_{\\max}]\\), not just the grid points.\n\nfunction chebyshev_solver(m::TwoTrees)\n    (; œÅ, œÉ, N) = m\n    # Chebyshev grid and mapping to [0,1]\n    x = reverse(cos.(pi .* (0:N) ./ N))\n    s = (x .+ 1) ./ 2\n    # Assemble the linear operator\n    L, b = zeros(N+1, N+1), copy(s)\n    for i in 1:N+1, j in 1:N+1\n        TÃÉ, dT, d2T = chebyshev_derivatives(j-1, s[i]; zmin = 0)\n        if i == 1 || i == N+1\n            L[i,j] = TÃÉ # Boundary points\n        else\n            Œºs = -2 * œÉ^2 * s[i] * (1 - s[i]) * (s[i] - 1/2)\n            œÉs = sqrt(2) * œÉ * s[i] * (1 - s[i])\n            L[i,j] = œÅ * TÃÉ - dT * Œºs - d2T * œÉs^2 / 2\n        end\n    end\n    b[end] = 1/œÅ # Boundary condition at s = 1\n    a = L \\ b  # Solve for the coefficients\n    return (; v = z -&gt; ChebyshevT(a)(2 * z - 1), s = s) \nend",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#the-solution-of-the-two-trees-model",
    "href": "Module03/Module03_Slides.html#the-solution-of-the-two-trees-model",
    "title": "Machine Learning for Computational Economics",
    "section": "The Solution of the Two-trees Model",
    "text": "The Solution of the Two-trees Model",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#finite-difference-methods-in-high-dimensional-problems",
    "href": "Module03/Module03_Slides.html#finite-difference-methods-in-high-dimensional-problems",
    "title": "Machine Learning for Computational Economics",
    "section": "Finite Difference Methods in High-Dimensional Problems",
    "text": "Finite Difference Methods in High-Dimensional Problems\nWe have seen how to use finite-difference and Chebyshev collocation methods to continuous time models.\n\nThese techniques are very effective in one or two dimensions.\nHowever, they suffer from the same three curses of dimensionality as in the discrete-time setting.\n\n\nFinite-difference schemes in higher dimensions.\n\nBecause finite-difference schemes represent the state space on a grid, they inherit the first curse of dimensionality.\nThe number of grid points grows exponentially with the number of state variables\n\n\n\n\n\n\nA second difficulty is maintaining the monotonicity of the scheme.\n\nIt is straightforward to guarantee monotonicity in one dimension\nIn higher dimensions, however, cross-derivative terms create complications.\nConsider an HJB equation with two state variables \\(\\mathbf{s} = (s_1, s_2)\\):\n\n\\[\n  \\rho v = u(\\mathbf{s})\n  + v_{s_1}\\,\\mu_{s_1}(\\mathbf{s})\n  + v_{s_2}\\,\\mu_{s_2}(\\mathbf{s})\n  + \\tfrac{1}{2}v_{s_1s_1}\\,\\sigma_{s_1}^2(\\mathbf{s})\n  + \\tfrac{1}{2}v_{s_2s_2}\\,\\sigma_{s_2}^2(\\mathbf{s})\n  + v_{s_1s_2}\\,\\sigma_{s_1s_2}(\\mathbf{s}).\n\\]\n\n\nA central-difference approximation for the mixed derivative, \\[\n  v_{s_1s_2} \\approx\n  \\frac{v_{i+1,j+1} - v_{i-1,j-1} - v_{i+1,j-1} + v_{i-1,j+1}}\n       {4 \\Delta s_1 \\Delta s_2},\n\\] produces nonnegative off-diagonal elements in the discretization matrix, violating the M-matrix conditions required for stability.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#chebyshev-collocation-in-high-dimensional-problems",
    "href": "Module03/Module03_Slides.html#chebyshev-collocation-in-high-dimensional-problems",
    "title": "Machine Learning for Computational Economics",
    "section": "Chebyshev Collocation in High-Dimensional Problems",
    "text": "Chebyshev Collocation in High-Dimensional Problems\nChebyshev collocation methods share the same first curse.\n\nIn one dimension, they achieve exponential accuracy with relatively few nodes.\nIn \\(d\\) dimensions, however, the tensor product of one-dimensional bases yields \\((N+1)^d\\) nodes.\n\nSeveral strategies mitigate this explosion.\n\nJudd et al. (2014) propose Smolyak sparse grids and anisotropic tensor-product bases\nThis dramatically reduces the number of collocation points.\nHowever, these approaches face steep scaling once \\(d\\) exceeds five to ten dimensions.\n\n\n\n\n\n\n\n\n\n\n\n\nThe need for new methods\n\n\nBoth finite-difference and collocation methods ultimately confront the same exponential barriers as their discrete-time counterparts.\n\nIn the next modules, we explore modern machine-learning-based approaches that circumvent these limitations.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module03/Module03_Slides.html#references",
    "href": "Module03/Module03_Slides.html#references",
    "title": "Machine Learning for Computational Economics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nAchdou, Yves, Jiequn Han, Jean-Michel Lasry, Pierre-Louis Lions, and Benjamin Moll. 2022. ‚ÄúIncome and Wealth Distribution in Macroeconomics: A Continuous-Time Approach.‚Äù The Review of Economic Studies 89 (1): 45‚Äì86.\n\n\nBarles, Guy, and Panagiotis E. Souganidis. 1991. ‚ÄúConvergence of Approximation Schemes for Fully Nonlinear Second Order Equations.‚Äù Asymptotic Analysis 4: 271‚Äì83.\n\n\nBoyd, John P. 2001. Chebyshev and Fourier Spectral Methods. Mineola, NY: Dover Publications.\n\n\nCochrane, John H., Francis A. Longstaff, and Pedro Santa-Clara. 2008. ‚ÄúTwo Trees.‚Äù The Review of Financial Studies 21 (1): 347‚Äì85. https://doi.org/10.1093/rfs/hhm059.\n\n\nJudd, Kenneth L., Lilia Maliar, Serguei Maliar, and Rafael Valero. 2014. ‚ÄúSmolyak Method for Solving Dynamic Economic Models: Lagrange Interpolation, Anisotropic Grid and Adaptive Domain.‚Äù Journal of Economic Dynamics and Control 44: 92‚Äì123. https://doi.org/10.1016/j.jedc.2014.03.005.\n\n\nKushner, Harold J., and Paul G. Dupuis. 2001. Numerical Methods for Stochastic Control Problems in Continuous Time. 2nd ed. Applications of Mathematics 24. New York: Springer.\n\n\nPhelan, Thomas, and Keyvan Eslami. 2022. ‚ÄúApplications of Markov Chain Approximation Methods to Optimal Control Problems in Economics.‚Äù Journal of Economic Dynamics and Control 143: 104437. https://doi.org/10.1016/j.jedc.2022.104437.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 3 ‚Äì Continuous-Time Methods"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#introduction",
    "href": "Module05/Module05_Slides.html#introduction",
    "title": "Machine Learning for Computational Economics",
    "section": "Introduction",
    "text": "Introduction\nIn this module, we use machine-learning tools to solve high-dimensional problems in continuous time.\n\nWe introduce the Deep Policy Iteration (DPI) method, proposed by Duarte, Duarte, and Silva (2024).\nIt combines stochastic optimization, automatic differentiation, and neural-network function approximation.\n\n\n\n\nWe proceed in two steps.\n\nWe show how to overcome the three curses of dimensionality.\nWe describe a range of applications of the DPI method in asset pricing, corporate finance, and portfolio choice.\n\n\n\n\nThe module is organized as follows:\n\nThe Hyper-Dual Approach to It√¥‚Äôs lemma\nThe DPI Method\nApplications",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-dynamic-programming-problem",
    "href": "Module05/Module05_Slides.html#the-dynamic-programming-problem",
    "title": "Machine Learning for Computational Economics",
    "section": "The Dynamic Programming Problem",
    "text": "The Dynamic Programming Problem\nConsider a continuous-time optimal control problem in which an infinitely lived agent faces a Markovian decision process (MDP).\n\nThe system‚Äôs state is represented by a vector \\(\\mathbf{s}_t \\in \\mathbb{R}^n\\)\nThe control is a vector \\(\\mathbf{c}_t \\in \\Gamma(\\mathbf{s}_t) \\subset \\mathbb{R}^p\\).\n\n\n\n\n\nThe agent‚Äôs objective is to maximize the expected discounted value of the reward function \\(u(\\mathbf{c}_t)\\): \\[\nV(\\mathbf{s}) = \\max_{\\{\\mathbf{c}_t\\}_{t=0}^\\infty} \\mathbb{E}\\left[ \\left. \\int_{0}^\\infty e^{-\\rho t} u(\\mathbf{c}_t) dt  \\right| \\mathbf{s}_0 = \\mathbf{s} \\right],\n\\] subject to the stochastic law of motion \\[\nd\\mathbf{s}_t = \\mathbf{f}(\\mathbf{s}_t, \\mathbf{c}_t) dt + \\mathbf{g}(\\mathbf{s}_t, \\mathbf{c}_t) d\\mathbf{B}_t.\n\\] where \\(\\mathbf{B}_t\\) is an \\(m \\times 1\\) vector of Brownian motions, \\(\\mathbf{f}(\\mathbf{s}_t, \\mathbf{c}_t) \\in \\mathbb{R}^n\\) is the drift, and \\(\\mathbf{g}(\\mathbf{s}_t, \\mathbf{c}_t) \\in \\mathbb{R}^{n\\times m}\\) is the diffusion matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nThe HJB Equation.\n\n\nThe value function \\(V(\\mathbf{s})\\) satisfies the Hamilton‚ÄìJacobi‚ÄìBellman (HJB) equation: \\[\n0 = \\max_{\\mathbf{c}\\in\\Gamma(\\mathbf{s})} u(\\mathbf{c}) - \\rho V(\\mathbf{s}) + \\nabla_{\\mathbf{s}} V(\\mathbf{s}) \\cdot \\mathbf{f}(\\mathbf{s},\\mathbf{c}) + \\tfrac{1}{2} \\operatorname{Tr}(\\mathbf{g}(\\mathbf{s},\\mathbf{c}) \\mathbf{H}_{\\mathbf{s}} V(\\mathbf{s}) \\mathbf{g}(\\mathbf{s},\\mathbf{c})),\n\\] where \\(\\nabla_{\\mathbf{s}} V(\\mathbf{s})\\) and \\(\\mathbf{H}_{\\mathbf{s}} V(\\mathbf{s})\\) denote, respectively, the gradient and Hessian of the value function.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#dealing-with-the-three-curses",
    "href": "Module05/Module05_Slides.html#dealing-with-the-three-curses",
    "title": "Machine Learning for Computational Economics",
    "section": "Dealing with the Three Curses",
    "text": "Dealing with the Three Curses\nAs discussed in Module 1, dynamic programming methods face three intertwined computational obstacles:\n\n\n\n\n\n1st curse\n\nRepresenting \\(V(\\mathbf{s})\\)\n\n\n\n\n\n2nd curse\n\nSolving for \\(\\mathbf{c}\\) given \\(V(\\mathbf{s})\\)\n\n\n\n\n\n3rd curse\n\nComputing \\(\\mathbb{E}[V(\\mathbf{s}')]\\)\n\n\n\n\n\nIn this module, we present an algorithm that addresses each of these curses using modern machine-learning tools:\n\nNeural networks provide compact representations of value and policy functions, circumventing the curse of representation;\nStochastic optimization replace costly root-finding procedures, alleviating the curse of optimization;\nAutomatic differentiation enables efficient computation of drift terms needed for the HJB, mitigating the curse of expectation;",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#from-integration-to-differentiation",
    "href": "Module05/Module05_Slides.html#from-integration-to-differentiation",
    "title": "Machine Learning for Computational Economics",
    "section": "From integration to differentiation",
    "text": "From integration to differentiation\nIn discrete time, the Bellman equation involves the expectation of the continuation value: \\[\n\\mathbb{E}[V(\\mathbf{s}')] = \\int \\!\\!\\cdots\\!\\! \\int V(\\mathbf{s} + \\mathbf{f}(\\mathbf{s},\\mathbf{c}) \\Delta t + \\mathbf{g}(\\mathbf{s},\\mathbf{c}) \\sqrt{\\Delta t} \\mathbf{Z}) \\phi(\\mathbf{Z}) d\\mathbf{Z}_1 \\cdots d\\mathbf{Z}_m,\n\\] where \\(\\phi(\\mathbf{Z})\\) is the joint density of the shocks.\n\n\n\n\nIn continuous time, the expected change in the value function can be written as \\[\n\\mathbb{E}[d V(\\mathbf{s})] = \\nabla_{\\mathbf{s}} V(\\mathbf{s}) \\cdot \\mathbf{f}(\\mathbf{s},\\mathbf{c}) + \\tfrac{1}{2} \\operatorname{Tr}(\\mathbf{g}(\\mathbf{s},\\mathbf{c}) \\mathbf{H}_{\\mathbf{s}} V(\\mathbf{s}) \\mathbf{g}(\\mathbf{s},\\mathbf{c})),\n\\]\nHence, instead of computing high-dimensional integrals, we only need to compute derivatives of the value function.\n\n\n\n\n\n\n\n\n\n\n\n\nComputational challenge.\n\n\nOne could use finite differences to compute these derivatives\n\nBut storing the solution on a grid becomes infeasible in higher dimensions.\n\nSuppose \\(n=10\\) and we use a grid of 100 points for each state variable.\n\nThen, just to store the grid in memory, we would need \\(10^{17}\\) terabytes of RAM.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#comparing-the-computational-costs",
    "href": "Module05/Module05_Slides.html#comparing-the-computational-costs",
    "title": "Machine Learning for Computational Economics",
    "section": "Comparing the Computational Costs",
    "text": "Comparing the Computational Costs\nTo illustrate the computational challenge, we study a simple example.\n\n\n\n\n\nConsider the following function: \\[\nV(\\mathbf{s}) = \\sum_{i=1}^n s_i^2,\n\\] where \\(\\mathbf{s} = (s_1, \\ldots, s_n)\\) is a vector of state variables.\n\nA high-dimensional problem with \\(n=100\\) state variables\n\nWe evaluate the drift at the point \\(\\mathbf{s} = \\mathbf{1}_{n\\times1}\\).\nWe focus on the case of a single shock (\\(m=1\\)).\n\n\n\n\n\n\nWe consider initially the following methods to compute the drift \\(\\mathbb{E}[d V(\\mathbf{s})]\\):\n\nFinite differences\nNaive autodiff\n\n\n\n\n\n\n\n\nComparing the Computational Costs.\n\n\n\n\n\nMethod\nFLOPs\nMemory\nError\n\n\n\n\n1. Finite differences\n9,190,800\n112,442,048\n1.58%\n\n\n2. Naive autodiff\n2,100,501\n25,673,640\n0.00%\n\n\n\n\n\n\n\nEven the naive autodiff method is computationally costly and memory intensive.\n\nThe naive method computes the Hessian by nested calls to the Jacobian function",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#overcoming-the-curse-of-expectation",
    "href": "Module05/Module05_Slides.html#overcoming-the-curse-of-expectation",
    "title": "Machine Learning for Computational Economics",
    "section": "Overcoming the Curse of Expectation",
    "text": "Overcoming the Curse of Expectation\nOne of the key insights from Module 4 is that the Jacobian‚Äìvector product (JVP) can be efficiently computed.\n\nMuch less expensive than forming the full Jacobian.\n\n\n\n\n\nIn the absence of shocks, computing the drift of \\(V(\\mathbf{s})\\) amounts to evaluating a JVP:\n\\[\n\\mathbb{E}[d V(\\mathbf{s})] = \\nabla_{\\mathbf{s}} V(\\mathbf{s})^\\top \\mathbf{f}(\\mathbf{s},\\mathbf{c}) dt,\n\\] which can be efficiently computed using forward-mode AD.\n\n\n\nIn the presence of shocks, the drift also depends on quadratic forms involving the Hessian of \\(V(\\mathbf{s})\\).\n\nIn this case, a JVP is no longer sufficient.\n\n\n\nThe Hyper-dual approach to It√¥‚Äôs lemma extends the idea of dual numbers to compute the drift of \\(V(\\mathbf{s})\\).\n\nRegular dual numbers only store the function value and its derivative.\nHyper-dual numbers store the function value, its drift, and its diffusion matrix.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-hyper-dual-approach-to-it√¥s-lemma",
    "href": "Module05/Module05_Slides.html#the-hyper-dual-approach-to-it√¥s-lemma",
    "title": "Machine Learning for Computational Economics",
    "section": "The Hyper-Dual Approach to It√¥‚Äôs lemma",
    "text": "The Hyper-Dual Approach to It√¥‚Äôs lemma\nThe next result formalizes the hyper-dual approach to It√¥‚Äôs lemma.\n\nIt reduces the problem of computing the drift of \\(V(\\mathbf{s})\\) to evaluating the second derivative of a univariate auxiliary function.\n\n\n\n\n\n\n\n\nHyper-dual It√¥‚Äôs lemma\n\n\nFor a given \\(\\mathbf{s}\\), define the auxiliary functions \\(F_i:\\mathbb{R}\\rightarrow\\mathbb{R}\\) as \\[\nF_i(\\epsilon;\\mathbf{s}) = V\\!\\left(\\mathbf{s} + \\tfrac{\\mathbf{g}_i(\\mathbf{s})}{\\sqrt{2}}\\,\\epsilon + \\tfrac{\\mathbf{f}(\\mathbf{s})}{2m}\\,\\epsilon^2 \\right),\n\\] where \\(\\mathbf{f}(\\mathbf{s})\\) is the drift of \\(\\mathbf{s}\\) and \\(\\mathbf{g}_i(\\mathbf{s})\\) is the \\(i\\)-th column of the diffusion matrix \\(\\mathbf{g}(\\mathbf{s})\\).\nThen:\n\n\nDiffusion: The \\(1\\times m\\) diffusion matrix of \\(V(\\mathbf{s})\\) is \\[\n\\nabla V(\\mathbf{s})^{\\top}\\mathbf{g}(\\mathbf{s}) = \\sqrt{2}\\,[F_1'(0;\\mathbf{s}), F_2'(0;\\mathbf{s}), \\ldots, F_m'(0;\\mathbf{s})].\n\\]\n\nDrift: The drift of \\(V(\\mathbf{s})\\) is \\[\n\\mathcal{D}V(\\mathbf{s}) = F''(0;\\mathbf{s}), \\qquad \\text{where} \\quad F(\\epsilon;\\mathbf{s}) = \\sum_{i=1}^m F_i(\\epsilon;\\mathbf{s})\n\\]\n\n\n\n\n\n\nAn implication of the hyper-dual approach is that the computational complexity is \\[\n\\mathcal{O}(m \\times \\text{cost}(V(\\mathbf{s}))),\n\\] which is independent of the number of state variables.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#julia-implementation",
    "href": "Module05/Module05_Slides.html#julia-implementation",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia Implementation",
    "text": "Julia Implementation\nIt is straightforward to implement the hyper-dual approach to It√¥‚Äôs lemma in Julia.\nusing ForwardDiff, LinearAlgebra\nV(s) = sum(s.^2) # example function\nn, m  = 100, 1 # number of state variables and shocks\ns0, f, g = ones(n), ones(n), ones(n,m) # example values\n\n# Analytical drift\n‚àáf, H = 2*s0, Matrix(2.0*I, n,n) # gradient and Hessian\ndrift_analytical = ‚àáf'*f + 0.5*tr(g'*H*g) # analytical drift\n\n# Hyper-dual approach\nF(œµ) = sum([V(s0 + g[:,i]*œµ/sqrt(2) + f/(2m)*œµ^2) for i = 1:m])\ndrift_hyperdual = ForwardDiff.derivative(œµ -&gt; ForwardDiff.derivative(F, œµ), 0.0) # scalar 2nd derivative\n\nTo verify correctness, we can compare the analytical and hyper-dual drifts:\n\n# Compare\ndrift_analytical, drift_hyperdual\n\n(300.0, 300.0)",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#computational-cost",
    "href": "Module05/Module05_Slides.html#computational-cost",
    "title": "Machine Learning for Computational Economics",
    "section": "Computational Cost",
    "text": "Computational Cost\nThis figure shows how the cost of computing the drift of a function \\(V\\) scales with the number of state variables and Brownian shocks.\n\nThe cost is independent of the number of state variables.\nIt increases linearly ‚Äî instead of exponentially ‚Äî with the number of Brownian shocks.\n\n\n\n\n\n\n\n\nNotes: Cost is measured as the execution time of \\(\\frac{\\mathbb{E}dV}{dt}(\\mathbf{s})\\) relative to that of \\(V(\\mathbf{s})\\). The left panel fixes \\(m=1\\) and varies \\(n\\) from 1 to 100; the right panel fixes \\(n=100\\) and varies \\(m\\) from 1 to 100. In this example, \\(V\\) is a two-layer neural network, and execution times are averaged over 10,000 runs on a mini-batch of 512 states.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#computational-cost-continued",
    "href": "Module05/Module05_Slides.html#computational-cost-continued",
    "title": "Machine Learning for Computational Economics",
    "section": "Computational Cost (continued)",
    "text": "Computational Cost (continued)\nWe benchmark the performance of the hyper-dual approach to It√¥‚Äôs lemma by comparing the execution time of alternative methods.\n\nIn addition to finite differences and naive autodiff, we also consider the analytical expression for the derivatives\n\nThe hyper-dual It√¥‚Äôs lemma method is faster and less memory-intensive than using the analytical expression for the derivatives.\n\n\n\nMethod\nFLOPs\nMemory\nError\n\n\n\n\n1. Finite differences\n9,190,800\n112,442,048\n1.58%\n\n\n2. Naive autodiff\n2,100,501\n25,673,640\n0.00%\n\n\n3. Analytical\n20,501\n44,428\n0.00%\n\n\n4. Hyper-dual It√¥\n599\n6,044\n0.00%",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#overcoming-the-curse-of-representation",
    "href": "Module05/Module05_Slides.html#overcoming-the-curse-of-representation",
    "title": "Machine Learning for Computational Economics",
    "section": "Overcoming the Curse of Representation",
    "text": "Overcoming the Curse of Representation\nOur objective is to compute the value function \\(V(\\mathbf{s})\\) and policy function \\(\\mathbf{c}(\\mathbf{s})\\) satisfying the coupled functional equations: \\[\n0 = \\operatorname{HJB}(\\mathbf{s}, \\mathbf{c}(\\mathbf{s}), V(\\mathbf{s})), \\qquad \\mathbf{c}(\\mathbf{s}) = \\arg \\max_{\\mathbf{c}\\in\\Gamma(\\mathbf{s})} \\operatorname{HJB}(\\mathbf{s},\\mathbf{c},V(\\mathbf{s})),\n\\] where \\[\n\\operatorname{HJB}(\\mathbf{s},\\mathbf{c},V) = u(\\mathbf{c}) - \\rho V + \\nabla_{\\mathbf{s}} V(\\mathbf{s}) \\cdot \\mathbf{f}(\\mathbf{s},\\mathbf{c}) + \\tfrac{1}{2} \\operatorname{Tr}(\\mathbf{g}(\\mathbf{s},\\mathbf{c}) \\mathbf{H}_{\\mathbf{s}} V(\\mathbf{s}) \\mathbf{g}(\\mathbf{s},\\mathbf{c})),\n\\] and \\(\\mathbf{f}(\\mathbf{s},\\mathbf{c})\\) is the drift of \\(\\mathbf{s}\\) and \\(\\mathbf{g}(\\mathbf{s},\\mathbf{c})\\) is the diffusion matrix.\n\n\n\n\nTo solve for \\(V(\\mathbf{s})\\) and \\(\\mathbf{c}(\\mathbf{s})\\) numerically, we must represent them on a computer.\n\nA traditional approach is to discretize the state space and interpolate between grid points.\nIn Module 4, we showed that such grid-based approximations can be viewed as shallow neural networks with fixed breakpoints.\n\nNeural networks generalize this idea\n\nIt allows us to learn flexible breakpoints and nonlinear combinations of basis functions.\nA DNN can approximate complex value and policy functions with relatively few parameters.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#overcoming-the-curse-of-optimization",
    "href": "Module05/Module05_Slides.html#overcoming-the-curse-of-optimization",
    "title": "Machine Learning for Computational Economics",
    "section": "Overcoming the Curse of Optimization",
    "text": "Overcoming the Curse of Optimization\nWe now turn to the challenge of training the DNNs to satisfy the functional equations above.\n\nA key difficulty lies in performing the maximization step efficiently, without resorting to costly root-finding procedures.\n\n\n\n\n\nOur approach builds on generalized policy iteration (see, e.g., Sutton and Barto (2018))\n\nCombining with deep function approximation, alternating between policy evaluation and policy improvement.\nThis leads to the Deep Policy Iteration (DPI) algorithm.\n\n\n\n\nThe algorithm proceeds in three stages:\n\nSampling\nPolicy improvement\nPolicy evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying assumptions.\n\n\nFor clarity, we make several simplifying assumptions that can be relaxed in practice.\n\nWe adopt plain stochastic gradient descent (SGD) for parameter updates.\nWe perform exactly one iteration of policy evaluation and policy improvement at each update.\nWe use a quadratic loss function for the policy evaluation step.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#step-1-sampling",
    "href": "Module05/Module05_Slides.html#step-1-sampling",
    "title": "Machine Learning for Computational Economics",
    "section": "Step 1: Sampling",
    "text": "Step 1: Sampling\nWe begin by sampling a mini-batch of states \\(\\{\\mathbf{s}_i\\}_{i=1}^I\\) from the state space.\n\nThis can be done using a uniform distribution within plausible state-space bounds.\nAlternatively, we can use an estimated ergodic distribution based on previous iterations.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#step-2-policy-improvement",
    "href": "Module05/Module05_Slides.html#step-2-policy-improvement",
    "title": "Machine Learning for Computational Economics",
    "section": "Step 2: Policy Improvement",
    "text": "Step 2: Policy Improvement\nThe policy improvement step involves solving the maximization step for each state in the mini-batch.\n\nThis step can be computationally demanding and lies at the heart of the curse of optimization.\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized policy iteration.\n\n\nIn Module 3, we introduced the policy function iteration, which alternates between policy evaluation and policy improvement steps.\n\nIn the policy evaluation step, we solve for the new value function \\(V_{n+1}(\\mathbf{s})\\) given the policy \\(\\mathbf{c}_n(\\mathbf{s})\\).\nIn the policy improvement step, we solve for the new policy \\(\\mathbf{c}_{n+1}(\\mathbf{s})\\) given the current value function \\(V_{n+1}(\\mathbf{s})\\).\n\nHowever, when the initial guess for \\(V\\) is far from optimal, fully solving the maximization problem at each iteration is inefficient.\n\nThis motivates an approximate policy improvement step that performs only a single gradient-based update in the direction of improvement.\n\n\n\n\n\n\n\n\n\n\nFix the current parameter vectors for the value and policy functions, \\(\\mathbf{\\theta}_V^{j-1}\\) and \\(\\mathbf{\\theta}_C^{j-1}\\).\n\nEvaluate the value and policy functions at the mini-batch of states: \\(V_{j-1,i} = V(\\mathbf{s}_i; \\mathbf{\\theta}_V^{j-1})\\) and \\(\\mathbf{c}_{j-1,i} = \\mathbf{c}(\\mathbf{s}_i; \\mathbf{\\theta}_C^{j-1})\\).\nWe can use the \\(\\{\\mathbf{s}_i, V_{j-1,i},\\mathbf{c}_{j-1,i}\\}_{i=1}^I\\) as a training data to update the policy function.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#step-2-policy-improvement-continued",
    "href": "Module05/Module05_Slides.html#step-2-policy-improvement-continued",
    "title": "Machine Learning for Computational Economics",
    "section": "Step 2: Policy Improvement (continued)",
    "text": "Step 2: Policy Improvement (continued)\nOur goal is to choose the policy function to maximize (or minimize minus) the value of the HJB operator\n\nWe can then define the loss function as follows: \\[\n\\mathcal{L}_p(\\mathbf{\\theta}_C) = - \\frac{1}{2I}\\sum_{i=1}^I \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_{C}^{j-1}, \\mathbf{\\theta}_{V}^{j-1}),\n\\] where \\(\\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_{C}^{j-1}, \\mathbf{\\theta}_{V}^{j-1}) \\equiv \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{c}_{j-1,i}, V_{j-1,i})\\).\n\n\n\n\n\nWe perform one step of gradient descent on the loss function to update the policy function parameters:\n\n\n\n\n\n\n\nPolicy improvement step.\n\n\n\\[\\begin{equation}\\label{eq:policy_improvement_step}\n    \\mathbf{\\theta}_C^{j}\n    = \\mathbf{\\theta}_C^{j-1}\n    + \\eta_C \\frac{1}{I}\\sum_{i=1}^I\n      \\nabla_{\\mathbf{\\theta}_C} \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j-1}, \\mathbf{\\theta}_V^{j-1}),\n  \\end{equation}\\]\n\n\n\n\nwhere \\(\\eta_C\\) is the learning rate controlling the step size in parameter space.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#step-3-policy-evaluation-i",
    "href": "Module05/Module05_Slides.html#step-3-policy-evaluation-i",
    "title": "Machine Learning for Computational Economics",
    "section": "Step 3: Policy Evaluation I",
    "text": "Step 3: Policy Evaluation I\nWe now update the value function given the new policy parameters, \\(\\mathbf{\\theta}_C^{j}\\).\n\nWe present two alternative update rules, each with distinct trade-offs.\n\n\n\n\n\nThe first rule mirrors the explicit method for finite-differences in Module 3.\n\nConsider a false-transient formulation that iterates the value function backward in (pseudo-)time:\n\n\\[\n\\frac{V(\\mathbf{s}; \\mathbf{\\theta}_V^{j}) - V(\\mathbf{s}_i; \\mathbf{\\theta}_V^{j-1})}{\\Delta t} = \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j-1}) \\qquad \\Rightarrow \\qquad V(\\mathbf{s}; \\mathbf{\\theta}_V^{j}) = \\underbrace{V(\\mathbf{s}_i; \\mathbf{\\theta}_V^{j-1}) + \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j-1}) \\Delta t}_{\\text{target value } \\hat{V}_i^{j-1}},\n\\] where the HJB is evaluated at the new policy parameters, \\(\\mathbf{\\theta}_C^{j}\\), but old value function parameters, \\(\\mathbf{\\theta}_V^{j-1}\\).\n\n\n\nThis turns the problem into a supervised learning task given the training data \\(\\{\\mathbf{s}_i, \\hat{V}_i^{j-1}\\}_{i=1}^I\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#step-3-policy-evaluation-i-continued",
    "href": "Module05/Module05_Slides.html#step-3-policy-evaluation-i-continued",
    "title": "Machine Learning for Computational Economics",
    "section": "Step 3: Policy Evaluation I (continued)",
    "text": "Step 3: Policy Evaluation I (continued)\nDefine the loss function as the mean-squared error between the target value and the value function: \\[\n\\mathcal{L}(\\mathbf{\\theta}_V) = \\frac{1}{2I}\\sum_{i=1}^I (V(\\mathbf{s}_i; \\mathbf{\\theta}_V) - \\hat{V}_i^{j-1})^2,\n\\]\n\n\n\n\nEvaluating the gradient at \\(\\mathbf{\\theta}_V^{j-1}\\) yields \\[\n\\nabla_{\\mathbf{\\theta}_V} \\mathcal{L}(\\mathbf{\\theta}_V^{j-1}) = -\\frac{\\Delta t}{I}\\sum_{i=1}^I \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j-1}) \\nabla_{\\mathbf{\\theta}_V} V(\\mathbf{s}_i; \\mathbf{\\theta}_V),\n\\]\n\n\n\n\n\nTaking one step of gradient descent, the corresponding update rule is:\n\n\n\n\n\n\n\nPolicy evaluation step.\n\n\n\\[\n    \\mathbf{\\theta}_V^{j}\n    = \\mathbf{\\theta}_V^{j-1}\n    + \\eta_V \\frac{\\Delta t}{I}\\sum_{i=1}^I \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j-1}) \\nabla_{\\mathbf{\\theta}_V} V(\\mathbf{s}_i; \\mathbf{\\theta}_V^{j-1}),\n  \\]\n\n\n\n\nwhere \\(\\eta_V\\) is the learning rate.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#step-3-policy-evaluation-ii",
    "href": "Module05/Module05_Slides.html#step-3-policy-evaluation-ii",
    "title": "Machine Learning for Computational Economics",
    "section": "Step 3: Policy Evaluation II",
    "text": "Step 3: Policy Evaluation II\nThe second rule mirrors the implicit method for finite-differences in Module 3: \\[\n\\frac{V(\\mathbf{s}; \\mathbf{\\theta}_V^{j}) - V(\\mathbf{s}_i; \\mathbf{\\theta}_V^{j-1})}{\\Delta t} = \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j}),\n\\] As in the implicit finite-difference method, we can take the limit as \\(\\Delta t \\to \\infty\\).\n\n\n\n\n\n\nIn this case, the loss function becomes MSE of HJB residuals: \\[\n\\mathcal{L}(\\mathbf{\\theta}_V) = \\frac{1}{2I}\\sum_{i=1}^I \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V)^2,\n\\]\n\nThe gradient is given by \\[\n\\nabla_{\\mathbf{\\theta}_V} \\mathcal{L}(\\mathbf{\\theta}_V) = \\frac{1}{I}\\sum_{i=1}^I \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V) \\nabla_{\\mathbf{\\theta}_V} \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V),\n\\]\n\n\n\n\n\n\nEvaluating at \\(\\mathbf{\\theta}_V^{j-1}\\) yields\n\n\n\n\n\n\n\nPolicy evaluation step 2.\n\n\n\\[\n    \\mathbf{\\theta}_V^{j}\n    = \\mathbf{\\theta}_V^{j-1}\n    - \\eta_V \\frac{1}{I}\\sum_{i=1}^I \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j-1}) \\nabla_{\\mathbf{\\theta}_V} \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V^{j-1}),\n  \\]\n\n\n\n\nwhere \\(\\eta_V\\) is the learning rate.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-deep-policy-iteration-algorithm",
    "href": "Module05/Module05_Slides.html#the-deep-policy-iteration-algorithm",
    "title": "Machine Learning for Computational Economics",
    "section": "The Deep Policy Iteration Algorithm",
    "text": "The Deep Policy Iteration Algorithm\nWe can now summarize the complete algorithm:\n\nAlgorithm: Deep Policy Iteration (DPI)\nInput: Initial parameters \\(\\mathbf{\\theta}_V^{0}\\) and \\(\\mathbf{\\theta}_C^{0}\\)\nOutput: Value function \\(V(\\mathbf{s}; \\mathbf{\\theta}_V)\\), policy function \\(\\mathbf{c}(\\mathbf{s}; \\mathbf{\\theta}_C)\\)\nInitialize: \\(j \\gets 0\\)\nRepeat for \\(j = 1, 2, \\ldots\\):\n\nSampling:\nSample a mini-batch of states \\(\\{\\mathbf{s}_i\\}_{i=1}^I\\).\nPolicy improvement (actor update):\nUpdate \\(\\mathbf{\\theta}_C\\) with one step of gradient descent on the loss function.\nPolicy evaluation (critic update):\nUpdate \\(\\mathbf{\\theta}_V\\) using the explicit or implicit policy evaluation step.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trade-off between the two update rules.\n\n\nMinimizing the Bellman residual directly is known to be more stable but it is often slower than iterative policy evaluation.\n\nMoreover, the implicit policy evaluation step requires relatively costly third-order derivatives, since \\(\\nabla_{\\mathbf{\\theta}_V} \\operatorname{HJB}(\\mathbf{s}_i, \\mathbf{\\theta}_C^{j}, \\mathbf{\\theta}_V)\\) involves the Hessian of \\(V\\).\nAs a rule of thumb, it is preferable to start with the explicit policy evaluation step for speed, and switch to the implicit policy evaluation step if necessary.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#asset-pricing-the-two-trees-model",
    "href": "Module05/Module05_Slides.html#asset-pricing-the-two-trees-model",
    "title": "Machine Learning for Computational Economics",
    "section": "Asset Pricing: The Two-Trees Model",
    "text": "Asset Pricing: The Two-Trees Model\nNext, we apply the DPI algorithm to solve a variety of economic and financial problems.\n\nWe focus on three canonical domains of finance: asset pricing, corporate finance, and portfolio choice.\nWe illustrate how the DPI algorithm can be applied to solve a variety of economic and financial problems.\n\n\n\n\n\nTo start, we consider the familiar two-trees model from Module 3.\n\n\n\n\n\nThe pricing condition for a log investor implies \\[\nv_t = \\mathbb{E}_t \\left[ \\int_{0}^{\\infty} e^{-\\rho s} s_{t+s}\\,ds \\right],\n\\] where the relative share process \\(s_t\\) evolves as \\[\nd s_t = - 2 \\sigma^2 s_t(1-s_t)\\!\\left(s_t - \\tfrac12\\right) dt\n          + \\sigma s_t(1-s_t)(dB_{1,t} - dB_{2,t}).\n\\]\n\nThe price-consumption ratio \\(v_t\\) satisfies the HJB equation: \\[\n\\rho v = s - v_s \\, 2\\sigma^2 s(1-s)\\!\\left(s-\\tfrac12\\right)\n          + \\frac{1}{2} v_{ss}\\,\\big(2\\sigma^2 s^2(1-s)^2\\big),\n\\] with boundary conditions \\(v(0) = 0\\) and \\(v(1) = 1/\\rho\\).\n\n\n\n\n\n\n\nIt is straightforward to solve this one-dimensional problem using finite differences or collocation methods\n\nBut we solve it here using the DPI framework to illustrate the workflow.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#julia-implementation-1",
    "href": "Module05/Module05_Slides.html#julia-implementation-1",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia Implementation",
    "text": "Julia Implementation\nWe now implement the two-trees model in Julia, starting with the model struct.\n\n@kwdef struct TwoTrees\n    œÅ::Float64 = 0.04\n    œÉ::Float64 = sqrt(0.04)\n    Œº::Float64 = 0.02\n    Œº‚Çõ::Function = s -&gt; @. -2 * œÉ^2 * s * (1-s) * (s-0.5)   # drift of s\n    œÉ‚Çõ::Function = s -&gt; @. sqrt(2) * œÉ * s * (1-s)          # diffusion of s\nend;\n\nm = TwoTrees()\n\nTwoTrees(0.04, 0.2, 0.02, var\"#6#10\"{Float64}(0.2), var\"#7#11\"{Float64}(0.2))\n\n\n\n\n\n\nWe next implement the hyper-dual approach to Ito‚Äôs lemma to compute the drift and diffusion of the state variable \\(s\\).\n\nusing ForwardDiff\nfunction drift_hyper(V::Function, s::AbstractMatrix, m::TwoTrees)\n    F(œµ) = V(s + m.œÉ‚Çõ(s)/sqrt(2)*œµ + m.Œº‚Çõ(s)/2*œµ^2)\n    return ForwardDiff.derivative(œµ -&gt; ForwardDiff.derivative(F, œµ), 0.0)\nend;\n\n\n\n\n\n\nTo validate the implementation, we can compare the analytical and hyper-dual drifts:\n\nusing Random\nrng = Xoshiro(0)  \ns   = rand(rng, 1, 1000)\n# Exact drift for test function\nV_test(s) = sum(s.^2, dims = 1)\ndrifts_exact = map(1:size(s, 2)) do i\n    ‚àáV, H = 2 * s[:,i], 2 * Matrix(I,length(s[:,i]),length(s[:,i]))\n    ‚àáV' * m.Œº‚Çõ(s[:,i]) + 0.5 * tr(m.œÉ‚Çõ(s[:,i])' * H * m.œÉ‚Çõ(s[:,i]))\nend'\ndrifts_hyper = drift_hyper(V_test, s, m)\nerrors = maximum(abs.(drifts_exact - drifts_hyper))\n\n1.734723475976807e-18",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#neural-network-implementation",
    "href": "Module05/Module05_Slides.html#neural-network-implementation",
    "title": "Machine Learning for Computational Economics",
    "section": "Neural Network Implementation",
    "text": "Neural Network Implementation\nWe now implement the neural network representation of the value function.\n\nusing Lux\nmodel = Chain(\n    Dense(1 =&gt; 25, Lux.gelu),\n    Dense(25 =&gt; 25, Lux.gelu),\n    Dense(20 =&gt; 1)\n)\n\n\nChain(\n    layer_1 = Dense(1 =&gt; 25, gelu_tanh),          # 50 parameters\n    layer_2 = Dense(25 =&gt; 25, gelu_tanh),         # 650 parameters\n    layer_3 = Dense(20 =&gt; 1),                     # 21 parameters\n)         # Total: 721 parameters,\n          #        plus 0 states.\n\n\n\n\n\n\n\nWe initialize the parameters and optimizer state using the Adam optimizer with a learning rate of \\(10^{-3}\\).\n\nusing Optimisers\nrng = Xoshiro(0)\nps, ls = Lux.setup(rng, model) |&gt; f64\nopt = Adam(1e-3)\nos = Optimisers.setup(opt, ps)\n\n\n(layer_1 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0; 0.0; ‚Ä¶ ; 0.0; 0.0;;], [0.0; 0.0; ‚Ä¶ ; 0.0; 0.0;;], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999)))), layer_2 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], [0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999)))), layer_3 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0 0.0 ‚Ä¶ 0.0 0.0], [0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0], [0.0], (0.9, 0.999)))))\n\n\n\n\n\n\n\n\nWe define the loss function as the mean-squared error between the value function and the target value.\n\nloss_fn(ps, ls, s, target) = mean(abs2, model(s, ps, ls)[1] - target)\n\n# Target value function\nfunction target(v, s, m; Œît = 0.2)\n    hjb = s + drift_hyper(v, s, m) - m.œÅ * v(s)\n    return v(s) + hjb * Œît\nend;",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#training-the-neural-network",
    "href": "Module05/Module05_Slides.html#training-the-neural-network",
    "title": "Machine Learning for Computational Economics",
    "section": "Training the Neural Network",
    "text": "Training the Neural Network\nWe now train the neural network using the Adam optimizer.\n# Training loop\nloss_history = Float64[]\nfor i = 1:40_000\n    s_batch = rand(rng, 1,  128)\n    tgt     = target(s-&gt; model(s, ps, ls)[1], s_batch, m, Œît = 1.0)\n    loss    = loss_fn(ps, ls, s_batch, tgt)\n    grad    = gradient(p -&gt; loss_fn(p, ls, s_batch, tgt), ps)[1]\n    os, ps  = Optimisers.update(os,ps, grad)\n    push!(loss_history, loss)\nend\n\n\n\n\n\n\n\n\n\nTraining loss over iterations.\n\n\n\n\n\n\n\n\nDPI prediction vs.¬†analytical solution.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-lucas-orchard-model",
    "href": "Module05/Module05_Slides.html#the-lucas-orchard-model",
    "title": "Machine Learning for Computational Economics",
    "section": "The Lucas Orchard Model",
    "text": "The Lucas Orchard Model\nWe now extend the two-trees model to a multi-tree economy, known as the Lucas orchard model (see, e.g., Martin (2013)).\n\nBy varying the number of trees, we can examine how the DPI algorithm scales with the dimensionality of the state space.\n\n\n\n\n\nConsider a representative investor with log utility who can invest in a riskless asset and \\(N\\) risky assets.\n\nEach risky asset \\(i\\) pays a continuous dividend stream \\(D_{i,t}\\) that follows a geometric Brownian motion: \\[\n\\frac{d D_{i,t}}{D_{i,t}} = \\mu_i\\,dt + \\sigma_i\\,dB_{i,t},\n\\] where each \\(B_{i,t}\\) is a Brownian motion satisfying \\(dB_{i,t}\\,dB_{j,t} = 0\\) for \\(i \\neq j\\).\n\n\n\n\n\n\n\n\n\n\nThe HJB equation.\n\n\nDefine the vector of state variables \\(\\mathbf{s}_t = (s_{1,t}, \\ldots, s_{N,t})^{\\!\\top}\\), where \\(s_{i,t} \\equiv D_{i,t}/C_t\\) and \\(C_t = \\sum_{i=1}^N D_{i,t}\\).\n\nLet \\(v_{i,t} \\equiv P_{i,t}/C_t\\) denote the price‚Äìconsumption ratio of asset \\(i\\).\nThe pricing condition for a log investor implies \\[\n\\rho\\,v_i(\\mathbf{s})\n= s_i\n  + \\nabla_{\\mathbf{s}} v_i(\\mathbf{s})^{\\!\\top} \\mathbf{\\mu}_s(\\mathbf{s})\n  + \\frac{1}{2}\\operatorname{Tr}\\!\\left[\n      \\mathbf{\\sigma}_s(\\mathbf{s})^{\\!\\top}\n      \\mathbf{H}_{\\mathbf{s}} v_i(\\mathbf{s})\n      \\mathbf{\\sigma}_s(\\mathbf{s})\n    \\right]\n\\] subject to the boundary conditions \\(v_i(0) = 0\\) and \\(v_i(1) = 1/\\rho\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#julia-implementation-2",
    "href": "Module05/Module05_Slides.html#julia-implementation-2",
    "title": "Machine Learning for Computational Economics",
    "section": "Julia Implementation",
    "text": "Julia Implementation\nWe now implement the Lucas orchard model in Julia.\n\nThe workflow of the DPI algorithm for the Lucas orchard model is virtually identical to that for the simple two-trees model.\n\nAs usual, we start by defining the model struct.\n\n@kwdef struct LucasOrchard\n    œÅ::Float64 = 0.04\n    N::Int = 10\n    œÉ::Vector{Float64} = sqrt(0.04) * ones(N)\n    Œº::Vector{Float64} = 0.02 * ones(N)\n    Œºc::Function = s -&gt; Œº' * s\n    œÉc::Function = s -&gt; [s[i,:]' * œÉ[i] for i in 1:N]\n    Œº‚Çõ::Function = s -&gt;  s .* (Œº .- Œºc(s)- s.*œÉ.^2 .+ sum(œÉc(s)[i].^2 for i in 1:N))\n    œÉ‚Çõ::Function = s -&gt; [s .* ([j == i ? œÉ[i] : 0 for j in 1:N] .- œÉc(s)[i]) for i in 1:N] \nend;\n\n\n\n\n\n\n# Instantiate the model\nusing Distributions, Random\nm         = LucasOrchard(N = 10) # number of assets\nrng, d    = MersenneTwister(0), Dirichlet(ones(m.N)) # Dirichlet distribution\ns_samples = rand(rng, d, 1_000) # N x 1_000 matrix\nm.Œº‚Çõ(s_samples) # N x 1_000 matrix\n\n10√ó1000 Matrix{Float64}:\n  4.24309e-5    0.000237275   0.000815818  ‚Ä¶   8.57681e-5   -0.00169363\n -0.000259516   0.000134325   0.000132953      0.000550971   0.000463926\n  0.000193812   0.000154873   0.000418759      0.000564647   5.89399e-5\n  0.000118302   0.00023403   -0.00379805       2.65856e-5    0.000710309\n  0.000187861   0.000109689   0.000126233      0.000119859  -0.00071501\n -2.52101e-6    4.92026e-5    0.000566168  ‚Ä¶   0.00054645    0.000122228\n  0.000199586   4.887e-6      7.50461e-5       0.000573028   0.000189173\n  2.76043e-5   -0.000877846   0.000535931      0.000335935   0.00040788\n -0.000704084  -9.64787e-5    0.000686905     -0.00331773    0.000103212\n  0.000196524   5.00425e-5    0.000440236      0.00051449    0.000352971",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-hyper-dual-approach-to-itos-lemma",
    "href": "Module05/Module05_Slides.html#the-hyper-dual-approach-to-itos-lemma",
    "title": "Machine Learning for Computational Economics",
    "section": "The Hyper-dual Approach to Ito‚Äôs Lemma",
    "text": "The Hyper-dual Approach to Ito‚Äôs Lemma\nWe next implement the hyper-dual approach to Ito‚Äôs lemma to compute the drift and diffusion of the state variable \\(s\\).\n\nusing ForwardDiff\nfunction drift_hyper(V::Function, s::AbstractMatrix, m::LucasOrchard)\n    N, œÉs, Œºs = m.N, m.œÉ‚Çõ(s), m.Œº‚Çõ(s) # Preallocations\n    F(œµ) = sum(V(s .+ œÉs[i] .* (œµ / sqrt(2)) .+ Œºs .* (œµ^2 / (2 * N))) for i in 1:N)\n    return ForwardDiff.derivative(œµ -&gt; ForwardDiff.derivative(F, œµ), 0.0)\nend;\n\n\n\n\nThe implementation is virtually identical to that for the two-trees model.\n\nBut now we are dealing with the case of multiple state variables and Brownian motions.\n\n\n\n\n\n\n\n\n\nThe importance of preallocations.\n\n\nRelative to the two-trees model, we now preallocate arrays for the drift and diffusion of the state variable \\(s\\),\n\nInstead of constructing them inside loops over \\(i = 1, \\ldots, N\\).\nIn higher-dimensional problems, preallocations avoid repeated memory allocation and garbage collection, improving performance.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#neural-network-implementation-1",
    "href": "Module05/Module05_Slides.html#neural-network-implementation-1",
    "title": "Machine Learning for Computational Economics",
    "section": "Neural Network Implementation",
    "text": "Neural Network Implementation\nWe next implement the neural network representation of the value function.\n\nWe use essentially the same architecture as in the two-trees model.\nBut now the input is the \\(N\\)-dimensional vector of state variables \\(\\mathbf{s}_t\\).\n\n\nmodel = Chain(\n    Dense(m.N =&gt; 25, Lux.gelu),\n    Dense(25 =&gt; 25, Lux.gelu),\n    Dense(25 =&gt; 1)\n)\n\n\nChain(\n    layer_1 = Dense(10 =&gt; 25, gelu_tanh),         # 275 parameters\n    layer_2 = Dense(25 =&gt; 25, gelu_tanh),         # 650 parameters\n    layer_3 = Dense(25 =&gt; 1),                     # 26 parameters\n)         # Total: 951 parameters,\n          #        plus 0 states.\n\n\n\n\n\n\n\nWe initialize the parameters and optimizer state using the Adam optimizer with a learning rate of \\(10^{-3}\\).\n\nps, ls = Lux.setup(rng, model) |&gt; f64\nopt = Adam(1e-3)\nos = Optimisers.setup(opt, ps)\n\n\n(layer_1 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], [0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999)))), layer_2 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], [0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  ‚Ä¶  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999)))), layer_3 = (weight = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0 0.0 ‚Ä¶ 0.0 0.0], [0.0 0.0 ‚Ä¶ 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), ([0.0], [0.0], (0.9, 0.999)))))\n\n\n\n\n\nWe define the loss function and the target function as before.\n\nloss_fn(ps, ls, s, target) = mean(abs2, model(s, ps, ls)[1] - target)\n\n# Target\nfunction target(v, s, m; Œît = 0.2)\n    vÃÖ = v(s)\n    hjb = s[1,:]' + drift_hyper(v, s, m) - m.œÅ * vÃÖ\n    return vÃÖ + hjb * Œît, mean(abs2, hjb)\nend;",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#training-the-neural-network-1",
    "href": "Module05/Module05_Slides.html#training-the-neural-network-1",
    "title": "Machine Learning for Computational Economics",
    "section": "Training the Neural Network",
    "text": "Training the Neural Network\n\n\n# Training parameters\nmax_iter, Œît = 40_000, 1.0  \n# Sampling interior and boundary states\nd_int  = Dirichlet(ones(m.N))            # Interior region\nd_edge = Dirichlet(0.05 .* ones(m.N))  # Boundary region\n# Loss history and exponential moving average loss\nloss_history, loss_ema_history, Œ±_ema = Float64[], Float64[], 0.99\n# Training loop\np = Progress(max_iter; desc=\"Training...\", dt=1.0) #progress bar\nfor i = 1:max_iter\n    if rand(rng) &lt; 0.50\n        s_batch = rand(rng, d_int, 128)\n    else\n        s_batch = rand(rng, d_edge, 128)\n    end\n    v(s)       = model(s, ps, ls)[1] # define value function\n    tgt, hjb_res = target(v, s_batch, m, Œît = Œît) #target/residual\n    loss, back = Zygote.pullback(p -&gt; loss_fn(p,ls,s_batch,tgt), ps)\n    grad       = first(back(1.0)) # gradient\n    os, ps     = Optimisers.update(os,ps, grad) # update parameters\n    loss_ema   = i==1 ? loss : Œ±_ema*loss_ema + (1.0-Œ±_ema)*loss\n    push!(loss_history, loss)\n    push!(loss_ema_history, loss_ema)\n    next!(p, showvalues = [(:iter, i),(\"Loss\", loss), \n        (\"Loss EMA\", loss_ema), (\"HJB residual\", hjb_res)])\nend\n\nWe sample from two Dirichlet distributions:\n\nInterior region: uniform distribution over the simplex.\nBoundary region: highly concentrated near the edges.\n\n\n\n\nTraining loss.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#test-set-evaluation-i",
    "href": "Module05/Module05_Slides.html#test-set-evaluation-i",
    "title": "Machine Learning for Computational Economics",
    "section": "Test set evaluation I",
    "text": "Test set evaluation I\nWe next evaluate the model‚Äôs performance on out-of-sample test sets.\n\n\n\n\n\n\n\n\nTwo-trees prediction.\n\n\n\nWe consider an extremely asymmetric configuration:\n\n\\(\\mathbf{s} = (s_1, 1 - s_1, 0, \\ldots, 0)\\), the two-trees special case.\nThis configuration lies outside the region used for training.\n\n\n\n\nThe network‚Äôs prediction replicates the analytical solution.\n\nEven though was not trained on this configuration,",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#test-set-evaluation-ii",
    "href": "Module05/Module05_Slides.html#test-set-evaluation-ii",
    "title": "Machine Learning for Computational Economics",
    "section": "Test set evaluation II",
    "text": "Test set evaluation II\nOur second test set draws states from a symmetric Dirichlet distribution with parameters \\(\\boldsymbol{\\alpha} = \\alpha_{\\text{scale}} (1, 1, \\ldots, 1)\\)\n\n\\(\\alpha_{\\text{scale}}\\) controls the concentration of points within the simplex.\nSamples are concentrated near the center (or edges) of the simplex when \\(\\alpha_{\\text{scale}} &gt; 1\\) (or \\(&lt; 1\\)).\n\n\n\n\n\n\n\nDirichlet densities.\n\n\n\n\n\n\n\n\n\nOrchard residuals.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#comparison-with-other-methods",
    "href": "Module05/Module05_Slides.html#comparison-with-other-methods",
    "title": "Machine Learning for Computational Economics",
    "section": "Comparison with other methods",
    "text": "Comparison with other methods\nWe next compare the DPI algorithm with other methods for solving high-dimensional dynamic models.\n\nWe use the Lucas orchard model as a benchmark.\n\n\n\n\n\nFinite-difference schemes become computationally infeasible beyond a few dimensions\n\nChebyshev collocation on full tensor-product grids also suffers from exponential growth in cost.\n\n\n\n\nWe then compare the time to solution of the DPI algorithm with the Smolyak method (Smolyak (1963)).\n\nThe Smolyak method is a sparse-grid technique for approximating multivariate functions\nIt is commonly used for solving high-dimensional dynamic models.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#time-to-solution",
    "href": "Module05/Module05_Slides.html#time-to-solution",
    "title": "Machine Learning for Computational Economics",
    "section": "Time to solution",
    "text": "Time to solution\n\n\nNotes: Figure shows the time-to-solution of the DPI algorithm, measured by the number of minutes required for the MSE or 90th-percentile squared error to fall below \\(10^{-8}\\). The parameter values are \\(\\rho = 0.04\\), \\(\\gamma = 1\\), \\(\\varrho = 0.0\\), \\(\\mu = 0.015\\), and \\(\\sigma = 0.1\\). The HJB residuals are computed on a random sample of \\(2^{13}\\) points from the state space.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#smolyak-vs.-dpi-algorithm",
    "href": "Module05/Module05_Slides.html#smolyak-vs.-dpi-algorithm",
    "title": "Machine Learning for Computational Economics",
    "section": "Smolyak vs.¬†DPI algorithm",
    "text": "Smolyak vs.¬†DPI algorithm\n\n\nNotes: Figure compares the time-to-solution of the DPI method and the Smolyak methods of orders 2, 3, and 4. The tolerance is set to \\(10^{-3}\\), the highest accuracy threshold reached by all Smolyak variants. The parameter values are \\(\\rho = 0.04\\), \\(\\gamma = 1\\), \\(\\varrho = 0.0\\), \\(\\mu = 0.015\\), and \\(\\sigma = 0.1\\). The HJB residuals are computed on a random sample of \\(2^{13}\\) points from the state space.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-hennessy-and-whited-2007-model",
    "href": "Module05/Module05_Slides.html#the-hennessy-and-whited-2007-model",
    "title": "Machine Learning for Computational Economics",
    "section": "The Hennessy and Whited (2007) Model",
    "text": "The Hennessy and Whited (2007) Model\nWe now apply the DPI algorithm to a corporate finance problem, a simplified version of the model in Hennessy and Whited (2007).\n\nThis problem illustrates how the DPI algorithm can handle problems with kinks and inaction regions.\n\n\n\n\n\n\n\nConsider a firm with operating profits \\(\\pi(k_t, z_t) = e^{z_t} k_t^\\alpha\\).\n\nLog productivity follows an Ornstein‚ÄìUhlenbeck process: \\[\nd z_t = -\\theta (z_t - \\bar{z})\\,dt + \\sigma\\,dB_t,\n\\] where \\(\\theta, \\sigma &gt; 0\\).\nGiven investment rate \\(i_t\\) and depreciation rate \\(\\delta\\), capital evolves as \\[\nd k_t = (i_t - \\delta)\\,k_t\\,dt.\n\\]\n\n\nThe firm faces linear equity issuance costs \\(\\lambda &gt; 0\\).\n\nOperating profits net of adjustment costs are \\[\nD^*(k_t, z_t, i_t) = e^{z_t} k_t^\\alpha - \\left(i_t + 0.5 \\chi i_t^2 \\right) k_t,\n\\] where \\(\\chi &gt; 0\\) is the adjustment cost parameter.\nThe firm‚Äôs dividend policy is given by \\[\nD_t = D_t^* (1 + \\lambda \\mathbf{1}_{D_t^* &lt; 0}).\n\\]\n\n\n\n\n\n\n\nThe firm chooses the investment rate \\(i_t\\) to maximize the expected discounted value of future dividends: \\[\nv(\\mathbf{s}_0) =\n  \\max_{\\{i_t\\}_{t\\ge 0}}\n  \\mathbb{E}\\left[ \\int_{0}^{\\infty} e^{-\\rho s} D(k_s, z_s, i_s)\\,ds \\right],\n\\] subject to the law of motion for the state vector \\(\\mathbf{s}_t = (k_t, z_t)^{\\!\\top}\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#the-hjb-equation",
    "href": "Module05/Module05_Slides.html#the-hjb-equation",
    "title": "Machine Learning for Computational Economics",
    "section": "The HJB Equation",
    "text": "The HJB Equation\nThe HJB equation is given by \\[\n  0 = \\max_{i}\\, \\operatorname{HJB}(\\mathbf{s}, i, v(\\mathbf{s})),\n\\] where \\[\n  \\operatorname{HJB}(\\mathbf{s}, i, v) =\n  D(k, z, i)\n  + \\nabla v^\\top \\mathbf{\\mu}_{s}(\\mathbf{s}, i)\n  + \\tfrac{1}{2}\\,\\mathbf{\\sigma}_{s}(\\mathbf{s}, i)^\\top \\mathbf{H}_{\\mathbf{s}} v\\, \\mathbf{\\sigma}_{s}(\\mathbf{s}, i)\n  - \\rho v.\n\\]\nThe first-order condition for the optimal investment rate is \\[\n  \\frac{\\partial \\operatorname{HJB}}{\\partial i}\n  = -\\big(1 + \\lambda\\,\\mathbf{1}_{D^*(k,z,i)&lt;0}\\big)\\big[1 + \\chi(i-\\delta)\\big]k\n    + v_k(\\mathbf{s})k = 0.\n\\]\n\n\n\n\nConsider a special case where there investment is fixed at \\(i = \\delta\\) and productivity is constant \\(\\theta = \\sigma = 0\\).\n\nIn this case, capital is constant\nFrom the HJB equation, we obtain the value function\n\n\\[\\begin{equation}\\label{eq:hw_value_function_special_case}\n  v(k, z) = \\frac{D(k, z, \\delta)}{\\rho} =\n  \\begin{cases}\n    \\frac{e^{z} k^\\alpha - \\delta k}{\\rho}, & \\text{if } k \\leq k_{\\max}(z), \\\\[5pt]\n    \\frac{e^{z} k^\\alpha - \\delta k}{\\rho} (1+\\lambda), & \\text{if } k &gt; k_{\\max}(z),\n  \\end{cases}\n\\end{equation}\\] where \\(k_{\\max}(z) = \\left(\\tfrac{e^{z}}{\\delta}\\right)^{\\!\\frac{1}{1-\\alpha}}\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#value-function-and-derivatives-for-special-case",
    "href": "Module05/Module05_Slides.html#value-function-and-derivatives-for-special-case",
    "title": "Machine Learning for Computational Economics",
    "section": "Value function and derivatives for special case",
    "text": "Value function and derivatives for special case\n\n\n\n\n\nValue function\n\n\n\n\n\n\n\n\nValue function derivative",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#optimal-dividend-policy-and-investment-rate",
    "href": "Module05/Module05_Slides.html#optimal-dividend-policy-and-investment-rate",
    "title": "Machine Learning for Computational Economics",
    "section": "Optimal Dividend Policy and Investment Rate",
    "text": "Optimal Dividend Policy and Investment Rate\n\n\n\n\n\nDividends\n\n\n\n\n\n\nInvestment Rate",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#global-sensitivity-analysis",
    "href": "Module05/Module05_Slides.html#global-sensitivity-analysis",
    "title": "Machine Learning for Computational Economics",
    "section": "Global sensitivity analysis",
    "text": "Global sensitivity analysis\nThis problem has only two state variables, so it can be easily solved using finite differences\n\nBut we are often interested in the solution for a large number of parameter values\nFor instance, how does the solution vary with investment or equity issuance costs?\n\n\n\n\n\nIn particular, we are interested in knowing which equilibrium moments are more sensitive to parameters\n\nThis way we know which aspects of the data are more informative about the parameters\nTo answer these questions, we need to perform a global sensitivity analysis\n\n\n\n\n\n\nPerforming such global sensitivity analysis can be computationally very costly\n\nUsing the DPI method, we can overcome this challenge\nSolution: add the vector of parameters to the state space\nIn deep-reinforcement learning, this is known as universal value functions",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#global-sensitivity-analysis-results",
    "href": "Module05/Module05_Slides.html#global-sensitivity-analysis-results",
    "title": "Machine Learning for Computational Economics",
    "section": "Global sensitivity analysis results",
    "text": "Global sensitivity analysis results",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#portfolio-choice-with-realistic-dynamics",
    "href": "Module05/Module05_Slides.html#portfolio-choice-with-realistic-dynamics",
    "title": "Machine Learning for Computational Economics",
    "section": "Portfolio Choice with Realistic Dynamics",
    "text": "Portfolio Choice with Realistic Dynamics\nAs our third application, we consider a portfolio choice problem with realistic dynamics\n\nThe problem will feature a large number of state variables, shocks, and controls\n\n\n\n\n\nWe consider the problem of an investor with Epstein-Zin preferences\n\nInvestor must choose consumption and portfolio\nInvestor has access to \\(5\\) risky assets and a risk-free asset\n\n\n\n\n\n\nVolatility is constant and expected returns are affine functions of the state \\(\\mathbf{x}_t \\in \\mathbb{R}^n\\)\n\nThe state variable follows a multivariate Ornstein-Uhlenbeck (O-U) process: \\[\nd \\mathbf{x}_t = - \\Phi \\mathbf{x}_t + \\sigma_{\\mathbf{x}} d \\mathbf{B}_t,\n\\]\n\n\n\n\n\n\nTo ensure absence of arbitrage, expected returns are derived from a state-price density (SPD)\n\nThe price of risk and interest rate are affine functions of the state \\[\nr(\\mathbf{x}_t) = r_0 + \\mathbf{r}_1^{\\top} \\mathbf{x}_t, \\qquad \\qquad\n\\boldsymbol{\\eta}(\\mathbf{x}_t) = \\boldsymbol{\\eta}_0 + \\boldsymbol{\\eta}_1^\\top \\mathbf{x}_t.\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#state-variables",
    "href": "Module05/Module05_Slides.html#state-variables",
    "title": "Machine Learning for Computational Economics",
    "section": "State variables",
    "text": "State variables\n\nList of State Variables Driving the Expected Returns of Assets\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\nMean\n\n\nS.D. (%)\n\n\n\n\n\n\n\\(\\pi_t\\)\n\n\nLog Inflation\n\n\n0.032\n\n\n2.3\n\n\n\n\n\\(y_t^{\\$}(1)\\)\n\n\nLog 1-Year Nominal Yield\n\n\n0.043\n\n\n3.1\n\n\n\n\n\\(yspr_t^{\\$}\\)\n\n\nLog 5-Year Minus 1-Year Nominal Yield Spread\n\n\n0.006\n\n\n0.7\n\n\n\n\n\\(\\Delta z_t\\)\n\n\nLog Real GDP Growth\n\n\n0.030\n\n\n2.4\n\n\n\n\n\\(\\Delta d_t\\)\n\n\nLog Stock Dividend-to-GDP Growth\n\n\n-0.002\n\n\n6.3\n\n\n\n\n\\(d_t\\)\n\n\nLog Stock Dividend-to-GDP Level\n\n\n-0.270\n\n\n30.5\n\n\n\n\n\\(pd_t\\)\n\n\nLog Stock Price-to-Dividend Ratio\n\n\n3.537\n\n\n42.6\n\n\n\n\n\\(\\Delta\\tau_t\\)\n\n\nLog Tax Revenue-to-GDP Growth\n\n\n0.000\n\n\n5\n\n\n\n\n\\(\\tau_t\\)\n\n\nLog Tax Revenue-to-GDP Level\n\n\n-1.739\n\n\n6.5\n\n\n\n\n\\(\\Delta g_t\\)\n\n\nLog Spending-to-GDP Growth\n\n\n0.006\n\n\n7.6\n\n\n\n\n\\(g_t\\)\n\n\nLog Spending-to-GDP Level\n\n\n-1.749\n\n\n12.9\n\n\n\n\n\n\nNotes: The table shows the list of 11 state variables driving expected returns in our economy, along with their mean and standard deviation. The data are collected from https://www.publicdebtvaluation.com/data. See Jiang et al. (2024) for more details.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#estimation-of-continuous-time-spd",
    "href": "Module05/Module05_Slides.html#estimation-of-continuous-time-spd",
    "title": "Machine Learning for Computational Economics",
    "section": "Estimation of continuous-time SPD",
    "text": "Estimation of continuous-time SPD\nState dynamics estimation\n\nWe run a discrete time VAR on the \\(n = 11\\) state variables: \\[\n\\mathbf{x}_t = \\Psi \\mathbf{x}_{t-1} + \\mathbf{u}_t.\n\\]\nFind \\(\\Phi\\) and \\(\\sigma_{\\mathbf{x}}\\) such that the time-integrated continuous-time process coincides with VAR\n\n\n\n\n\nState-price density estimation\n\nFind \\((r_0,\\mathbf{r}_1,\\boldsymbol{\\eta}_0, \\boldsymbol{\\eta}_1)\\) to minimize squared deviations of model-implied and data moments\nMoments: time-integrated time series for nominal yields, real yields, and expected stock returns",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#bond-yields-and-equity-expected-returns",
    "href": "Module05/Module05_Slides.html#bond-yields-and-equity-expected-returns",
    "title": "Machine Learning for Computational Economics",
    "section": "Bond Yields and Equity Expected Returns",
    "text": "Bond Yields and Equity Expected Returns",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#time-series-of-expected-returns-and-optimal-allocations",
    "href": "Module05/Module05_Slides.html#time-series-of-expected-returns-and-optimal-allocations",
    "title": "Machine Learning for Computational Economics",
    "section": "Time Series of Expected Returns and Optimal Allocations",
    "text": "Time Series of Expected Returns and Optimal Allocations\n\n\n\n\n\nExpected Returns\n\n\n\n\n\n\nAsset Allocation",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#optimal-allocation",
    "href": "Module05/Module05_Slides.html#optimal-allocation",
    "title": "Machine Learning for Computational Economics",
    "section": "Optimal allocation",
    "text": "Optimal allocation\n\n\n\n\n\nConsumption-Wealth Ratio\n\n\n\n\n\n\nStock",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#optimal-portfolio-continued",
    "href": "Module05/Module05_Slides.html#optimal-portfolio-continued",
    "title": "Machine Learning for Computational Economics",
    "section": "Optimal portfolio (continued)",
    "text": "Optimal portfolio (continued)\n\n\n\n\n\nNominal Long-Term Bond\n\n\n\n\n\n\nReal Long-Term Bond",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#conclusion",
    "href": "Module05/Module05_Slides.html#conclusion",
    "title": "Machine Learning for Computational Economics",
    "section": "Conclusion",
    "text": "Conclusion\nIn this lecture, we learned methods that alleviate the three curses of dimensionality\n\nUse deep neural networks to represent \\(V(\\mathbf{s})\\)\nCompute expectations using It√¥‚Äôs lemma and automatic differentiation\nGradient-based update rule that does not rely on root-finding procedures\n\n\n\n\nThe DPI algorithm allows us to solve high-dimensional problems\n\nMethod is effective in situations where leading numerical methods fail\n\n\n\n\nAbility to solve rich problems can be an invaluable tool in economic analysis\n\nWe oftentimes make assumptions that have no clear economic interest\n\nAssumptions are made only for tractability reasons\n\nOur method enables researchers to focus on economically interesting models\n\nInstead of focusing on models that we could easily solve",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  },
  {
    "objectID": "Module05/Module05_Slides.html#references",
    "href": "Module05/Module05_Slides.html#references",
    "title": "Machine Learning for Computational Economics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nDuarte, Victor, Diogo Duarte, and Dejanir H. Silva. 2024. ‚ÄúMachine Learning for Continuous-Time Finance.‚Äù The Review of Financial Studies 37 (11): 3217‚Äì71. https://doi.org/10.1093/rfs/hhae043.\n\n\nHennessy, Christopher A, and Toni M Whited. 2007. ‚ÄúHow Costly Is External Financing? Evidence from a Structural Estimation.‚Äù Journal of Finance 62 (4): 1705‚Äì45.\n\n\nJiang, Zhengyang, Hanno Lustig, Stijn Van Nieuwerburgh, and Mindy Z. Xiaolan. 2024. ‚ÄúThe u.s. Public Debt Valuation Puzzle.‚Äù Econometrica 92 (4): 1309‚Äì47. https://doi.org/10.3982/ECTA17674.\n\n\nMartin, Ian. 2013. ‚ÄúThe Lucas Orchard.‚Äù Econometrica 81 (1): 55‚Äì111. https://doi.org/10.3982/ECTA8446.\n\n\nSmolyak, Sergei Abramovich. 1963. ‚ÄúQuadrature and Interpolation Formulas for Tensor Products of Certain Classes of Functions.‚Äù In Doklady Akademii Nauk, 148:1042‚Äì45. 5. Russian Academy of Sciences.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press. http://incompleteideas.net/book/the-book-2nd.html.",
    "crumbs": [
      "Home",
      "Slides",
      "Module 5 ‚Äì The Deep Policy Iteration Method"
    ]
  }
]